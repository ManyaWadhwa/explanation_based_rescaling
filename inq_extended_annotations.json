{"1": {"dataset": "inq-ext", "article_num": 0, "question_details": {"0": {"question_id": "0", "question": "How was the situation resovled? ", "system": {"davinci": {"answer": "The White House said it was a mixup.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is not very helpful and misses information in the article", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer misrepresents the key information from the article", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not state the resolution accurately. Sentence 18 holds the needed data. It is said there that Fox was not formally turned down, but was operating like the interview is not happening.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The WH says the interview is still off. However, the WH has not formally turned down Fox for the interview but has not agreed, either. It is being handled as \"not happening.\"", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "The situation was not resolved, and the interview did not take place. After Fox claimed they would interview the president and that there had been a mixup with the White House, the Biden administration later said the interview was still off, stating that Fox's claim of the interview being rescheduled was inaccurate.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and relevant and consistent with the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "1": {"question_id": "0", "question": "Why couldn't Biden sit down for an interview?", "system": {"davinci": {"answer": "The answer is simple.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is useless and misses all relevant information from the article ", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question is in sentence 4, which says that Fox Corp asked for the interview to be canceled.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Joe Biden wanted to do an interview with Fox Soul but Fox canceled the interview instead. They changed their minds but it was too late. The WH didn't care to reschedule.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Biden could not sit down for the interview because it was assumed that the interview had been canceled. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "President Joe Biden did not sit down for an interview with Fox before the Super Bowl because of the contentious relationship between his administration and the right-wing television network. The White House had initially tried to arrange an interview with Fox Soul, a different streaming platform, to avoid speaking directly with Fox News anchors or reporters. However, the interview did not materialize, with both parties citing miscommunication and cancelations as the reasons.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the events in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer provides a good summary response of the situation that answers the question well. Info from sentences 12-14, 2, 4-5 support the answer.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The White House had proposed an interview with Fox Soul, a small streaming platform aimed at Black Americans, but Fox Corp. had asked for the interview to be cancelled. Later, Fox said it would interview the president after all, claiming there had been a mixup with the White House, but the White House said the interview was still off.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes use of sentences 2,4,5, and 7 to effectively answer the question.", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "Biden was not able to sit down for an interview with Fox before the Super Bowl because the White House and Fox had a contentious relationship and the White House had proposed an interview with Fox Soul, a small streaming platform aimed at Black Americans, which Fox Corp. had asked to be cancelled. Additionally, President Donald Trump had declined to sit down with anchor Lester Holt when NBC hosted the Super Bowl in 2018.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, though some information was not directly relevant or necessary", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 2 confirm the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "There is no reason to believe that Trump's actions had anything to do with Biden's decision.", "ebr_raw": "70", "ebr_num": 70, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently, but some unnecessary information is also included", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}}}}}, "2": {"question_id": "1", "question": "How long did it take for the Fox reporters to reach Washington?", "system": {"davinci": {"answer": "The distance between Los Angeles and Washington is about 2,500 miles.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is not from article info and is not very helpful. Answer is not directly stated.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer was not taken from the text, though no real answer is contained therein", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not say how long the trip took.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information in the answer is not in the article. The article does not state how long it took to travel.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The specific duration it took the reporters to get to Washington is not stated in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The article does not provide information on how long it took for the Fox reporters to reach Washington.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information on how long it took the Fox reporters to reach Washington. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and consistent with the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "3": {"question_id": "1", "question": "Why has there been a back and forth between the Biden administration and Fox?", "system": {"davinci": {"answer": "The answer is simple. The White House is trying to avoid Fox News.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response misses all of the relevant information in the article that could at least partly answer the question, and instead uses outside information not contained in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is not relevant as it is a commentary on the answer. The second sentence answers the question and is supported by 13", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Practically all relevant information was missed", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Joe Biden wanted to do an interview with Fox Soul but Fox canceled the interview instead. They changed their minds but it was too late. The WH didn't care to reschedule.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There have been a back and forth because there have been a mix up of information. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The back and forth between the Biden administration and Fox occurred because the White House tried to set up an interview with Fox Soul, a smaller streaming platform aimed at Black Americans, as an alternative to an interview with Fox News, with which the administration has a contentious relationship. There was confusion and disagreement about the interview being canceled or rescheduled, leading to a series of statements and miscommunications between both parties.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the turn out of events in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer provides a good summary response that answers as to why there was a back and forth between the two parties. Information from sentences 2, 12-14, 3-5, and 7 support the answer.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The White House has a contentious relationship with Fox News and has not granted the network a single interview during Biden's presidency. The White House attempted to circumvent Fox News by trying to set up an interview with Fox Soul, a small streaming platform aimed at Black Americans, but Fox Corp. asked for the interview to be cancelled.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Fox New continually portrays Biden and his administration in negativity and Tucker Carlson viciously attacks Biden and rants about him regularly.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is sufficient as written, but could have included a bit more information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Sentences 12 and 16 are used to make the first sentence of the answer. The second answer sentence's use of sentences 13 and 4 of the document is relevant, but the portion that uses sentence 2 is not, as the question is not asking for background on what Fox Soul is. ", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The Biden administration and Fox have had a back and forth due to the Biden administration's reluctance to grant Fox News an interview. The White House attempted to circumvent Fox News by trying to set up an interview with Fox Soul, a small streaming platform aimed at Black Americans, instead of Fox News. President Donald Trump, who villainized the press with lies and smears, declined to sit down with anchor Lester Holt when NBC hosted the Super Bowl in 2018, which may have been a factor in the Biden administration", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2, 13 and 14 summarizes the machine response.", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "There is no reason to believe that Trump's actions had anything to do with Biden's decision.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "4": {"question_id": "2", "question": "How did this end the back and forth between the Biden administration and Fox?", "system": {"davinci": {"answer": "The White House said President Joe Biden will not sit down for an interview with Fox before the Super Bowl, ending a strange back and forth between the Biden administration and the television network with which it frequently spars.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct but just restates the question and misses some relevant info", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer merely repeats sentence 1 without saying how this ended the back and forth. Such info is in sentence 2 which indicates that the press secretary said the interview would not happen.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The White house had put out the information for the interview cancellation so that there will be an end to the mix up of information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "This ended the back and forth between the Biden administration and Fox because the White House ultimately confirmed that President Joe Biden will not participate in the interview with Fox before the Super Bowl. This decision put an end to the ongoing discussions and disagreements between both parties regarding the interview.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Despite Fox News had sent reporters earlier in the day for the interview, the White House still called off the interview.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes use of 7 and 18 to answer the question.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The White House announced that President Joe Biden will not sit down for an interview with Fox before the Super Bowl, ending the back and forth between the Biden administration and the television network.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer simply repeats the information in sentence 1 without explaining how it ends the back and forth. There is no information in the document which explicitly states how this is the end. Sentence 9 would indicate that it is not ended with Fox putting out another statement.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "The Biden administration's decision to not sit down for an interview with Fox before the Super Bowl ended the back and forth between the two, as the White House had proposed an interview with Fox Soul, a small streaming platform aimed at Black Americans, but Fox Corp. had asked for the interview to be cancelled.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Fox said they would interview Biden after all but the WH administration said the interview was still off.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 confirms that the White House tried to set up an interview with Fox soul, so that they will stop been in the negative light", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The White House said the interview was still canceled after Fox said it was back on.", "ebr_raw": "10", "ebr_num": 10, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "5": {"question_id": "2", "question": "How long had the interview been scheduled?", "system": {"davinci": {"answer": "The interview was scheduled for the Friday before the Super Bowl.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't specifically answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer indicates when the interview was scheduled, but it fails to state the amount of time that interview had been on the schedule. The question is not answerable however as the document does not provide the needed information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information in the answer is not in the article. The article does not say how long the interview had been scheduled.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "The article does not provide specific information regarding how long the interview had been scheduled before it was cancelled.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state how long the interview had been scheduled.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate as the document doesn't provide a direct answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "6": {"question_id": "3", "question": "Why did Fox put out an inaccurate statement?", "system": {"davinci": {"answer": "Fox Corp. put out an inaccurate statement because the White House had not been formally turned down.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't directly address the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a vague answer that at least fits with the overall theme of the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not provide info which would explain why they made an inaccurate statement.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "There was also a mix up of information.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "gpt4": {"answer": "The article does not specify why Fox put out an inaccurate statement. However, it mentions that there was a mixup with the White House, which indicates that there could have been miscommunication or confusion that led to Fox stating that the interview was rescheduled.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable interpretation of the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and reasonable based on the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "7": {"question_id": "3", "question": "Why did Fox retract their interview proposal?", "system": {"davinci": {"answer": "Fox retracted their interview proposal because they were caught in a lie.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response misses some relevant information.", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is drawn from outside the document. The document does not say why Fox retracted the proposal.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Relevant information from the article was missed", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Fox didn't want to let Joe Biden do the interview with Fox Soul because it was primarily aimed at Blacks.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 5 states that Fox did not retract their interview proposal, there was a mix up of information. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The article does not explicitly state why Fox retracted their interview proposal with Fox Soul. However, it mentions that there was a strange back and forth between the Biden administration and Fox, with both parties providing conflicting information about the interview. The White House says that Fox Corp asked for the interview to be canceled, while later Fox claimed there had been a mixup with the White House and that they would interview the president after all. Ultimately, the interview did not take place.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "In summary, Fox retracted their interview proposal because the White House insisted on cancelling the interview. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "As much information as is present in the article is included in the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It missed the information in sentence 17.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is correct in identifying that the info needed to answer the question is not present in the document. The speculative information which follows in the answer is backed up by the document, but is essentially just saying the information is not known, which makes it not relevant as it adds nothing new to the answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " It is unclear why Fox retracted their interview proposal. The White House said Fox had asked for the interview to be cancelled, but Fox later claimed there had been a mixup with the White House.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that it is not made clear. Sentences 4 and 5 provide the additional relevant details included.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "It is unclear why Fox retracted their interview proposal. The White House has stated that Fox Corp. asked for the interview to be cancelled, but Fox has since put out a statement indicating the interview was rescheduled, which the White House has said is inaccurate.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The above sentences presumes that the proposal for the interview was retracted because Fox Corp had cancelled the interview", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "8": {"question_id": "4", "question": "Why are these issues relevant topics for the president?", "system": {"davinci": {"answer": "The president is the president of the United States of America. He is the leader of the free world. He is the most powerful man in the world. He is the most powerful man in the world. He is the most powerful man in the world. He is the most powerful man in the world. He is the most powerful man in the world. He is the most powerful man in the world. He is the most powerful man in the world. He is the most powerful man in the world", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is unhelpful and the article doesn't address the question further", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is repetitive, making much of it not relevant. It also draws its information from outside the document. The document does not expand on why the issues mentioned are important.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no explicit answer available in the document, but an answer could have been reasonably inferred and given instead of this incoherent mess", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although it was specifically said in the article, it can be inferred that Joe Biden cares about the black community and wanted to connect with the through this interview.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deuced from the article. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "gpt4": {"answer": "These issues are relevant topics for the president because they cover various aspects of his role and responsibilities. Discussing the Super Bowl allows the president to connect with the American public on a widely popular event, the State of the Union represents the president's message on the current state and goals of the administration, and addressing critical issues impacting the everyday lives of Black Americans demonstrates his commitment to addressing racial and social inequalities in the country. Engaging in conversations about these topics can help the president to convey his administration's priorities, reach out to various constituencies, and exhibit his leadership on important national matters.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Some part of the machine response can be deduced from the article. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer was not directly taken from the text, but could be reasonably inferred", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not state why these issues are relevant to the president. Outside info is therefore needed to answer the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " These topics are relevant because the Super Bowl is a major event that brings attention to the country, the State of the Union is an annual address to the nation, and the critical issues impacting the everyday lives of Black Americans are important to address in order to ensure that the Black community is being supported and heard.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information in the answer is not backed up by any of the sentences within the document. The info needed to answer the question is not provided by the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "These issues are relevant topics for the president because they are important to the everyday lives of Black Americans. The Super Bowl is a major event in the United States, and the State of the Union is an important address that outlines the president's policy agenda. By discussing these topics, the president can demonstrate his commitment to addressing the issues that are important to Black Americans.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response.", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "9": {"question_id": "4", "question": "Why did Fox put out a statement?", "system": {"davinci": {"answer": "Fox put out a statement because the White House said the interview was rescheduled.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't provide a specific answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer misrepresents the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not say that the white house rescheduled the interview. It also does not provide info which sates why Fox put out a statement, making the question not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not say why Fox chose to put out the statement that the interview was back on. It can be inferred that they thought the WH would jump at the chance to do the interview.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "Fox put out a statement to claim that the interview with President Joe Biden was rescheduled, after the White House Press Secretary announced that Fox Corp had canceled the interview. The statement was likely an attempt to clarify their position and manage public perception regarding the situation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the turn out of event in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer includes some speculation, but it is based on the text", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is consistent with the article and draws reasonable conclusions or inferences based on the information provided.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "10": {"question_id": "5", "question": "Why are such interviews pre-taped?", "system": {"davinci": {"answer": "The answer is simple.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is useless and the article doesn't provide a direct answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "No attempt was made to answer the question, but the article does not provide an answer", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer fails to provide any relevant information. The document does not say why the interviews are usually pre taped, making answering the question not possible.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not cover why interviews are pre-taped. One can assume that the interview needs to happen when the POTUS is available to do one.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The interviews are pre-taped so that they can be aired during the Super Bowl according to the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "Such interviews are typically pre-taped to ensure they are ready for broadcast during the predetermined schedule, allow for any necessary editing, provide time to resolve technical or logistical issues, and accommodate the schedules of the people being interviewed. This is particularly important for high-profile events like the Super Bowl, where timing and smooth execution of the broadcast are crucial.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the answer is correct, the article does not provide any of this information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is a reasonable and seemingly accurate answer, but it was not taken from the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "While the response is almost certainly correct, it appears to rely completely on outside information, as the article does not provide a clear answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}}}, "11": {"question_id": "5", "question": "Why was there a mix up?", "system": {"davinci": {"answer": "The White House was not aware that Fox had sent reporters to Washington to interview the president.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is reasonable but misses some details", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not align with the text, which indicates in sentence 2 that there was a mix up because the White House believed that Fox pulled the plug on the proposal to give an interview. The info from sentence 6 about the interviewers flying to Washington was an effect of the mix up, not the cause.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "All information in the article was ignored, and the answer is not supported by text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although it is not specifically mentioned, it can be inferred that Fox canceled the interview because they were irritated, then realized they'd made a mistake, changed their minds, but it was too late. The WH had moved on from the issue. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "The article does not provide specific details about the reason for the mix-up between Fox and the White House. It only states that Fox claimed there had been a mixup with the White House regarding the interview with President Joe Biden.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the turn out of events in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. It missed the information in sentences 6,7,8,9 in the document.", "ebr_raw": "10", "ebr_num": 10, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is correct in its identification of the data not being provided. The second answer sentence is not relevant because it is merely repeating the already known information stated in 5. ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " It is unclear why there was a mix up, but it appears that the White House had proposed an interview with Fox Soul, a small streaming platform aimed at Black Americans, while Fox had expected an interview with one of their traditional news anchors.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article doesn't explain what the mix up was, just that there was one. ", "ebr_raw": "20", "ebr_num": 20, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that it is not clear. Sentences 2 and 6 provide the additional relevant details mentioned.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "It is unclear why there was a mix up between the White House and Fox. It appears that Fox initially pulled the plug on the administration's proposal to give an interview with Fox Soul, a small streaming platform aimed at Black Americans. However, Fox later said it would interview the president after all, claiming there had been a mixup with the White House.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Fox said the the interview was rescheduled but the that was inaccurate.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "According to the document, I believe there was a mix up as teh White house had initially thought Fox News had called off the interview, but Fox News said their reporters had flown to Washington for the interview", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "12": {"question_id": "6", "question": "How are such interviews pre-taped?", "system": {"davinci": {"answer": "The interview is recorded on a video tape. The tape is then sent to the network. The network then edits the tape and airs it.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is likely correct, but contains speculation/outside info, as the article does not state a specific answer.", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable inference that can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The news team at Fox would travel to the POTUS to conduct the interview and tape it.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. There is no explanation in the document of how pre taped interviews work.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The network airing the event is also present to do the recordings. ", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}}}, "gpt4": {"answer": "Such interviews are pre-taped by recording them in advance, usually using video cameras and audio equipment, before the actual event or airing date. In this case, the interviews would be conducted and recorded on the Friday before the Super Bowl, and then the recorded footage would be edited and prepared for broadcast during the pre-game show on the day of the event. This allows any potential issues or edits to be addressed in advance and ensures a smooth presentation during the live broadcast.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed in the article information. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not detail HOW the interviews are pre-taped.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes some assumptions, but they are reasonable, and are based on the limited information available in the text", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is reasonable and likely 100% correct, but is entirely sourced from outside information as the document does not address the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}}}, "13": {"question_id": "6", "question": "How was the situation resovled? ", "system": {"text-davinci-003_few_shot": {"answer": "The White House said the interview was still off and Fox had put out a statement indicating the interview was rescheduled, which was inaccurate.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer which is drawn from sentence 7 suffices to answer the question. The info from sentence 9 is not relevant as it is saying the same thing, that the interview is off. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "Fox said it would still interview the President, claiming there had been a mixup with the White House. The White House later clarified that the interview was still off.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, but it could have had more detail (earlier responses to this duplicate question were much better)", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not state that the issue was resolved. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "14": {"question_id": "7", "question": "How long did it take for the Fox reporters to reach Washington?", "system": {"text-davinci-003_few_shot": {"answer": " It is not stated in the article how long it took for the Fox reporters to reach Washington.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Question requires outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. Such into is not given in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "It is not stated how long it took for the Fox reporters to reach Washington.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "the document did not indeed state how long it took Fox reporters to reach Washington. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}}}, "15": {"question_id": "8", "question": "How long had the interview been scheduled?", "system": {"text-davinci-003_few_shot": {"answer": " The White House had arranged the interview with Fox Sports Host Mike Hill & Vivica A. Fox with the President ahead of the Super Bowl. It is unclear how long the interview had been scheduled for.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question required outside information, but this is a valid attempt at answering it", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is not relevant as it is simply repeating info from sentence 8 without giving any additional info in regards to time span. The second sentence of the answer serves to answer the question in that it picks up on the fact that this info was not given in the document.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "The White House had arranged an interview with Fox Sports Host Mike Hill & Vivica A. Fox with the President ahead of the Super Bowl, but Fox Corp had the interview cancelled. It is unclear how long the interview had been scheduled for.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The period the interview had been scheduled for is not stated in teh document", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "16": {"question_id": "9", "question": "Why did Fox put out an inaccurate statement?", "system": {"text-davinci-003_few_shot": {"answer": " It is unclear why Fox put out an inaccurate statement. The White House spokesperson stated that Fox had asked for the interview to be cancelled and that the statement indicating the interview was rescheduled was inaccurate.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is badly written, but it reflects the information in the article", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer gives as much information as can be taken from the text", "ebr_raw": "95", "ebr_num": 95, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Only the first sentence is needed to answer the question. It is correct in that this information is not given in the document. The info which comes after is just repeating already known information.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "It is unclear why Fox put out an inaccurate statement indicating that the interview with President Biden had been rescheduled. The White House spokesperson stated that Fox Corp had asked for the interview to be cancelled and that the statement was inaccurate.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "There was a mix up between the White House and Fox news, hence this could have been the reason why Fox News pit out a wrong information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "17": {"question_id": "10", "question": "Why did Fox put out a statement?", "system": {"text-davinci-003_few_shot": {"answer": "Fox had claimed there had been a mixup with the White House and that its reporters had flown to Washington from Los Angeles earlier in the day to conduct the interview.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why Fox put out a false statement.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text, but the same question has already been answered in more (and better) detail", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not explain why a statment was put out that indicated the interview was rescheduled. Such information is not present in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "Fox put out a statement indicating that the interview with President Joe Biden had been rescheduled, which was inaccurate. The White House had previously arranged an interview with Fox Soul, but Fox Corp had asked for the interview to be cancelled. The White House then clarified that the interview was still off.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The why question is not directly addressed by the article, but relevant information is included in the answer", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "It is unknown why Fox put out the inaccurate statement.", "ebr_raw": "50. The provided response does mention that Fox put out an inaccurate statement, but it does not explain why Fox put out the inaccurate statement.", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "I believe Fox put out a statement so as to clarify the mixed up information between Fox and the White House", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article doesn't mention WHY Fox put out the statement.", "ebr_raw": "30", "ebr_num": 30, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "18": {"question_id": "11", "question": "Why are such interviews pre-taped?", "system": {"text-davinci-003_few_shot": {"answer": " Pre-taping interviews allows for the network to edit the interview and ensure that the content is appropriate for the audience. It also allows for the network to ensure that the interview is aired at the most opportune time.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why interviews are pre-taped.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer is very plausible, but it is not taken from the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This information was not taken from the article, but it appears to be a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why interviews are pre-taped.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not backed up by and of the sentences in the document. The document does not identify why such interviews are pre taped. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": " Pre-taping interviews before the Super Bowl allows the network airing the event to have more control over the content of the interview and to ensure that the interview is aired at the most appropriate time. This also allows the network to edit the interview if necessary and to ensure that the interview is of the highest quality.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is reasonable speculation, but it is not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why interviews are pre-taped.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 is the only part of the document that mentions the Pre-tape, but I believe this is done so that the network airing the event can release the tape whenever they are ready", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why interviews are pre-taped.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer makes reasonable assumptions, but the information is not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}}}, "19": {"question_id": "12", "question": "How are such interviews pre-taped?", "system": {"text-davinci-003_few_shot": {"answer": "The interviews are typically pre-taped on the Friday before the Super Bowl with the network airing the event.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not address how the interview are pre taped. The question appears to be asking about the production aspects of the interview which are not addressed in the document. As such, the info needed to answer the question is not present in the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "Typically, such interviews are pre-taped by having the interviewer and interviewee record their conversation in advance of the Super Bowl, with the network airing the event then broadcasting the pre-recorded interview.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article does not explain how the interviews are pre-taped.", "ebr_raw": "30", "ebr_num": 30, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although the document did not mention how the interviews are pre-taped but I believe the machine response is correct as pre-taping will involve recording of a conversation by the interviewer. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain HOW the interviews are pre-taped.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is an apparently valid answer, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}}}}}, "3": {"dataset": "inq-ext", "article_num": 1, "question_details": {"0": {"question_id": "0", "question": "How have citizens responded to the foreign objects in North American airspace?", "system": {"davinci": {"answer": "The citizens have responded by asking questions.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't directly address the question ", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The text says nothing about how citizens responded", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into how the citizens felt.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 32 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 32 says The Biden administration has been subjected to a slew of questions , but it's unclear where the questions were from.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide any information on how citizens have responded to the foreign objects in North American airspace.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Citizens have asked a lot of questions from The Biden administration", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document doesn't say anything about the citizens' response..", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "1": {"question_id": "0", "question": "How long did it take to remove the object?", "system": {"davinci": {"answer": "The object was shot down on Saturday, February 13, 2015. The object was shot down over Canadian territory in central Yukon. The object was flying at an altitude of approximately 40,000 feet, had unlawfully entered Canadian airspace and posed a reasonable threat to the safety of civilian flight. The object was shot down approximately 100 miles from the Canada-United States border over Canadian territory in central Yukon.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The dates were not taken from the article, and some key detail is missing", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The object had not been removed at the time the article was written. It had been decided that Canada would lea the recovery operation and both the US and Canada stressed the importance of recovering the object.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer is sourced from the document but doesn't answer the question.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It can be inferred from sentence 6 that it took about 24hours to remove the object.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is irrelevant to the question. The document doesn't mention anything about how long it took to remove the object.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide information on how long it took to remove the object.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer could have mentioned that the search is ongoing", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the object was spotted on Friday evening and it was brought down on Saturday, so it took around 24hours to remove the object.. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is technically correct but misses some relevant information suggesting the operation is ongoing", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in that the specific amount of time was not given in the article. Sentences 1 and 6 together indicate however that the object was seen on Friday and shot down on Saturday.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}}}}}, "2": {"question_id": "0", "question": "Why did Trudeau ask for a US fighter jet to shoot in Canadian airspace?", "system": {"davinci": {"answer": " The answer is simple. The US fighter jet was in Canadian airspace.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not helpful and misses all relevant article info.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer ignores both the text and the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Because there was a jet in the Canadian that was giving a major security concern. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 11,12,15,16,17,18.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Canadian Prime Minister Trudeau ordered the shootdown of an unidentified object in Canadian airspace as a joint response with the United States, given that the object posed a potential threat to civilian flight. Both Canadian and U.S. aircraft were scrambled to monitor and assess the object, and the decision to shoot it down was authorized not only by Trudeau but also by U.S. President Joe Biden, as mentioned in the White House readout of the call between the two leaders. The operation reflects the strong defense and security cooperation between the two countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "While the answer is perhaps too detailed, it covers all relevant information and is consistent with the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "While most parts of the response are correct, the last sentence is unsupported and incorrect.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Trudeau ordered the take down of an unidentified object that violated Canadian airspace. The object was flying at an altitude of approximately 40,000 feet, had unlawfully entered Canadian airspace and posed a reasonable threat to the safety of civilian flight.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated within the document why specifically he ordered a US jet. The answer should reflect as such as the answer as it stands does not answer the question. It lists why it would be shot down, but not why it was by a US craft in Canada.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "There was shooting in the Canadian airspace because there was an unidentified object that violated Canadian airspace.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": " Trudeau asked for a US fighter jet to shoot down the unidentified object in Canadian airspace because it posed a reasonable threat to the safety of civilian flight. The object was flying at an altitude of approximately 40,000 feet and had unlawfully entered Canadian airspace. Trudeau and US President Joe Biden authorized the take-down of the object, and the Pentagon said the object was first spotted over Alaska on Friday evening.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer provided does not address why a US jet specifically was used. That is what the question is seeking to know. With that said, the question is not answerable as this information is not provided within the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "3": {"question_id": "1", "question": "How did Trudeau become aware of the 'unidentified object'?", "system": {"davinci": {"answer": " The answer is simple. The US military informed the Canadian government of the 'unidentified object' and the Canadian government informed the Prime Minister.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer appears reasonable but misses some relevant detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is technically correct, but does not seem to have been based on a reading of the article", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 16,17,18,23.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not explicitly detail how Canadian Prime Minister Trudeau became aware of the unidentified object. However, considering the object was first spotted over Alaska, we can infer that the coordinated defense and communication between NORAD and the two countries likely led to Trudeau being informed about the object.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is not directly answered by the article, but the answer makes a reasonable assumption (and labels it as such)", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is supported by the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and reasonable given the article information.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last part of the response inferral is unsupported by the document.", "ebr_raw": "Score: 75", "ebr_num": 75, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The object was first spotted over Alaska on Friday evening and two F-22 fighter jets were scrambled to monitor the object. Trudeau was then briefed on the object on Thursday evening.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_all", "feedback": "The answer given about Trudeau is actually about the USA and Biden.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated within the document when Trudeau became aware. The answer is incorrect in stating that it was on Thursday because that date was in reference to Biden being briefed (sentence 39) ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": " Trudeau became aware of the unidentified object after it was detected by the North American Aerospace Defense Command (NORAD). NORAD detected the object over northern Canada and scrambled Canadian and U.S. aircraft to investigate. Trudeau then spoke with U.S. President Joe Biden and authorized the take-down of the object.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The Canadian Defense Minister Anita Anand informed Trudeau about the unidentified object ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer speaks more to when Trudeau learned of the incident rather then how. The document never specifies how he was informed.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "4": {"question_id": "1", "question": "How was the object first spotted?", "system": {"davinci": {"answer": "The object was first spotted by a civilian.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "This answer is not supported by the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The object was not spotted by a civilian as stated in the answer. The balloon was spotted after entering the US Air Defense Identification Zone over Alaska on January 28 before flying over Canada. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article seems to suggest that the response is not correct.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 23 and 26 states how the object was first spotted.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide information on how the object was first spotted over Alaska.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The object was spotted by NORAD on Saturday", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information, and the available fact is given in the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The article seems to provide some relevant information that the response misses", "ebr_raw": "20.", "ebr_num": 20, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The object was detected and monitored by The North American Aerospace Defense Command (NORAD)", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question comes from 26, which indicates that the object was detected by radar.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "5": {"question_id": "1", "question": "Why is the balloon suspected to be a Chinese surveillance balloon?", "system": {"davinci": {"answer": "The answer is simple. The US military shot down the balloon because it was flying over the US. The US military did not shoot down the balloon because it was a Chinese surveillance balloon.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer is not helpful, while explanation in the article is limited but some relevant info may be present", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is incoherent, but it at least fits with the text", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not say the current object was suspected to be Chinese. The article referred to the object shot down the previous week being suspected as a Chinese surveillance balloon.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The object was smaller than the suspected Chinese balloon that was shot down last weekend. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is unsupported and nonsense. It missed the information in sentence 10.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details on why the balloon is suspected to be a Chinese surveillance balloon, but it implies that various factors led to this suspicion, prompting the decision to shoot it down.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is overall correct but does miss some info that would help answer why the object may be suspected to be Chinese", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Other previous balloons that were brought had slightly similar appearance.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "As much information as is present in the article is included in the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The document doesn't explain about the suspicion, but the last part of the response is just its implication and is not stated in the document.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}}}, "6": {"question_id": "2", "question": "How many unidentified objects have been spotted in North American airspace?", "system": {"davinci": {"answer": "The answer is:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is incomplete and irrelevant. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There have been two recent sightings. The article does not go into sightings before this.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Two objects have been spotted based on the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentence 8.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Three unidentified objects have been spotted in North American airspace in the past week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100.", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "7": {"question_id": "2", "question": "How was the unidentified object shot down?", "system": {"davinci": {"answer": "The object was shot down by a US F-22 fighter jet using an AIM 9X missile.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and sourced from the article ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, for once", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "gpt4": {"answer": "The unidentified object was shot down by a U.S. F-22 fighter jet using an AIM 9X missile.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and consistent with the article ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " A US F-22 fighter jet successfully fired at the object.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is sufficient as written, but could have included a bit more information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer makes use of sentence 4, but is misses the info in sentence 18 which gives the type of missile used (an AIM 9X)", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 corresponds to the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more detail could have been included.", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The object was shot down by a US F-22 fighter jet using an AIM 9X missile, according to a statement from Pentagon spokesman Brig. Gen. Patrick Ryder.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer makes use of sentences 14, 15, and 18. Only sentence 18 is relevant however. Sentences 14 and 15 speak to the person who made the accouchement about the method used to shoot down the object, which was beyond the scope of the question.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}}}}}, "8": {"question_id": "2", "question": "How will the team recover the object?", "system": {"davinci": {"answer": "The object is on the sea ice. The team will use a helicopter to lift the object.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer is based on the text, but it makes assumptions that cannot be confirmed", "ebr_raw": "25", "ebr_num": 25, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention how the balloon would be recovered, just that it would be recovered.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response does not seem to be supported by the article, which doesn't provide a specific answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 29 and 41 only state the location of the object that was brought down, but there is no information about how it will be retrieved. ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The last sentence of the response is unsupported. It missed the information in sentences 19,29,40,41.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details on how the team will recover the object.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The object will be recovered form the sea ice, the techniques that will be used is not stated in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Some relevant information was missed", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The article doesn't provide a full answer to the question but contains some relevant detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that there is not mention of how the operations would be conducted. Sentence 29 notes that recovery teams are attempting to retrieve the debris, but there is no info stated on how they are doing that.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}}}}}, "9": {"question_id": "3", "question": "How did the object violate Canadian airspace?", "system": {"davinci": {"answer": " The object was flying over Canadian airspace.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is concise but correct, consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer is technically correct, and no additional information about \"how\" was given in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 21.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "gpt4": {"answer": "The object violated Canadian airspace by unlawfully entering it and flying at an altitude of approximately 40,000 feet, which posed a reasonable threat to the safety of civilian flight.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and sourced from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The object was \u201ccylindrical\u201d and smaller than the suspected Chinese balloon shot down last weekend, Canadian Defense Minister Anita Anand said on Saturday evening. It was flying at an altitude of approximately 40,000 feet, had unlawfully entered Canadian airspace and posed a reasonable threat to the safety of civilian flight.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently (but with some information that was not truly necessary)", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first answer sentence is not relevant, as it is merely describing the object. The second sentence just notes that it unlawfully entered Canadian airspace. The question is looking for how that went about, which is explained in sentences 15, 16, and 17.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "It unlawfully entered Canadian airspace and posed a threat to safety of civilian flight according to sentence 21", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The object was flying at an altitude of approximately 40,000 feet, had unlawfully entered Canadian airspace and posed a reasonable threat to the safety of civilian flight.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer is giving the altitude, which is not relevant. The second part of the answer is relevant as the object violated the space by unlawfully entering into it. The third part of the sentence is not relevant as it is not material to the situation. It was a violation of the airspace once the object entered the airspace.", "ebr_raw": "Score: 67", "ebr_num": 67, "workerid": "6"}}}}}, "10": {"question_id": "3", "question": "How will the government respond if these indcidents continue? ", "system": {"davinci": {"answer": "The government will respond by saying that the object shot down over Canada is not related to the spy balloon shot down last week or the unidentified object shot down over Alaska on Friday.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Response is not useful, and misses relevant info.", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't specifically mention what would be done if there are more object, but one can assume they would treat them in the same manner as the current two objects, by shooting them down.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the government will always respond by bringing down any unidentified object", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It missed the information in sentences 12,17,20,40.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not directly address how the government would respond if these incidents continue. However, it does mention that the leaders discussed the importance of recovering the object to determine details on its purpose or origin, and that the FBI will work closely with the Royal Canadian Mounted Police on the case. Additionally, it shows both Canadian and US military forces are coordinating their efforts and working together to protect their sovereignty and ensure the safety of civilian flights, which suggests they may continue to take necessary actions in response to any such incidents in the future.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is mostly relevant and is consistent with the article", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 20 summarizes the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "11": {"question_id": "3", "question": "What does the team plan to do with the object?", "system": {"davinci": {"answer": "The object is being recovered by the Canadian Forces.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, though the article does not say it directly", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Once recovered, the US and Canada will determine more details on its purpose or origin.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is correct but misses some useful details.", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 12 states what the team plans to do with the object.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It missed the informaiton in sentence 29.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The team plans to recover the object in order to determine more details on its purpose or origin.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Response is correct and consistent with the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer is not relevant, as there is no need to again mention that they plan to recover it. That is already known from 7. The second portion of the answer drawn from 12 is correct however in noting that they are going to determine more details on its purpose and origin.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}}}}}, "12": {"question_id": "4", "question": "How has the government responded to the foreign objects in their airspace?", "system": {"davinci": {"answer": "The government has responded by shooting them down.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a brief but valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is correct and consistent with the article but misses a few relevant details", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentences 11 and 12.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "gpt4": {"answer": "The government has responded by ordering the shoot-down of the foreign objects that violated their airspace. Canadian Prime Minister Trudeau and US President Biden authorized the takedown of the unidentified objects. Military aircraft, including F-22 and F-35 fighter jets, were scrambled to monitor and assess the objects, which were ultimately shot down. Both leaders also emphasized the importance of recovering the objects to determine their purpose or origin. The FBI and the Royal Canadian Mounted Police are working together during the recovery operations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is supported by information in the article. ", "ebr_raw": "75", "ebr_num": 75, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and based on information provided in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is supported by 8 and 9 and effectively answers the question. The info which follows in the answer is not relevant, as the question asks about the objects in general, and the further info in the answer is specific to the most recent downed object shot down over Canada.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}}}}}, "13": {"question_id": "4", "question": "How have the governments reassured citizens after the objects were shot down?", "system": {"davinci": {"answer": "The Canadian government has said that the object shot down on Saturday was smaller than the object shot down last weekend. The US government has said that the object shot down on Friday was a balloon.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is not useful and misses relevant info.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The text does not really address this question ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It quotes the US Defense Secretary, Lloyd Austin, as saying \"we \u2019ll always defend our sovereignty together.\"", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 29 states how the government has reassured citizens.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information in sentences 35 and 36.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "gpt4": {"answer": "The article does not explicitly mention any specific reassurances provided by the governments to the citizens after the objects were shot down. However, it does mention several actions and statements from officials, such as discussing the importance of recovering the objects for details on their purpose or origin, affirming that the countries will always defend their sovereignty together, conducting recovery operations to learn more about the objects, and working closely with each other's law enforcement agencies. These actions and statements are likely intended to inform the public and show that the governments are taking the situation seriously and working together to address it.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 16-20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is not addressed by the article, but the answer is still comprehensive, pulling out a lot of partly relevant information", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and is sourced from the article or draws reasonable inferences based on the information in it", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence of the response is not mentioned anywhere in the document.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "1"}}}}}, "14": {"question_id": "4", "question": "What does it mean that the aircraft were scrambled?", "system": {"davinci": {"answer": " The word 'scramble' is used to describe the process of getting aircraft airborne as quickly as possible.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and reasonably inferred from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer can be reasonably inferred from the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the event in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "When aircraft are \"scrambled,\" it means that they are quickly prepared and deployed for an urgent situation, such as an unidentified object or threat entering a country's airspace. In this context, Canadian and U.S. aircraft were rapidly sent into action to respond to the unidentified object that violated Canadian airspace.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer was not given directly in the text, but it was reasonably inferred", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the event that warranted the term scrambled to be used in the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable and accurate based on the information in the article", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response appears to be correct but the first part about \"scrambled\" is unsupported by the document.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Scrambling an aircraft means to launch it quickly in response to an emergency or threat. In this case, Canadian and US aircraft were launched quickly in response to the unidentified object that had violated Canadian airspace.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Some outside information is given, but it answers the question in the context of the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The term is not defined in the document, so the info provided in the answer is external to the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document confirms that a quick response was done by organizing the take down of the unidentified object which was done by the Canadian and U.S aircraft", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, though some information is not directly taken from the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "Scrambling is the process of quickly launching military aircraft in response to a perceived threat. In this case, Canadian and US aircraft were quickly launched in response to an unidentified object that had violated Canadian airspace. The US F-22 fighter jet was then able to successfully fire at the object.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer contains some outside information, but it is reasonably based on the article", "ebr_raw": "65", "ebr_num": 65, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document talks about two an emergency attack was launched to bring down the object, the Canadian and US aircraft were put together to bring down the object ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer includes some outside information, but it is reasonable given the text", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info from the answer defining scrambling is not drawn from the document. The document does not define scrambling.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "15": {"question_id": "5", "question": "How long did it take to remove the object?", "system": {"text-davinci-003_few_shot": {"answer": " It is not clear how long it took to remove the object, but it was first spotted over Alaska on Friday evening and the U.S. F-22 fighter jet successfully fired at the object.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first part of the answer is correct but then it goes on to give out incorrect information about Alaska and the fighter jets.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in that the specific time was not given. Also, it was indeed stated in sentence 15 that it was spotted on Friday. The answer cuts off however and does not list the day the f22 shot at the object, taking it down, which was on Saturday. That info is also contained in sentence 15.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The above sentences suggests that it took one day to get the unidentified object removed.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "It is not clear how long it took to remove the object. The White House confirmed that Trudeau and US President Joe Biden authorized the shoot-down and the Pentagon said the object was first spotted over Alaska on Friday evening. Canadian and U.S. aircraft were scrambled, and a U.S. F-22 successfully fired at the object. However, the Pentagon has not released any information on the timeline of the operation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Since the object was spotted on Friday, and it was brought down on Saturday, I believe it took barely 24hours to bring down the object.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer is correct in that the timing is not clear. There is a rough estimate provided however in sentence 15.  It was first noticed over Alaska on Friday and then shot down on Saturday. Additionally, the 1st part of sentence 2 and sentence 3 of the answer are both not related to the question. The final answer sentence is not drawn from the document.", "ebr_raw": "40", "ebr_num": 40, "workerid": "6"}}}}}, "16": {"question_id": "6", "question": "How was the object first spotted?", "system": {"text-davinci-003_few_shot": {"answer": "The object was first spotted over Alaska on Friday evening by the Pentagon.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the limited information available in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated wtihin the document how this object in particular was spotted.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The object was first spotted over Alaska on Friday evening by the Pentagon.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid, if concise, answer", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It was announced by the Pentagon but not spotted by them. The object was spotted by NORAD.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not specific this information. The answer merely repeats what was in sentence 6. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "17": {"question_id": "7", "question": "How will the team recover the object?", "system": {"text-davinci-003_few_shot": {"answer": "The team will be conducting search and recovery operations for the object shot down over Alaska. The recovery activities are occurring on sea ice.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "According to info in sentences 27-29, the incident talked about by the answer was separate from what the question is asking about. Info about the recovery operations beyond what was given in sentence 7 was not talked about in the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 41 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The team will recover the object by conducting search and recovery operations on sea ice. The Pentagon has said that the object was shot down in Canadian territory in central Yukon, and Canadian authorities will be leading the recovery operation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 40 and 41 corresponds to the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first sentence of the answer contradicts sentences 37 and 41 in that the sea ice operation was related to a different object. The second sentence notes where the object was shot down and repeats info from sentence 7 but does not address how the operations would be recovered. Such information is not given in the document (sentence 19 mentions the FBI and Mounted Police working closely but that does not address how it will be found)", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "18": {"question_id": "8", "question": "What does the team plan to do with the object?", "system": {"text-davinci-003_few_shot": {"answer": "The team plans to recover the object in order to determine more details on its purpose or origin.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer draws from sentence 12 to effectively answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9, 41 and 42 confirms that the object will be retrieved so that the purpose and origin will be checked", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The team plans to recover the object in order to determine more details on its purpose or origin.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms that the team wants to determine the details of the purpose or origin of the object", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 provides the answer in that they plant to determine more details  on its purpose and origin. It is rather vague, but it does address what they want to do with the object.", "ebr_raw": "95", "ebr_num": 95, "workerid": "6"}}}}}, "19": {"question_id": "9", "question": "How has the government responded to the foreign objects in their airspace?", "system": {"text-davinci-003_few_shot": {"answer": " The US and Canadian governments have authorized the shoot-down of the objects and the Pentagon and White House have confirmed the take-down. The leaders of both countries have discussed the importance of recovering the object in order to determine more details on its purpose or origin. The US military has sent fighter jets to investigate the objects and search and recovery operations are ongoing.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "For sentence 1, Sentence 11 notes the authorizations. The part about confirmation is not relevant, as the response was the shoot down. The second sentence is relevant, as recovery was part of the response noted in sentence 12. The final sentence refers to action of the military, which are separate in this context and the status of the recovery operation is not relevant either as the question asked about the response, not the progress made.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document states that the Canadian government has involved the US government to help in taking down the object", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The governments of Canada and the United States have both authorized the shoot-down of the objects in their airspace. Canadian Prime Minister Trudeau and US President Joe Biden both authorized the take-down of the object shot down over Canada on Saturday, and the Pentagon said the object was first spotted over Alaska on Friday evening. The US military also shot down a suspected Chinese surveillance balloon on February 4, and the Biden administration was briefed on the second object on Thursday evening. The US and Canadian governments are now working together", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The question is valid and sufficient, but it looks like it was slightly cut off", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on the document, the government seeked the help of the US  and authorized the object be brought down using the U.S F-22 aircraft", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer might have been slightly cut off, but there is little extra detail to add", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first answer sentence is relevant and mostly answers the question. The final sentence of the answer is also relevant and helpful. It would also be relevent to include info from sentence 20 which further discusses their working together. The other answer sentences are not relevant as they are either just repeating known info is expanding the topic beyond the scope of the question. ", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}}}}}, "20": {"question_id": "10", "question": "How have citizens responded to the foreign objects in North American airspace?", "system": {"text-davinci-003_few_shot": {"answer": " Citizens have been questioning the timing of the President's decision to shoot down the suspected Chinese surveillance balloon over the Atlantic Ocean. There has been a diplomatic fallout between Washington and Beijing as the American public tracked the balloon from Montana all the way to the Carolinas.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no mention of citizens or how the citizens felt.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Citizen response was not reported on in the document. The question cannot be answered by what was written in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 32-34 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The response from citizens to the foreign objects in North American airspace has been largely one of concern and confusion. Many have questioned why the objects were in the airspace in the first place and why they were shot down. There has also been some criticism of the US and Canadian governments for not providing more information about the objects and their origins.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Much of the answer is not taken directly from the article, but it is a reasonable supposition", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention how the citizens feel about the flying objects.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 32 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes some reasonable assumptions, given the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not drawn from the document. The document does not address this topic.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "21": {"question_id": "11", "question": "Why is the balloon suspected to be a Chinese surveillance balloon?", "system": {"text-davinci-003_few_shot": {"answer": "The balloon was spotted after entering the US Air Defense Identification Zone over Alaska on January 28 before flying over Canada, a Department of Defense official told lawmakers last week. It then reentered continental US airspace three days later. Officials said that the risk of intelligence collection against the US was low, while the risk to people and property on the ground, if the balloon were to be shot down over the US, was high given the balloon\u2019s size and weight.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not explain why the object was suspected to be a Chinese surveillance balloon nor the article explain it.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated in the document why the balloon was suspected to be a Chinese spy balloon.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "These above sentences states that there was an earlier take down of a Chinese surveillance balloon, and the balloon had entered the US Air Defense Identification Zone over Alaska on January 28 before flying over Canada", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The balloon is suspected to be a Chinese surveillance balloon because it was first spotted after entering the US Air Defense Identification Zone over Alaska on January 28 before flying over Canada. This raised suspicions that it may have been sent by China for surveillance purposes. Additionally, the US military shot it down over water after it crossed over the East Coast of the US, which further suggests that it may have been a Chinese surveillance balloon.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "These are reasonable assumptions based on facts given in the article", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The object looked smaller than the previously suspected Chinese baloon, and also the new object flew in the airspace earlier days before new object was noticed", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not state why this suspicion was attached to the balloon.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "22": {"question_id": "12", "question": "How many unidentified objects have been spotted in North American airspace?", "system": {"text-davinci-003_few_shot": {"answer": " Three unidentified objects have been spotted in North American airspace. The first was a suspected Chinese surveillance balloon shot down on February 4 by a US F-22 fighter jet, the second was an unidentified object shot down over Alaska on Friday, and the third was an unidentified object shot down over Canada on Saturday.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 gives the total number. Sentence 9 gives the info on the other two incidents. Sentence 1 gives the info on the object shot down over Canada.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 9 confirms that there has only been two unidentified objects that have been spotted", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "Three unidentified objects have been spotted in North American airspace. The first was a suspected Chinese surveillance balloon shot down on February 4 by a US F-22 fighter jet. The second was an unidentified object shot down over Alaska on Friday, and the third was an unidentified object shot down over Canadian airspace on Saturday.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct using sentences 8, 1, 9, and 27.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}, "23": {"question_id": "13", "question": "How will the government respond if these indcidents continue? ", "system": {"text-davinci-003_few_shot": {"answer": "The White House has confirmed that Trudeau and US President Joe Biden authorized the shoot-down and the Pentagon said the object was first spotted over Alaska on Friday evening. The Biden administration has been subjected to a slew of questions this week about the timing of the president\u2019s decision to shoot the spy balloon. The US military has also been conducting search and recovery operations for the object shot down over Alaska. It is likely that the government will continue to take similar measures to investigate and respond to any further incidents", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer given is irrelevant, and an appropriate passage from the text was missed", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information is not given in the document. The speculation entered into by the answer is not stated within the document in any of its sentences. The facts on the other hand presented by the answer are in the document, but they are related to past events only/", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document states that the government will always be quick in taking emergency action in the face of a threat", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The Biden administration has not yet indicated how it will respond if these incidents continue. However, they have stated that they are working closely with the Royal Canadian Mounted Police and the Federal Bureau of Investigation to investigate the object shot down over Canada and to determine its purpose or origin. The US military is also conducting search and recovery operations for the object shot down over Alaska. It is likely that the US and Canadian governments will continue to work together to monitor and respond to any further unidentified objects that enter North American", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is slightly cut off (only missing the word \"airspace\" at the end)", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not state how the government will respond if the incident continues ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more information could have been included", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence answer the question in that the info is not provided. The rest of the info is not relevant as it does not address the question directly. The document does not speculate about this topic.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "24": {"question_id": "14", "question": "How have the governments reassured citizens after the objects were shot down?", "system": {"text-davinci-003_few_shot": {"answer": " The White House confirmed that Trudeau and US President Joe Biden authorized the shoot-down and the Pentagon said the object was first spotted over Alaska on Friday evening. Trudeau said that he spoke with Biden on Saturday and that Canadian forces will lead the object recovery operation. The Pentagon spokesman Brig. Gen. Patrick Ryder said the object shot down on Saturday was first noticed over Alaska on Friday evening. The US military shot down a \u201chigh-altitude object\u201d over Alaska after US officials determined that it posed", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Directly relevant information was missed", "ebr_raw": "30", "ebr_num": 30, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "None of the info in the answer is reassuring. Sentence 20 is in that is reaffirms a commitment. Sentence 35 lists the risk being low of intelligence being gathered which is also reassuring.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not mention how the governments reassured citizens after the objects were shot down", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some relevant information was missed", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The governments of Canada and the United States have reassured citizens that the objects shot down were done so in order to protect civilian air traffic and that the risk of intelligence collection against the US was low. Canadian Defense Minister Anita Anand said on Saturday evening that the object shot down over Canada was \u201ccylindrical\u201d and smaller than the suspected Chinese balloon shot down last weekend. The White House confirmed that Trudeau and US President Joe Biden authorized the shoot-down and the Pentagon said the object was first", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is cut off, but the relevant information was included", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 19 and 20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is valid, but cut off", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Sentences 20 and 35 have reassuring info. The rest is not reassuring as it talks of the unknown.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}}}}}, "6": {"dataset": "inq-ext", "article_num": 2, "question_details": {"0": {"question_id": "0", "question": "Why is the Super Bowl cheaper this year?", "system": {"davinci": {"answer": "The Super Bowl is cheaper this year because the home team is playing in their own city and the country\u2019s second-largest metropolitan area.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The tickets are less than last year because the home team LAST YEAR, was playing in their own city.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response misstates the correct answer", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "2"}}}, "gpt4": {"answer": "The Super Bowl is cheaper this year because last year's game was played in Los Angeles, featuring the home team, the Los Angeles Rams, in the country's second-largest metropolitan area, which increased demand and ticket prices. This year, the game is in Glendale, Arizona, and the participating teams are the Philadelphia Eagles and Kansas City Chiefs. The changed circumstances have led to a decrease in ticket prices compared to last year.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The first answer sentence is supported by 6 and 7. The second and third answer sentences are supported by 3-5, which note the location teams, and how prices have been dropping once the teams were announced", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " This year's game is taking place in Glendale, Arizona, rather than in the home city of one of the teams, which was the case last year. Additionally, ticket prices have dropped more than 10% since the teams were decided on February 12. Hotel prices in the Phoenix area are also relatively expensive, but still lower than the most expensive Super Bowl in 2016 in San Francisco.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant, answers the question and is consistent with the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence in the response is incorrect and contradictory, as the home team was playing in their own city, as per sentence 7.", "ebr_raw": "30", "ebr_num": 30, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears to be accurate, though the article text is tricky to understand", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "This year's Super Bowl is cheaper than last year's because the home team is not playing in their own city and the country's second-largest metropolitan area. Additionally, hotel prices in the Phoenix area are relatively expensive, making it the second-highest level for a Super Bowl week, according to STR, a hospitality analytics firm that tracks prices.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 corresponds with the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Relevant information is taken from the article, but the second half of the answer is not relevant (or helptul) in answering the question", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first sentence in the response is wrong as the home team was actually playing in their own city, as per sentence 7. The second sentence of the response is irrelevant to the question.", "ebr_raw": "Score: 0 (The response did not include any relevant information from the document to answer the question.)", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence accurately uses info from doc sentence 7 to answer the question. The second answer sentence is not relevant however as it speaks of prices rising at hotels, which is not in line with what the question is asking.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}}}}}, "1": {"question_id": "1", "question": "How much does it typically cost to attend the Super Bowl in person?", "system": {"davinci": {"answer": "The Super Bowl is one of the most expensive sporting events to attend in person. Last year, the average ticket price on the secondary market was $5,500, according to SeatGeek.com. That\u2019s more than double the average ticket price for the 2018 World Series ($2,400) and the 2018 NBA Finals ($2,400).", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information in the answer does not appear anywhere in the article. The article does not mention how much it typically costs to attend a Super Bowl.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "These figures are not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Both of the above sentences makes reports of the average ticket price for different times. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response appears to use outside information not consistent with the article facts", "ebr_raw": "10", "ebr_num": 10, "workerid": "2"}}}, "gpt4": {"answer": "Attending the Super Bowl in person typically costs thousands of dollars. In the article, the average price customers paid for a ticket to Super Bowl LVII was $6,800, while the cheapest seats were selling at about $3,200 apiece.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 3 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not give average information on a year by year basis. There are a couple of years mentioned, as indicated in the answer, but those were one year examples and not average rates, which is what would be needed to answer the question. Further, information on average lodging, food, and transportation costs for year of multiple years would be needed, but that is not given either.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the information in sentence 9.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " StubHub said that the average price customers are paying for a ticket Thursday was $6,800. SeatGeek another ticket selling website, shows ticket prices for Sunday\u2019s game slowly declining and were averaging about $6,500 on Thursday. The most expensive ticket on its website costs $30,000 and the cheapest is $4,200. At an average of $445 per night, however, hotel prices in the Phoenix area will be relatively expensive, making it the second-highest level", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete, answers the question well and contains info from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The article included some more detail", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The average price customers are paying for a ticket to Super Bowl LVII is $6,800, but the cheapest seats are selling at about $3,200 apiece. In addition to the ticket cost, hotel prices in the Phoenix area for Super Bowl week are relatively expensive, averaging $445 per night.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response, each ticket could cost $6,800", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The question is asking for a historical average which is not made available within the document. The answer is providing info for the current year which does not go far enough to answer the question given it is looking for info on a larger scale.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "2": {"question_id": "2", "question": "How expensive are the most expensive tickets?", "system": {"davinci": {"answer": "The most expensive ticket on StubHub is $30,000.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The related website should be SeatGeek, as per sentence 10. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Response is correct and consistent with the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "gpt4": {"answer": "The most expensive tickets mentioned in the article cost $30,000 and are located near the field.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by 10 and correctly reports the most expensive ticket mentioned in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The most expensive ticket on StubHub's website costs $30,000 and is located near the field.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is not given in the document, but the response is not really relevant as it states the average price not the high price", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The most expensive ticket on StubHub's website costs $30,000 and is located near the field.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms that the most expensive ticket is $30,000", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct in using info from sentence 10 however it uses too much in that the location of the tickets is not needed to answer the question.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}}}}}, "3": {"question_id": "3", "question": "Why are the cheapest seats still so expensive?", "system": {"davinci": {"answer": "The cheapest seats are still expensive because the Super Bowl is a very popular event.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, though the article does not say it directly", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the description in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 13 and 14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is reasonable and consistent with the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "gpt4": {"answer": "The cheapest seats for the game between the Philadelphia Eagles and the Kansas City Chiefs are still so expensive because it is a highly anticipated event, demand for tickets is high, and prices for major sporting events like the Super Bowl generally tend to be high.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Prices are also still high because the city is also hosting a PGA  event ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable inference based on the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question is not provided in the document. The answer is drawn from an outside source.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The Super Bowl is a highly sought after event and tickets are in high demand. Additionally, the city is also hosting a PGA event that features golf\u2019s biggest players, which is driving up hotel prices and making it more expensive for people to attend the game.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is generally accurate and relevant but states some information not contained directly in the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first sentence in the response is unsupported and it doesn't explain the reasons for the expensive seat price, which are explained in sentences 1 and 3.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is valid, but some information is not directly relevant", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain why the cheapest seats are so expensive.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Some information in the article is not relevant to the question, though the first half is a reasonable assumption based on the text.", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The cheapest seats to Sunday's match up between the Philadelphia Eagles and Kansas City Chiefs are still so expensive because attending the Super Bowl is a highly sought after experience. The game is one of the most watched events in the world and tickets are in high demand. Additionally, the city is also hosting a PGA event that features golf's biggest players, which is driving up hotel prices and making it more expensive for people to attend the game.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document specifies that biggest golf players will be playing and people always want to watch the game, hence even a cheap ticket is still expensive", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, though it makes assumptions, and the last sentence is not relevant to the question", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\" because attending the Super Bowl is a highly sought after experience. The game is one of the most watched events in the world and tickets are in high demand.\" in the response is unsupported, so the response is partially correct.", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first two answer sentences are not backed up by sentences within the document. It is not said within the document why the prices are high in the way asked. The last answer sentence is not relevant as the question is asking about seat prices not overall costs which is where the hotel fees would lie.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "4": {"question_id": "4", "question": "Why are prices trending down?", "system": {"davinci": {"answer": "The answer is simple. The demand for tickets is not as high as it was last year.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer misrepresents the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 7 also states part of the reason why the prices are trending down. ", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response may not be fully correct or relevant, and the article doesn't provide a specific explanation", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}}}, "gpt4": {"answer": "The article does not provide a specific reason for the trending down of prices. However, it is worth noting that this year's Super Bowl is costing fans substantially less than the previous year when the home team was playing in their own city and the country's second-largest metropolitan area, which could have affected the demand and pricing.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Prices are trending down because the home team is playing in their own city. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first part of the answer is correct in identifying that the specific reason is not given. Sentence 5 should be used in place of the example which follows in the answer however because the connection between last year's location and this year's declining prices is not clear. It is more clear to say that the teams involved likely have something to do with it as prices have dropped following their being announced.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and the answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Prices are dropping due to the fact that the home team is not playing in their own city and the country\u2019s second-largest metropolitan area. In addition, the city is also hosting a PGA event that features golf\u2019s biggest players, which is driving up hotel occupancy rates.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct generally, but the second sentence isn't really relevant", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first sentence in the response is incorrect. As per sentence 7, the home team was playing in their own city and the country \u2019s second - largest metropolitan area.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer misrepresents the key information from the article", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Arizona hosting a PGA game is not a reason why the prices are trending downward.", "ebr_raw": "60", "ebr_num": 60, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The information about the PGA tour is not relevant to this question, and it is incorrect to refer to one side as \"The home team\" in this context.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": " Prices are trending down because the teams for the game were decided on February 12, and since then, ticket prices have dropped more than 10%. Additionally, hotel prices in the Phoenix area are relatively expensive due to the Super Bowl and a PGA event taking place on the same weekend, which has increased the local hotel occupancy rate to 94%.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Prices are trending down because the home team will be playing in their own city", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer completely misses the point of the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response doesn't mention the reason for the drop in price, as per sentence 7. Also, the third line in the response starting from \"Additionally\" to the end is irrelevant to the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer is never explicitly stated within the document. That being said, the answer does pick up on the possibilities hinted at in the document. Sentence 1 is incorrect tough in that it was not the act of the teams being picked that was implied in doc sentence 5, but rather the teams themselves that were implied to cause the price drop. The second sentences uses doc. sentences 11, 13, and 14 to indicate that hotel prices could be playing a role too.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "6"}}}}}, "5": {"question_id": "5", "question": "How do these trends compare to other Super Bowl sales in previous years?", "system": {"davinci": {"answer": "The following chart shows the average price of a ticket for the last five Super Bowls.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not compare the actual ticket prices to previous Super Bowls. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the superbowls for previous years have had really expensive ticket prices", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentences 6 and 12.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not useful, misses all relevant info", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article does not provide information on how the trends in ticket pricing for this Super Bowl compare to previous years. It only gives a comparison to last year's game, where ticket prices were averaging nearly $10,000 each.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 8.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much relevant information as can be taken from the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in noting the missing info and in providing the comparison info that is available from 8.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "Tickets were averaging nearly $10,000 each for last year's match up between the Cincinnati Bengals and Los Angeles Rams at LA\u2019s SoFi Stadium. At an average of $445 per night, however, hotel prices in the Phoenix area will be relatively expensive, making it the second-highest level for a Super Bowl week, according to STR, a hospitality analytics firm that tracks prices. The most expensive Super Bowl was in 2016 in San Francisco when rooms averaged $451 per night.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and relevant and answers the question with info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response doesn't mention the price of this year's ticket, which is stated in sentence 9.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some important information was missed", "ebr_raw": "65", "ebr_num": 65, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The prices for this year's Super Bowl in Glendale, Arizona are trending down compared to previous years. According to StubHub, ticket prices for this year's game are averaging around $6,800, which is substantially less than last year's match up between the Cincinnati Bengals and Los Angeles Rams at LA's SoFi Stadium, where tickets were averaging nearly $10,000 each. In addition, hotel prices in the Phoenix area for this year's Super Bowl are relatively expensive, averaging $445 per", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Previous super bowls have always had higher ticket prices. Some parts of the documents also makes reference to hotel prices been an aspect that affects ticket prices", "ebr_raw": "Score: 85", "ebr_num": 85, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but it is cut off", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response doesn't mention the hosting of a PGA event this year which drives up the hotel prices.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "It should first be noted that only 1 other year of data is given in the document in terms of ticket cost. The last answer sentence is not relevant given the context and question which is looking for info on the ticket prices themselves. The info from sentences 2, 6, and 8 does effectively answer the question.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}}}}}, "6": {"question_id": "6", "question": "Why is the Super Bowl cheaper this year?", "system": {"davinci": {"answer": "The Super Bowl is cheaper this year because the Rams are not playing.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Last year the Rams were playing at home in the country's second-largest metro area which made the tickets very desirable to local fans.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer misrepresents the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 7 states why the Super Bowl is cheaper this year. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentence 7.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is insufficient and misses useful information", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "2"}}}, "gpt4": {"answer": "The Super Bowl is cheaper this year because last year's game was played in Los Angeles, the country's second-largest metropolitan area, and featured the home team, the Los Angeles Rams. This increased the demand for tickets, resulting in higher ticket prices.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The super bowl is cheaper this year because the home team is playing in their own city.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly makes use of 6 and 7 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " This year's game is cheaper because the home team is not playing in their own city and the country's second-largest metropolitan area. In addition, the city is also hosting a PGA event that features golf's biggest players, which is driving up hotel occupancy rates and prices.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response contains the correct answer but the second sentence isn't really relevant", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first sentence in the response is contradictory and incorrect, as per sentence 7. The last sentence is irrelevant.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer misrepresents the key information from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Arizona hosting a PGA game is not a reason why the prices are trending downward.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "This is a duplicate question and answer pair, and the same issues remain.  The information about the PGA tour is irrelevant, and there is not a \"home team\" in this case, if both sides are playing out of state.", "ebr_raw": "Score: 25", "ebr_num": 25, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "This year's Super Bowl is cheaper than last year's because the home team was not playing in their own city and the country's second-largest metropolitan area. Additionally, hotel prices in the Phoenix area are relatively expensive, making it the second-highest level for a Super Bowl week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "It is cheaper because the home team will be playing in their own city according to document 7", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is valid, but the second half is irrelevant to the question", "ebr_raw": "Score: 75", "ebr_num": 75, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first sentence of the response is incorrect as the home team was playing in their own city, as per sentence 7. The second sentence of the response is irrelevant to the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The 1st sentence uses info from doc. sentence 7 to provide a reason for why it is cheaper. Another answer could come from sentence 5, which indicates that the teams played a role. Sentence 13 indicates there is another big event in town which also could play a role. The 2nd answer sentence could be looked at as both increasing the cost of attending the superbowl (which is not relevant) or lowering the ticket costs as demand dropped due to high accommodations cost.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "6"}}}}}, "7": {"question_id": "7", "question": "How is the view from the cheaper seats?", "system": {"davinci": {"answer": "The view from the cheaper seats is not bad at all.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not get into what the view was like from cheaper seats.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer is not supported by the text, which does not opine on the view from the cheapest seats", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The view from the cheaper seats is not discussed in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Article does not provide a specific answer", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}}}, "gpt4": {"answer": "The article does not provide information about the view from the cheaper seats, which cost $4,200.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information about the view from the cheaper seat. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information (or opinion)", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document doesn't mention the view of the cheapest seats.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification of the info needed being missing.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The view from the cheaper seats is not specified, but it is likely to be further away from the field than the more expensive seats.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is basically correct, as it is not stated in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "the home team was playing in their own city and the country \u2019s second - largest metropolitan area.", "ebr_raw": "30", "ebr_num": 30, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the limited text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not address this question, but the answer is a reasonable assumption", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The view from the cheaper seats will still be good, but not as close to the field as the more expensive tickets.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The view from thh cheaper seat is not discussed in the document but I believe from the document the cheaper seats will be behind the more expensive seats", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention the views but the cheaper seats are generally not good for viewing the field. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question was not given in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "8": {"question_id": "8", "question": "Why are the hotel prices so expensive when the tickets are cheaper than in previous years?", "system": {"davinci": {"answer": "The hotel prices are not expensive. The hotel prices are normal.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into detail as to why the hotel prices are high.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "All information in the article was ignored, and the answer directly contradicts the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The hotel prices are still expensive because in addition to the Super Bowl, the city is also hosting a PGA event for golf's biggest players. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentence 13 and 14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is not useful and misses relevant info. ", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "2"}}}, "gpt4": {"answer": "The hotel prices in the Phoenix area are expensive because the city is not only hosting the Super Bowl but also a PGA event featuring golf's biggest players, such as Rory McIlroy and Jordan Spieth. These two major events on the same weekend are driving up hotel occupancy rates to 94% and increasing hotel prices. The ticket prices do not have a direct relation to hotel prices in this case.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 and 14 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in making use of information from 12-14 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Most parts of the response are correct except the last sentence, which is unsupported by the document.", "ebr_raw": "Score: 75", "ebr_num": 75, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The city is also hosting a PGA event that features golf\u2019s biggest players, including Rory McIlroy and Jordan Spieth. Those two events on the same weekend puts the local hotel occupancy rate to 94%, the firm said. The normal rate for a hotel room is around $160 per night, STR said.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant, answers the question and is consistent with the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The hotel prices are expensive because the city is hosting both the Super Bowl and a PGA event on the same weekend, which has pushed the local hotel occupancy rate to 94%. The high demand for hotel rooms has caused prices to be higher than normal.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13-15 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly uses info from sentences 12-15 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}}}, "9": {"dataset": "inq-ext", "article_num": 3, "question_details": {"0": {"question_id": "0", "question": "How have the clashes affected ordinary citizens?", "system": {"davinci": {"answer": " The clashes have affected the ordinary citizens in the following ways:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is incomplete and the article doesn't really address the question directly.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The German branch of search and rescue have halted their operations until the area is safer to be in. There is looting, fighting, and other crimes being committed. It is generally unsafe for all citizens and food & water will become for scarce.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The clashes have made people seek shelter in base camp. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentence 13.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific information on how the clashes between unidentified groups in Hatay province have affected ordinary citizens. However, the overall security situation resulting from the clashes and the earthquake has caused rescue efforts to be delayed or paused and has also led to more than 48 people being arrested for looting. With food supplies dwindling, it is also anticipated that the security situation will worsen.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "As much information as is present in the article is included in the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and reasonable based on the information in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the information in sentences 3,17,20.", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "1"}}}}}, "1": {"question_id": "0", "question": "How many people have died because of the earthquake?", "system": {"davinci": {"answer": "The death toll in Turkey and Syria from the quake has surpassed 28,000, and hope of finding many more survivors is fading despite some miraculous rescues. German rescuers and the Austrian army paused search operations on Saturday, citing clashes between unnamed groups. Security is expected to worsen as food supplies dwindle, one rescuer said.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is good and relevant overall though it contains some additional unnecesary info", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer contains the info needed to answer the question (drawn from sentence 2) but also contains a lot of info that is not relevant as it goes beyond the scope of the question. The only portion needed is the portion that states, \"The death toll in Turkey and Syria from the quake has surpassed 28,000\" ", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "The death toll in Turkey and Syria from the earthquake has surpassed 28,000.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " The death toll in Turkey and Syria from the quake has surpassed 28,000.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 corresponds with the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appropriately uses sentence 2 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly uses sentence 2.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The death toll in Turkey and Syria from the quake has surpassed 28,000.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "2": {"question_id": "0", "question": "Why are there clashes? ", "system": {"davinci": {"answer": "The answer is simple. The quake hit the region where the Syrian refugees are living. The Turkish government has been trying to send them back to Syria. The refugees are not willing to go back to Syria. They are afraid of the Syrian government. They are afraid of the Syrian government's army. They are afraid of the Syrian government's militias. They are afraid of the Syrian government's secret police. They are afraid of the Syrian government's intelligence. They are afraid of the Syrian government's prisons.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Nothing in the answer is mentioned in the articles. The factions seem to be clashing over looting, stealing, and destruction of property in this area. Security is getting worse and food, water, and other necessities will become more scare.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "While there is little information in the article to address the \"why\" question, this answer was not based on the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is insufficient and misses useful information", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The clashes is attributed to a long time civil war. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is drawn from sources outside the document. The document never specifically says what is the cause of the clashes but sentences 8 and 13 indicate that there is increasing aggression between factions in Turkey and that security is expected to worsen as food, water, and hope become more scarce.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide explicit reasons for the clashes. However, it mentions that there is increasing aggression between factions in Turkey, and security is expected to worsen as food supplies dwindle.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The clashes is also attributed to civil war that has been ongoing with the opposition groups since 2011.", "ebr_raw": "20", "ebr_num": 20, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and consistent with the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It missed the information in sentences 35 and 36.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "3": {"question_id": "1", "question": "How/why does lack of food supplies cause issues with security? ", "system": {"davinci": {"answer": " The answer is simple. When people are hungry, they become desperate. Desperate people will do desperate things.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a somewhat reasonable inference, based on the limited information available", "ebr_raw": "35", "ebr_num": 35, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable based on the info in the article ", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Lack of food supply causes looting and kidnapping. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is drawn from sources outside the document. The document does not explain why lack of food/supplies causes issues with security.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Lack of food supplies can cause issues with security as it can lead to increased tensions and conflicts between different factions, as people compete for limited resources. This can also result in looting, kidnapping, and other criminal activities, further exacerbating the security situation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and reasonable given the information contained in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, based on the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 20.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}}}, "4": {"question_id": "1", "question": "What have other international organizations done in response to the earthquake and the clashes?", "system": {"davinci": {"answer": " The International Federation of Red Cross and Red Crescent Societies (IFRC) has launched an emergency appeal for $10 million to support the Turkish Red Crescent Society (Kizilay) in its response to the disaster. The IFRC has also released $1 million from its Disaster Relief Emergency Fund (DREF) to support the Turkish Red Crescent's response. The IFRC is also supporting the Turkish Red Crescent's efforts to provide shelter, water, sanitation and hygiene (WASH) assistance to the", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer seems to be outside information and misses relevant article info", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "None of the information in the answer can be found in the article which doesn't mention anything about the Red Cross or Red Crescent Societies. Germany has provided rescuers along with Austria sending army troops to help in operations. There is no mention of other countries.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The international organization have tried to put in rescue effort but they had to put it on hold because of the unrest.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 7,10,11,14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "According to text in the article, international organizations are participating in aid efforts in response to the earthquake in Turkey. For example, the German branch of the search and rescue group ISAR and Germany's Federal Agency for Technical Relief (TSW) were also in Turkey to assist with rescue efforts following the earthquake. However, they suspended operations there due to security concerns related to clashes between different factions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 and 11 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and consistent with the facts in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed the information in sentences 7,40,43.", "ebr_raw": "Score: 38", "ebr_num": 38, "workerid": "1"}}}}}, "5": {"question_id": "1", "question": "What rescue efforts are being made following the earthquake?", "system": {"davinci": {"answer": "The rescue efforts are being made by the Turkish army, the Turkish Red Crescent, the Turkish police, the Turkish coast guard, the Turkish fire department, the Turkish health ministry, the Turkish disaster management agency, the Turkish air force, the Turkish navy, the Turkish national intelligence agency, the Turkish national police, the Turkish national intelligence agency, the Turkish national intelligence agency, the Turkish national intelligence agency, the Turkish national intelligence agency, the Turkish national intelligence agency, the Turkish national intelligence agency, the Turkish national", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer is partially incorrect and poorly written, while missing key info.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer was not based on the article, which does not really address rescue operations in much detail", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The teams are on a search and rescue mission to find all victims, alive or deceased. They are moving and search through rubble for victims.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The info needed comes from 34 which notes that more than one hundred countries have sent people, and 40 and 3 give the specific examples of the Syrian Civil Defense Force and Germany and Austria all being involved in search operations.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Different rescue mission groups have been sent but operations have had to be stopped because of the unending clash. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "Following the earthquake, rescue efforts are being made to locate and save survivors amidst the rubble and debris in Turkey and Syria.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a brief but valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is correct but misses some additional information", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "Rescue efforts include search operations by the Austrian army, German rescuers, and the Turkish army. There are also efforts to provide food, water, and hope to those affected by the earthquake. Additionally, there have been arrests for looting and guns have been seized.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in different sentences like 1,3,4 and 6", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Rescue efforts have been paused and continued by Germany and Austria due to security issues. ", "ebr_raw": "30", "ebr_num": 30, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 3 supports the fact that German rescuers and the Austrian army were doing search operations. The Turkish army however was not said within the document to be doing the same. Sentence 10 indicates they were providing protection. Sentence 34 notes that hundreds of countries have sent people and sentence 40 lists the Syrian Civil Defense Force's involvement in Syria. The last two answer sentences are not relevant as they are not related to rescue efforts.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The final sentence is not relevant to talking about rescues. The second sentence does not relate to rescues specifically but more to providing aid. The first answer sentence is supported by sentence 3, except for the part about the Turkish army (sentence 10 indicates they are offering protection instead) Sentences 34 and 40 provide additional info about others that is not included in the answer.", "ebr_raw": "Score: 15", "ebr_num": 15, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "Rescue efforts following the earthquake include search and rescue operations by international organisations such as the Austrian Forces Disaster Relief Unit, the German branch of the search and rescue group ISAR, and Germany's Federal Agency for Technical Relief (TSW). Turkish authorities have also stepped in to offer protection, allowing the rescue operations to resume. In addition, the Syrian government has agreed to let UN aid into areas controlled by opposition groups, and the border crossing between Armenia and Turkey has reopened to allow aid through.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "6": {"question_id": "2", "question": "How is aggression increasing between factions?", "system": {"davinci": {"answer": "The reader is asking how is it possible to increase aggression between factions.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is useless while the article doesn't provide a specific answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article mentions destroying  property, looting, kidnapping and general fighting going on.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the events in the article, aggression is increasing as a result of the food scarcity and people are engaging in looting. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is nonsense and it missed the information in sentences 18,19,20.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not explain how aggression is specifically increasing between factions in Turkey, but it is causing unrest in the southern Hatay province and has disrupted rescue efforts following a deadly earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "As much relevant information as can be taken from the article is included in this answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is reasonable and relevant and consistent with the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document, but the answer is stated in sentences 5,7,18,19,20 in the document,", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "7": {"question_id": "2", "question": "What kinds of powers are emergency powers?", "system": {"davinci": {"answer": " Emergency powers are the powers that the government can use in a state of emergency.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Key information from the article is missing", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is reasonable given the information contained in the article", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is supported by 17 and 18. Examples are given in 19 (arrests for looting) and 21/22 (arrests of contractors over the collapse of buildings).", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "gpt4": {"answer": "Emergency powers are generally special legal powers granted to a president or governing authority during a crisis, disaster, or other extraordinary circumstance. These powers usually allow the president or government to take actions they couldn't under normal circumstances, in order to preserve public safety, manage resources, and maintain social and political order. The specific emergency powers granted would depend on the country and situation, but they might include things like imposing curfews, allocating resources, or suspending certain laws or regulations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the events described in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not  explain what emergency powers are, just that Erdogan said he would use them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't provide a specific answer, and the response is likely correct but uses outside information", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer was not taken from the text, but it can be reasonably inferred from the information therein", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but missed the information in sentences 17 and 18.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "8": {"question_id": "2", "question": "Why is there currently unrest in Southern Turkey?", "system": {"davinci": {"answer": "The unrest in Southern Turkey is due to the fact that the Turkish government has been conducting a military operation in the region.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is incomplete and not consistent with article info", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There has been a disaster earthquake, food and water is getting scarce, people are looting and destroying property, people have lost their loved ones. People are hungry, scared, and feeling hopeless. The government couldn't have prevented a lot of the damage but they didn't.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not align with what is written in the text. 8 and 13 indicate that there is increasing aggression between factions in Turkey and security is expected to worsen as food, water and hope become more scarce.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "It si also termed as a result of civil war since 2011", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The article does not provide specific information about why there is unrest in Southern Turkey, where clashes between unnamed groups disrupted rescue efforts. The Austrian army, German rescuers, and Germany's Federal Agency for Technical Relief all paused search and rescue operations due to safety risks. Security is expected to worsen as food supplies dwindle, and looting and other crimes have increased.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The unrest can also be attributed to civil war with the opposition groups that has been ongoing since 2011", "ebr_raw": "40", "ebr_num": 40, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article does not address the \"why\" question, but the answer gives relevant information", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document, but sentences 8,18,20 in the document mention the reasons. The response answer is irrelevant to the question.,", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is complete and correct and sourced from the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The unrest is due to clashes between unnamed groups, which has caused the Austrian army and German rescuers to pause search operations. There have also been reports of looting and arrests for breaking the law.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 corresponds with the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a mostly sufficient answer", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not accurate in that it is talking about the results of unrest, the symptoms as it were. The question is answered by sentence 13, which lists food, water, and hope being scarce. That is what is causing the unrest.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer ascribes the unrest being due to clashes, but that is a symptom of the unrest. Sentence 13 gives the causes as being resources becoming scarce.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The exact cause of the unrest in Southern Turkey is unclear, but it is likely related to the aftermath of the deadly earthquake that occurred on Monday. The earthquake has caused widespread destruction and displacement, and the lack of resources and security in the region may have led to tensions between different groups.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Food supplies are dwindling and people are being arrested for having gun and looting.", "ebr_raw": "20", "ebr_num": 20, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response failed to mention the aggression and clashes between different factions, as well as the problems caused by regional politics, as per sentences 8,11,12,35.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "1"}}}}}, "9": {"question_id": "3", "question": "How is the Turkish army offering protection?", "system": {"davinci": {"answer": "The Turkish army is offering protection by patrolling the streets and stopping the looting and violence.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable and essentially correct based on article informaiton.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Key details from the article were missed", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response corresponds with the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 20,21,22.", "ebr_raw": "Score: 0. The response misses all the information from the document needed to answer the question. It also does not reference the article in any way.", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details on how the Turkish army is offering protection to rescue efforts. However, it mentions that after the Austrian army paused its rescue operations, Turkish forces stepped in, allowing the rescue efforts to resume. This suggests that Turkey's military is likely providing security to deter potential aggressive factions and ensure the area is safe for the rescue teams to work in.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, and it is directly based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is supported by the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and makes reasonable inferences and conclusions based on the article info", "ebr_raw": "95", "ebr_num": 95, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document, but the answer is stated in sentences19,20,21.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "10": {"question_id": "3", "question": "How will people be punished for breaking the law?", "system": {"davinci": {"answer": "The Turkish president said he would use emergency powers to punish anyone breaking the law.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain what the punishments might be.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response simply restates the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer just repeats already known info without providing any examples. It is therefore not relevant. The document does not say specifically what will be done, but sentence 19 notes that there have been arrests.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide specific details on how people will be punished for breaking the law.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "People will be arrested for breaking the law. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "While the answer may be technically correct, it does miss some relevant information that could be used to better answer the question", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "At least a partial answer could have been taken from the article", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. It missed the information in sentences 19,20,21,22 in the document.", "ebr_raw": "20", "ebr_num": 20, "workerid": "1"}}}}}, "11": {"question_id": "3", "question": "Why is hope of finding more survivors fading? ", "system": {"davinci": {"answer": "Hope of finding more survivors is fading because the rescue teams have not found any survivors after 100 hours of search.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not consistent with the article, which does not give a specific answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Rescues have been made, regardless of what the answer claims, ", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The rescuers have found survivors after 100 hours but as time goes on, it is less likely the victims will be alive.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not align with the document, which never says that rescue efforts failed in the given time span. Rather, sentences 3 and 27 indicate that hope is fading because there is a pause in search and rescue operations and finding people after 100 hours was referred to as miraculous.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "From the article, there is a delay in the rescue mission, and the more delays, the harder it becomes to find survivor. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "Hope of finding more survivors is fading because time has passed since the earthquake and the chances of finding people alive under the rubble decrease as more time goes by.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable inference that can be made from the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is not stated in the document anywhere. This makes the response incorrect and unsupported. It missed the information in sentences 3 and 4.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable and logical based on the article info, even if not specifically stated in it", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " Security is expected to worsen as food supplies dwindle, and nearly 50 people have been arrested for looting, with several guns seized, local media reported. Additionally, rescue groups have paused search operations due to clashes between unnamed groups.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "It is fading rescue groups have paused search operations due to clashes between unnamed groups according to sentence 3", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text, but there is little directly relevant information within", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The main answer to this question is not given within the document (as time passes, people's chance of survival drops in these types of situations). That being said, a minor reason is given in answer sentence 2 which draws from document sentence 3 , that being that search operations are being paused. The first answer sentence is not relevant as it more making an update on the situation rather than explaining why hope of survivors is fading.", "ebr_raw": "25", "ebr_num": 25, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first sentence of the answer is not relevant as it talks about security and not hope of finding survivors. The final answer sentence making use of sentence 3 is correct in that such a pause will cause hope to fade. That being said, the specific reason is never actually stated in the document, that being that as time passes, less people can survive in such conditions.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "Hope of finding more survivors is fading because the death toll from the earthquake has surpassed 28,000 and the amount of time since the earthquake has passed, making it increasingly unlikely that more survivors will be found. Additionally, the unrest in southern Turkey has disrupted rescue efforts, making it more difficult for rescue teams to access the affected areas.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed that the rescue equipment never came, as per sentence 42 .", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}}}, "12": {"question_id": "4", "question": "How long have the clashes been going on?", "system": {"davinci": {"answer": " The clashes started on Friday, the day before the earthquake.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This question is not addressed by the text, and the answer misrepresents what the article actually says", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is incorrect. The article does not mention when the clashes began.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not contained in the article and it doesn't seem to provide a clear answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on sentence 1, the clashes started following Monday's deadly earthquake. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. Relevant info is contained in sentence 3 however, which indicates that search operations were stopped on Saturday, but the exact timing of the clash's beginning is not discussed.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not specify how long the clashes have been going on.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 37 states that there has been civil war since 2011. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. it missed the information in sentence 36 in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "13": {"question_id": "4", "question": "What did the German rescuers and Austrian army do to help?", "system": {"davinci": {"answer": "They were part of the rescue teams.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is simple but correct based on the document", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by 10 and 14.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The German rescuers and the Austrian army were carrying out search and rescue operations following the earthquake in southern Turkey when social unrest disrupted their efforts, causing them to pause their search operations on Saturday for safety reasons.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 and 11 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct based on the article info", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " They paused search operations on Saturday due to clashes between unnamed groups.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The German rescuers and Austrian army started a rescue mission but they had to stop because of clashes between unnamed groups", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some relevant information was missed", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Austria and Germany are doing the same thing Turkish rescuers are doing...looking for survivors.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated what their tasks were. The answer is incorrect in that it does not answer the question, it just repeats the sentence that came before.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer merely repeats info from the pervious sentence. I believe the question is looking for info on what they did in terms of rescuing people, and the specific details there are not given in the document. The better answer is then that such info is not provided.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "5": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Germans and Austrians are helping in the actual rescues of survivors and supplying needed food and water to citizens.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The German rescuers and Austrian army had been helping with search and rescue operations following the deadly earthquake in Turkey and Syria on Monday. However, they paused their operations on Saturday due to clashes between unnamed groups in the Hatay province.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "14": {"question_id": "5", "question": "Why are there clashes? ", "system": {"text-davinci-003_few_shot": {"answer": " The essay does not provide a specific reason for the clashes, but it is likely due to the dwindling food supplies and the desperation of those affected by the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct as the document did not give a specific reason why there are clashes", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Relevant information was missed", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in that it is never specifically stated. Sentence 13 provides the info that supports the speculation. The answer does miss that water was also included in sentence 13 as something that was scarce.", "ebr_raw": "Score: 75", "ebr_num": 75, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in that such info is not specified in the document. The speculation is accurate, coming from sentence 13 of the document. The answer does miss water being included in the list however.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The cause of the clashes between unnamed groups in the Hatay province of Turkey is unknown. However, it is likely that the security situation has deteriorated due to dwindling food supplies and the desperation of those affected by the earthquake. Additionally, there have been reports of looting and arrests for breaking the law, which could have contributed to the unrest.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the limited information in the text", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "15": {"question_id": "6", "question": "How/why does lack of food supplies cause issues with security? ", "system": {"text-davinci-003_few_shot": {"answer": " When food supplies dwindle, people become desperate and may resort to looting or other criminal activities in order to survive. This can lead to increased violence and unrest, which can make it difficult or dangerous for rescue operations to continue.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4, 11 and 13 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is never specifically stated within the document why food shortages lead to insecurity. The info in the answer is not backed up by any sentences in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question is not given in the document. The answer draws on info outside of the document to answer the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "When food supplies dwindle, people become desperate and may resort to desperate measures to get food. This can lead to looting, theft, and other criminal activities, which can lead to an increase in violence and unrest. This can make it difficult and dangerous for rescue workers to do their jobs, as they may be targeted by criminals or caught in the crossfire of clashes between different groups.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer is supported by the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes some reasonable assumptions, given the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\"as they may be targeted by criminals or caught in the crossfire of clashes between different groups.\" is unsupported and this makes the response partially incorrect.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}}}, "16": {"question_id": "7", "question": "What kinds of powers are emergency powers?", "system": {"text-davinci-003_few_shot": {"answer": " Emergency powers are powers granted to a government to allow them to take action in times of crisis. In this case, the president of Turkey is using emergency powers to punish anyone breaking the law in the wake of the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The powers are not furthered defined or specified in the document. The info from sentence 6 used in the answer does not answer the question as it just repeats already known information. The definition provided in answer sentence 1 does not come from within the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The term is not defined in the document. As such the appropriate answer would be that such information is not included in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "5": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not define what emergency powers would include.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "Emergency powers are powers that are granted to a government or leader in order to respond to a crisis or emergency situation. These powers are usually granted by a legislature or other governing body and can include the ability to suspend certain laws, impose martial law, or take other measures to protect the public. In the case of Turkey, President Recep Tayyip Erdogan has declared a state of emergency in response to the earthquake, which gives him the power to take action against anyone breaking the law.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article does not explain what emergency powers is composed of.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant available information is included in the answer, but there is also some outside information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain what emergency powers is composed of.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "17": {"question_id": "8", "question": "How will people be punished for breaking the law?", "system": {"text-davinci-003_few_shot": {"answer": " It is not specified how people will be punished for breaking the law, but President Erdogan has stated that he will use emergency powers to do so.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not state how people will be punished for breaking the law", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, given the limited text, but some relevant passages were skipped over", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first part of the answer is correct in that it is not specifically specified. The second part however is not relevant as it is just a repetition of already known information. Sentence 18 of the document does provide additional info however and that should be included in place of the info from sentence 6.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer is correct in that this info is not specified within the document. That being said, the second part of the answer is not relevant in that the reader knows that emergency powers will be done to do the punishing. They are asking about how. It would be better to include info from sentence 18, which notes that the state's firm hand is on their backs.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "Turkey's President Recep Tayyip Erdogan has not specified how people will be punished for breaking the law, but he has stated that he will use emergency powers to do so. This could include fines, imprisonment, or other forms of punishment depending on the severity of the crime.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence in the response is unsupport so the response is partially incorrect.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}}}, "18": {"question_id": "9", "question": "How long have the clashes been going on?", "system": {"text-davinci-003_few_shot": {"answer": " It is not specified in the article how long the clashes have been going on, only that they occurred on Saturday.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not specify the duration of the clash", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first part of the answer is correct in saying the info is not given in the document. The second part of the answer is incorrect however in that sentence 7 does not indicate that the clashes happened on Saturday, but rather that it was on Saturday that the were discussed by a spokesman. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer is correct in that the specific time frame is not specified. The answer is wrong however in that sentence 7 lists the spokesman talking on Saturday. It does not state that the clashes occurred on Saturday.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The clashes between unidentified groups in the Hatay province have been going on since early Saturday.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not say that the clashes began on Saturday, just that reports about them were made on that day", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The text does not answer this question, but the answer implies that the clashes started on Saturday, which is incorrect", "ebr_raw": "20", "ebr_num": 20, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "19": {"question_id": "10", "question": "How have the clashes affected ordinary citizens?", "system": {"text-davinci-003_few_shot": {"answer": "The security is expected to worsen as food supplies dwindle, one rescuer said. Nearly 50 people have been arrested for looting, with several guns seized, local media reported. Turkey's president said he would use emergency powers to punish anyone breaking the law.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Ordinary citizens cannot eb rescued as the rescue team cannot get to them due to the clashes", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info given does not address how citizens have been affected so far, which is what the question is asking. Sentence 3 provides the best response in that search operations were paused. That would affect ordinary citizens. This info is not picked up on by the answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The final two sentences are not relevant as they are not talking about ordinary citizens but rather criminals. The first sentence is not relevant as it is addressing the future rather than the past, which is what the question is asking about. Sentence 3 of the document provides relevant info, in that search operations were paused, which would affect ordinary people as they are the ones that need to be found.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "5": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Food and water are dwindling and are being stolen by looters. Kidnapping and general safety is a big problem.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The clashes have disrupted rescue efforts, leading to the suspension of search operations by German and Austrian rescue groups. The security situation is expected to worsen as food supplies dwindle, and nearly 50 people have been arrested for looting. The Turkish government has declared a state of emergency and has promised to take action against anyone breaking the law. The unrest has also caused 5.3 million Syrians to become homeless.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The clashes were not solely responsible for the homelessness, but otherwise, the answer is valid", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The clashes were not solely responsible for the homelessness, but otherwise, the answer is valid", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The last sentence in the response is incorrect, as per sentence 43. It was the quake that caused the homelessness, not the unrest. Also the response missed the fact that the unrest caused kidnapping as per sentence 18.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}}}, "20": {"question_id": "11", "question": "What have other international organizations done in response to the earthquake and the clashes?", "system": {"text-davinci-003_few_shot": {"answer": " Other international organizations have provided shelter for the Austrian Forces Disaster Relief Unit and have been monitoring the security situation closely. The Turkish army has also stepped in to offer protection, allowing the rescue operations to resume. The German branch of the search and rescue group ISAR and Germany's Federal Agency for Technical Relief (TSW) have also suspended operations, citing security concerns. The Vice President of Turkey, Fuat Oktay announced on Saturday the death toll in Turkey has risen to 24,617. Turkish police", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 14 and 16 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer was cut off, and some important information was also missed", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The second and final sentences of the answer are not relevant as they are not talking about international operations. The 1st answer sentence uses sentence 7 correctly except for the info about monitoring the situation closely was not mentioned. The 3rd answer sentence is supported by sentence 11. Sentence ", "ebr_raw": "73", "ebr_num": 73, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The second and 4th sentences are not relevant as they are not talking about international organizations but rather those domestic to Turkey. The 3rd answer sentence is supported by sentence 11. The first sentence is supported by sentence 7, except for the part about monitoring the situation closely. That was said to be done by the ISAR groups in sentence 13.", "ebr_raw": "85", "ebr_num": 85, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": " Other international organizations have been providing support to the Austrian Forces Disaster Relief Unit by offering shelter in their base camps. They have also been providing aid to the affected areas in the form of food, water, medical supplies, and other essential items. Additionally, some organizations have been working to facilitate the safe passage of aid into Syria, which has been affected by the earthquake as well.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more detail could have been included.", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article does not mention HOW the other countries assisted.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Only the first sentence of the response is supported, the rest is unsupported.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}}}, "21": {"question_id": "12", "question": "How is aggression increasing between factions?", "system": {"text-davinci-003_few_shot": {"answer": " Security is expected to worsen as food supplies dwindle, one rescuer said. Turkish police have also reportedly detained 12 people over collapsed buildings in the provinces of Gaziantep and Sanliurfa. There have been reports of clashes between different factions, shots have also been fired.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 11 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The only relevant info in the answer comes from sentence 11, which mentions shots being fired. The rest of the info is merely restating prior info or is not related to the question.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The 1st answer sentence does not specify how aggression is increasing. The second sentence is not related to the topic. The 3rd answer sentence is correct in its final section, with reports of shots being fired.   ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The exact details of how aggression is increasing between factions in Turkey are not clear, but it is likely due to the ongoing rescue efforts following the deadly earthquake. With food, water, and hope becoming more scarce, tensions are likely rising between different groups as they compete for resources. Additionally, reports of looting and arrests for breaking the law may be contributing to the unrest.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the limited information in the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "22": {"question_id": "13", "question": "How is the Turkish army offering protection?", "system": {"text-davinci-003_few_shot": {"answer": " The Turkish army is offering protection by providing security to the Austrian Forces Disaster Relief Unit personnel and other international organisations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but it could have had more detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info on how the protection is being provided is not contained with in the document. What exactly they are doing is not listed in any of the sentences.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is never actually stated how the protection is being provided. The info is not in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "5": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The Turkish army is offering protection to the Austrian Forces Disaster Relief Unit by providing security in the Hatay province, where clashes between unidentified groups have been reported. This security is allowing the rescue operations to resume, as the chances of saving a life now bear a reasonable relation to the safety risk.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some other details in the article are likely relevant to this question", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence in the response is contradictory, as per sentence 9.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}}}}}, "11": {"dataset": "inq-ext", "article_num": 4, "question_details": {"0": {"question_id": "0", "question": "How is a double earthquake different to regular earthquakes?", "system": {"davinci": {"answer": "The double earthquake is a rare phenomenon. It is a combination of two earthquakes that occur in close proximity to each other. The first earthquake is called the foreshock and the second is called the mainshock. The mainshock is the larger of the two earthquakes.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer appears to be valid, but it is not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information in the answer is not in the article. The article does not go into how much worse a double earthquake is. However, one can assume that two earthquakes would be far worse than one.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer is mostly correct but contains some outside information.", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms a part of the machine response. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not present the differences among the types of earthquakes, making the question not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "A double earthquake refers to two separate earthquakes occurring closely in time and location. In the context of the article, it refers to two earthquakes that took place on the same day with magnitudes of 7.8 and 7.5. A single, or regular, earthquake typically consists of just one major seismic event.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on the occurrence of the earthquake described in the article, the machine response is correct.  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable inference based on the article text", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence in the response is unsupported by the document.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first and last answer sentences are drawn from outside the document. The second answer sentences is supported by 6 and 7 and serves to answer the question.", "ebr_raw": "Score: 67", "ebr_num": 67, "workerid": "6"}}}}}, "1": {"question_id": "0", "question": "How many earthquakes have occurred in Turkey?", "system": {"davinci": {"answer": "According to the Turkish Disaster and Emergency Management Presidency (AFAD), there have been more than 1,000 earthquakes in Turkey since the year 2000.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This may be an accurate answer, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention how many earthquakes have occurred in Turkey.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not provide numbers as far as the amount of earthquakes that have occurred in Turkey.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document only sates that Turkey ahs more experience of earthquake than other countries. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is not contained in the article, and misses the only relevant answer I can find.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article does not specify the exact number of earthquakes that have occurred in Turkey. However, it does mention that Turkey lies on two fault lines and has experienced numerous earthquakes.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification. Further, sentence 5 contains the info indicating the placement on double fault lines and 14 supports that Turkey has experienced numerous earthquakes.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed by the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "This response incorrectly states that the information is not in the document. It missed the information in sentences 7,15,23 in the document.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Turkey lies on two fault lines and has earthquake building codes dating back more than 80 years. Since 1939, there have been at least two major earthquakes in Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_all", "feedback": "The answer is correct and complete. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not contain info listing the total number of earthquakes in Turkey.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is relevant and generally accurate, but misses a small amount of info about another quake that happened", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document mentioned an earthquake that occurred in 1939, and a recent one, so makes it two earthquakes", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "Turkey lies on two fault lines and has experienced earthquakes for more than 80 years. Since 1939, the most devastating earthquake to occur in Turkey was the double earthquake that occurred on Monday, registering magnitudes of 7.8 and 7.5.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Earthquake has occurred twice, one in 1939 and the second happened recently.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Neither the article or the answer says how many earthquakes have occurred in Turkey.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "2": {"question_id": "1", "question": "How did the rescue operation respond to the earthquakes?", "system": {"davinci": {"answer": "The first quake registered magnitude 7.8 at 04:17, followed by another of 7.5 dozens of miles away. It required a massive rescue operation spread across 10 of Turkey's 81 provinces. But it took time for the response to build and some villages could not be reached for days. More than 30,000 people from the professional and voluntary sector eventually arrived, along with teams from many other countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Some key details were left out", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Though it contains some extra information, the answer is generally relevant and contains information consistent with the article.", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6-12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first two sentences provide background info already known from previous sentences. They are not relevant to the answer. The 3rd answer sentence The info which follows is relevant and is supported by 10 and 11.", "ebr_raw": "Score: 90", "ebr_num": 90, "workerid": "6"}}}, "gpt4": {"answer": "The rescue operation in response to the earthquakes involved more than 30,000 people from the professional and voluntary sector, along with teams from many other countries. However, it took time for the response to build, and some villages could not be reached for days. The initial hours were critical, but roads were damaged, and search and rescue teams struggled to get through until day two or day three.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9-13 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 17.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first two answer sentences are correct, providing an overview of the response drawn from 10, 11. The third answer sentence speaks more to why it took time for the response to build and therefore goes beyond the scope of the question.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}}}}}, "3": {"question_id": "1", "question": "How have people responded to Erdogan's time in power?", "system": {"davinci": {"answer": " Erdogan has been in power for 20 years. He was elected as prime minister in 2003 and became president in 2014. He has been accused of becoming increasingly authoritarian and polarising the country. He has also been criticised for his response to the 2016 coup attempt.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, though some minor facts were added from outside information", "ebr_raw": "95", "ebr_num": 95, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first two sentences of the answer describe his roles and the amount of time he has been in power, but the question asks how the response to that has been. The 3rd sentence is supported by 54. The 4th sentence is not backed up by the document, as 59 mentions the coup, but does not speak of the president being criticized for it. Relevant info is missing from 56 57, which speak of parties unifying to try and defeat the president and people not wanting to listen to him.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer is mostly correct but seems to contain some unsupported information", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}}}, "gpt4": {"answer": "In Recep Tayyip Erdogan's 20 years in power, Turkey has become a highly polarized country. Many towns and cities are run by his party, the AKP. However, he has become increasingly intolerant of criticism, with many opponents in jail or having fled abroad. After an attempted coup against him in 2016, he arrested tens of thousands of Turks and sacked civil servants. The economy has been in freefall, with inflation at 57%, leading to sky-high costs of living. Six opposition parties are expected to announce a unified candidate to challenge Erdogan in the upcoming elections. His hopes of unifying the country ahead of the elections are likely to be unsuccessful.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer reports facts from the following sentences to answer the question:54, 53, 58, 59, 60, 56, 57.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 36-39 confirms part of the machine response. ", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " People have become increasingly polarised and intolerant of criticism. Opposition parties are poised to announce a unified candidate in a bid to bring down the president.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "For the first sentence of the answer, the first portion is supported by sentence 54. The second portion of the sentence is incorrect however in that it was Erdogan is said to be the one intolerant of criticism (sentences 57/58). The second sentence of the answer is supported by sentence 56. ", "ebr_raw": "Score: 75", "ebr_num": 75, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and accurate based on the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 54, 55 and 57 confirms the machine response, Mr Erdogan 's hopes of unifying the country ahead of those elections are likely to fall on deaf ears.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "People have responded to Erdogan's time in power with increasing polarization. His pleas for national unity have gone largely unheeded, and his government has been criticized for its handling of the recent earthquake disaster. Additionally, many of his opponents are in jail or have fled abroad, and the economy has been in freefall with a 57% inflation rate leading to a sky-high cost of living.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Different part of the document talks about how people did not yield to Erdogan's pleas, and the situation of the country has ruined the economy, many people have fled", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "There was an attempted coup that ended in bloodshed and thousands of Turk arrests and sacking civil servants.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "4": {"question_id": "2", "question": "Why could some villages not be reached for days?", "system": {"davinci": {"answer": "The answer is simple. The villages were in the mountains.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "There is no information in the article that can either confirm or refute what the answer's author has given here", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It took time for response to build and there was so much damage to roads that it was difficult to reach some areas for days.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer appears to be speculation as the article does not provide a specific answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Some villages could not be reached because it took time for the response to build. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not align with the document. 12 and 13 indicate that those with the disaster authority were themselves caught in the earthquakes. There were damaged roads and the search and rescue teams struggled to get through.", "ebr_raw": "0.", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Some villages could not be reached for days due to the damage caused by the earthquakes. The initial hours were critical, but roads were damaged, and search and rescue teams struggled to get through until day two or day three.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on how intensity of the earthquake, the machine response is confirmed in the article. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is well supported by 13. Information from 18 is left out of the answer however that should be included. 18 explains that there was a delay as well due to the military being left out of the planning and having to wait for an order from the goverment. ", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "6"}}}}}, "5": {"question_id": "2", "question": "Why were there shortcomings in Erdogan's response? ", "system": {"davinci": {"answer": " Erdogan's response was not the problem. The problem was the lack of response from the people. Erdogan's response was not the problem. The problem was the lack of response from the people. Erdogan's response was not the problem. The problem was the lack of response from the people. Erdogan's response was not the problem. The problem was the lack of response from the people. Erdogan's response was not the problem. The problem was the lack of response from the people. Erdogan's response was not", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "While Erdogan admitted shortcomings, the articles don't mention why they happened. However, Erdogan blames fate and says it was a part of destiny's plan.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not relevant as it repeats info which does not answer the question. 19 contains the data needed. It is said there that the search efforts were not as fast as the government wanted, despite having the largest search and rescue team in the world. 10 notes that some villages were not reached for days.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "There were shortcomings from people, as they did not heed to the warnings and instructions concerning the upcoming earthquake. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not helpful and misses useful information in the article. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "There were shortcomings in Erdogan's response to the earthquake because search and rescue efforts were not as fast as the government wanted. Politics also interfered in the emergency response, with the Erdogan government sidelining military involvement, which led to delays in rescue and search operations. Furthermore, building codes were not followed, resulting in a large number of collapsed buildings and casualties.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The questioned is answered via the first two answer sentences (drawn from 19, 15, 18) The final answer sentence speaks to why the damage was as bad as it was and was related to events prior to the earthquakes occurrence. Thus was not a part of the response but part of the leadup and is therefore beyond the scope of the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article fully backs up the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information in sentences 36,37,45,46,48,49,61,62.", "ebr_raw": "Score: 25", "ebr_num": 25, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The response took time to build and some villages could not be reached for days. The potential rescue effort was far bigger than in 1999, but with the military left out of the planning it had to wait for an order from the government. Building codes were not followed, leading to heavy casualties. The large sums collected through two \"earthquake solidarity taxes\" created after the 1999 quake were not fully explained where the money has been spent. There was a 2018 government amnesty that meant violations of the building code", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is supported by sentence 10. The second sentence does not need to reference 1999, as that is not relevant here and doesn't make sense as it is written. The info is drawn from sentence 18, and it would be better to just say the miltary was left out of planning and had to wait for an order which created a delay. The sentences which follow are not about the response but what lead to the disaster being so bad.", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is helpful, relevant and complete and contains info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Enough preventive preparations were not made by the people according to sentence 19 and 26", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The shortcomings in Erdogan's response were due to the delay in the start of the rescue and search operations. This delay was caused by the government not utilizing the military in the planning process, which is traditionally the most organized and logistically powerful organization in the world. Additionally, the building codes in Turkey were not followed, leading to the dramatic loss of life.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Shortcomings were due to the delay in rescue mission, teh government did not ensure that building codes were followed so as to reduce the extent of damage caused by the earthquake", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "6": {"question_id": "3", "question": "Why did teams from other countries assist in the rescue operation?", "system": {"davinci": {"answer": "The answer is simple. Turkey is a member of the International Search and Rescue Advisory Group (INSARAG).", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer may be valid, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There was nothing in the article about the INSARAG. The article doesn't mention why other countries helped but it is customary for it to happen.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is outside information, while the article does not provide a specific answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The earthquake was far more intense than past ones, hence Turkey needed all the help it could get", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not explain why other countries wanted to help.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not specify the exact reasons, but it is common for international teams to provide assistance during large-scale natural disasters as a humanitarian response, offering support and expertise to enhance local rescue and recovery efforts.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the limited information in the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "According to the turn up of rescue missions after the disaster,  it can be deduced that it is normal for  teams from other countries to assist in the rescue operation", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct in identifying that the info needed to answer the question is not present in the document. The info provided about international teams helping in such events comes from an outside source.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "7": {"question_id": "3", "question": "Why do the building codes date back more than 80 years?", "system": {"davinci": {"answer": "The building codes in Turkey were first introduced in 1927. The 1927 building code was revised in 1934, and then in 1939. The 1939 building code was revised in 1954, and then in 1964. The 1964 building code was revised in 1973, and then in 1983. The 1983 building code was revised in 1994, and then in 2004. The 2004 building code was revised in 2009, and then in 2018.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question, but the text can only provide a partial answer, at best", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information in the answer does not appear in the article. Turkey lies on two fault lines so it has been important to have building codes to help reduce damage caused by earthquakes.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer lists date information for the codes, but it does not say why the codes are dated back over 80 years. The document does not state why they were started specifically 80 years ago, but the general sentiment of the area is summed up in 27, which notes that the earthquakes cannot be stopped, so they have to diminish the damage created by them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no information as to why the building codes date back more than 80 years. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not supported by the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The building codes in Turkey date back more than 80 years because the country has a long history of being prone to earthquakes due to lying on two fault lines. These codes were created to help minimize the impact and damage caused by earthquakes on buildings and infrastructure.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first answer sentence is a rephrasing of sentence 5 and is therefore not relevant as it does not speak to why the codes were set up in the given time period as opposed to any other time period. The second answer sentence sounds correct but does not come from any of the document's sentences and it does not address why specifically 80 years ago the codes came into existance.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "While not directly given in the article, this answer can be very reasonably inferred from the text", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 6.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The building codes date back more than 80 years because Turkey has more experience of earthquakes than almost any other country.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not sufficient to explain why their experience leads to old building codes. Rather, the information should be drawn from sentences 30 (which indicates the codes were updated) 42 (which explains that the updated codes could by bypassed via paying a fine)", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer isn't missing anything relevant but draws conclusions that aren't explicitly stated in the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 14 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The building codes in Turkey date back more than 80 years because the country has a long history of earthquakes and has been preparing for them for a long time. The codes are designed to help protect buildings from the effects of earthquakes and ensure that they are built to withstand the tremors.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "From sentence 1, earthquake has been occurring since 1939, so the government had put measures in place to avoid earthquake damage", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}}}}}, "8": {"question_id": "4", "question": "How did teams from other countries assist in the rescue operation?", "system": {"davinci": {"answer": "The answer is simple. Turkey is a member of the International Search and Rescue Advisory Group (INSARAG).", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer ignores the question and the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention what the other countries did but one can infer that they all helped in the search & rescue and food & water deliveries.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article provides no further useful details to answer the question. The response appears to be outside information and is not supported by the article.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the teams from other countries joined in the operation of rescuing people from the earthquake. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is neither drawn from the document nor explains how that helped in rescue operations. The question is not answerable however as the document does not explain how the teams from other countries helped.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide specific details about how teams from other countries assisted in the rescue operation, but it mentions that they were part of the massive rescue operation that spread across 10 of Turkey's 81 provinces. It can be inferred that these international teams likely helped with search and rescue efforts, medical support, and other forms of humanitarian assistance.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the limited information in the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence of the response is unsupported by the document.", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is correct, drawing on 9 to answer the question. The second answer sentence sounds correct, but it is not backed up by any of the document's sentences.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}}}}}, "9": {"question_id": "4", "question": "How does being on two fault lines relate to earthquakes?", "system": {"davinci": {"answer": "The answer is simple. The more faults a region has, the more earthquakes it will have.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a factually-correct answer, and the text provides little to no additional insight.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from any of the document's sentences. It is never specifically defined in the document what more fault lines mean. 14 notes however that Turkey has more experience of earthquakes than almost any other country.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The two fault lines makes the earthquake more disastrous, and this could be due to the presence of heavy rocks. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is reasonable and consistent with the article information", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "gpt4": {"answer": "Being on two fault lines relates to earthquakes because fault lines are the boundaries between tectonic plates, and their movement can cause earthquakes. When two fault lines intersect, it increases the likelihood of earthquakes occurring in that area due to the interaction between the tectonic plates.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not specifically define the relationship in question, but sentence 14 states that Turkey has more experience of earthquakes than almost any other country. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article mentions Turkey being on two fault lines but does not explain the significance of fault lines nor does it mention tectonic plates.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is apparently a factually accurate answer, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the disastrous earthquake occurrence in Turkey", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Being on two fault lines increases the risk of earthquakes, as the movement of the tectonic plates can cause seismic activity. The earthquake building codes are in place to help reduce the damage caused by earthquakes.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain why being on two fault lines increases the chances for earthquakes.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info about the building codes is not relevant, as the question is not asking about that here. Further,  the details about \"tectonic plates can cause seismic activity\" is not provided within the document. The document does not provide info that would allow the question to be answered.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is generally correct and accurate, but uses outside information that isn't explicitly stated in the document", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document explains how the earthquakes occurs, and how the extent of teh damage is brought about due to lack of adherence to the regulations concerning the preventive measures to be taken when building homes", "ebr_raw": "Score: 95", "ebr_num": 95, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "Being on two fault lines means that Turkey is more prone to earthquakes due to the increased seismic activity in the area. Earthquakes occur when two tectonic plates move against each other, and when a country is located on two fault lines, it is more likely to experience earthquakes due to the increased seismic activity in the area. The earthquake building codes that have been in place for more than 80 years are designed to help protect buildings and people from the effects of earthquakes.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Turkey is located in two fault lines according to the passage, and there is an increased seismic activity in the area, this is the main reason for earthquake according to the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is valid, though it does contain some outside information", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The article does not explain what being on two fault lines means nor does it mention tectonic plates.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain how two fault lines relates to earthquakes nor does it mention tectonic plates.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, though some information is not directly taken from the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}}}}}, "10": {"question_id": "5", "question": "Why was the earthquake 'far more intense' than other earthquakes in the past?", "system": {"davinci": {"answer": "The intensity of an earthquake is measured by the magnitude of the earthquake. The magnitude of an earthquake is the amount of energy released by the earthquake. The magnitude of an earthquake is determined by the amount of energy released at the focus of the earthquake. The magnitude of an earthquake is determined by the amount of energy released at the focus of the earthquake. The magnitude of an earthquake is determined by the amount of energy released at the focus of the earthquake. The magnitude of an earthquake is determined by the amount of", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer does nothing to address the \"why\" question, but the text provides no real information, either", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "There were two earthquakes of 7.5 and 7.8 magnitude which is high intensity in one area.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document and contains many repetitions that are not relevant. The info needed is in 6 and 7 which give the magnitudes of the recent earth quake. Given the context, it can be assumed that these magnitudes were stronger than anything that came since 1939.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 summarizes the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer is not useful or relevant to the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article does not explicitly state why the recent earthquake in Turkey was far more intense than other earthquakes in the past. It mentions that the first quake registered magnitude 7.8, followed by another of magnitude 7.5, making it the most devastating earthquake since 1939. However, the specific reasons for the increased intensity are not provided.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification of insufficient data as well as its use of sentence 7.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "All available and relevant information from the text was included in this answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There were various reasons cited as to why the recent earthquake was far more intense, this was said to be due to negligence, ignorance of the building industry, government not providing funds to put things in place, ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The first quake registered magnitude 7.8 at 04:17, followed by another of 7.5 dozens of miles away. This was more intense than the magnitude 7.4 earthquake in 1939.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Some of this information was not given in the text, but it appears to be valid", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The info in the answer drawn from sentence 7 works to answer the question, but the info which follows does not as that info is not contained within the document.  ", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is helpful and likely correct, however the article does not actually state the exact magnitude of the 1939 quake", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This was more intense because regulations for buildings were not followed.", "ebr_raw": "50. The response does not directly address regulations not being followed, which is a key point in the article, and therefore misses important information needed to answer the question.", "ebr_num": 50, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The earthquake was far more intense than other earthquakes in the past because it registered a magnitude of 7.8 and 7.5, which is higher than any earthquake since 1939. Additionally, the earthquake was spread across 10 of Turkey's 81 provinces, making it more widespread than other earthquakes in the past.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Building codes were not followed according to Prof. Mustafa Erdik", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "11": {"question_id": "6", "question": "How is a double earthquake different to regular earthquakes?", "system": {"text-davinci-003_few_shot": {"answer": "A double earthquake is two earthquakes that occur in close succession, usually within a few minutes or hours of each other. They are usually caused by the same fault line and can be more intense than a single earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info is not present in the document. The answer should state such rather than coming up with info from outside the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is likely correct and accurate but uses outside information not directly from the article", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not explain the difference between double earthquake different to regular earthquakes", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "A double earthquake is two earthquakes that occur in close succession, usually within minutes of each other. The two earthquakes can be of different magnitudes and can occur on different fault lines. This type of earthquake is more intense than a single earthquake, as the energy released is greater.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is some outside information added, though the article did little to address this quesiton", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "According to the document, a double earthquake happens after a previous one, double earthquake has more damage effect than the single one, and this is evident in the last earthquake that Turkey experienced", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain the how a double earthquake is different from one earthquake.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the limited information in the text", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}}}}}, "12": {"question_id": "7", "question": "How did the rescue operation respond to the earthquakes?", "system": {"text-davinci-003_few_shot": {"answer": " The rescue operation took time to build and some villages could not be reached for days. More than 30,000 people from the professional and voluntary sector eventually arrived, along with teams from many other countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer properly makes use of sentences 10, 11 to craft its response.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is useful and accurate, and uses info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 and 11 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The rescue operation responded to the earthquakes by mobilizing more than 30,000 people from the professional and voluntary sector, as well as teams from many other countries. They worked to reach villages that had been cut off due to damaged roads and to search for survivors.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "13": {"question_id": "8", "question": "Why could some villages not be reached for days?", "system": {"text-davinci-003_few_shot": {"answer": " Roads were damaged and search and rescue teams struggled to get through until day two or day three.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer picks up on the data in document sentence 13, but it misses the information in sentence 18 which talks about the delayed involvement of the military. That played a key role too.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is clear and accurate, and comes from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "Some villages could not be reached for days because roads were damaged and search and rescue teams struggled to get through until day two or day three.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12-13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "14": {"question_id": "9", "question": "Why did teams from other countries assist in the rescue operation?", "system": {"text-davinci-003_few_shot": {"answer": " Turkey's civil disaster authority had a staff of 10-15,000, but the military was left out of the planning and it took time for the response to build. Teams from other countries assisted in the rescue operation to help Turkey respond to the disaster more quickly.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated within the document as to why other countries helped. The answer assumes the reason, but it is not backed up with what was said within the documentation. The info included from sentence 17 only explains which Turkish groups were helping.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears to be accurate and uses info from the document to answer the question", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document states that Turkey is very large and the disaster was very intense, hence team from other countries had to assist in the rescue mission", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": " Teams from other countries assisted in the rescue operation in Turkey due to the magnitude of the earthquake and the scale of the tragedy. The earthquake was far more intense than anything seen since 1939 and caused the collapse of more than 6,000 buildings. The magnitude of the disaster required a massive rescue operation spread across 10 of Turkey's 81 provinces, and the international community responded to the call for help.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document mentions that the damage of the earthquake was intense, the magnitude was high. Hence rescue team had to come in from other countries to help ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article does not explain really WHY other countries assisted in rescue operations.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "15": {"question_id": "10", "question": "How did teams from other countries assist in the rescue operation?", "system": {"text-davinci-003_few_shot": {"answer": " Teams from other countries provided assistance in the form of search and rescue teams, medical aid, and other forms of aid.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is mostly reasonable speculation, but it fits with the article text overall", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not provide the info needed to answer the question. There is no mention of the other countries' teams beyond the brief mention in sentence 11.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is likely correct but seems to infer or speculate the specific types of aid given as the article doesn't actually say specifically", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms some part of the machine response.", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": " Teams from other countries assisted in the rescue operation by providing additional personnel, resources, and expertise to help with the search and rescue efforts. They also provided support in the form of medical aid, food, and other supplies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, though it makes some assumptions", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document only mentioned that rescue team from different countries came, but did not specify the types of aids that they offered", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not detail exactly how the teams from other countries help with the disaster.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention HOW the other countries assisted.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}}}, "12": {"dataset": "inq-ext", "article_num": 5, "question_details": {"0": {"question_id": "0", "question": "How was the baby injured? ", "system": {"davinci": {"answer": "The baby was injured by the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This answer is technically correct, but the question could have been interpreted in a way that would have allowed for a better answer", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms how the baby was injured", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "gpt4": {"answer": "The baby was injured due to the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and consistent with the article ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "1": {"question_id": "0", "question": "How were the children wounded?", "system": {"davinci": {"answer": "The children were wounded by the collapsed buildings.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 8.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 4.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "gpt4": {"answer": "The children were wounded due to the earthquake that caused buildings to collapse and other damage in the region.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and is supported by 4 and 8.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The children were wounded in the earthquake that struck Turkey. Many of them were rescued from collapsed buildings in other regions and brought to Adana because the hospital is still standing.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms that the children were wounded in an earthquake", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The children were wounded during the earthquake as confirmed in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The children were wounded in the earthquake that struck the region. They were brought to Adana City Hospital because it was still standing, while many other medical centers in the disaster zone had fallen or were damaged.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 mentions an earthquake, hence the children were wounded during the earthquake.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The machine response answered the question correctly but missed some relevant information. In its response it referred to \"the region\" but it was not clear where that is. Line 13 clarifies that it is in Turkey.", "ebr_raw": "80", "ebr_num": 80, "workerid": "0"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article never explecitily says that they were injured in an earthquake however, this information is implied given sentences 4 and 16. Additionally, sentence 8 provides an added clue about the direct cause of their injuries, that being from bein in collapsed buildings. The second answer sentence is not relevant as the question was not asking about how they came to be at that hospital.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}}}}}, "2": {"question_id": "1", "question": "How are the doctors trying to find an address?", "system": {"davinci": {"answer": "They are trying to find the address of the parents of the children.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The doctors are keeping track of where the kids were found in hopes that they lived close to the area. They are starting at that point.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They are trying to get the names of the children, and also posting information on social media ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported. It missed the information in sentences 26,27,28. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response isn't really useful and doesn't use any information from the article, which itself provides no further explanation.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article does not provide specific details on how doctors are trying to find the addresses of the injured children. It mentions that health officials are trying to match unidentified children to addresses, but often the addresses are just ruins.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They know where the children were found and can assume they were in their homes. Also, they are searching social media where people have posted pictures and information of missing children.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "They have also been using social media to get information by posting the children's pictures on social media platform. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "More information could have been included", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the information in sentences 28 and 29.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and all facts are supported by the article.  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "3": {"question_id": "1", "question": "Where is Adana City? ", "system": {"davinci": {"answer": " Adana is a city in southern Turkey, on the Seyhan River, near the Mediterranean coast. It is the administrative center of the Adana Province. The city has a population of 1.7 million, making it the fifth most populous city in Turkey. Adana lies in the heart of Cilicia, a geographical, economical and cultural region. It is an important commercial and industrial city, home to the Adana Stock Exchange and the Adana Chamber of Commerce. The city is renowned", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer seems to be valid, but it is not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is nothing in the article that mention where Adana is in Turkey or anything else about it.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Adana is mentioned to be a city in Turkey from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not give the exact location but 13 indicates that it is in Turkey.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Adana City is in Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "While the exact location is not specifically stated within the document, sentence 13 does indicate that it is in Turkey.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Adana City is located in Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document is focused on Turkey as where the earthquake occurred, so Adana city is in Turkey", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "Adana City is located in Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The main point of the document is Turkey, hence Adana is in Turkey ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Machine response missed that the city's proper name is Adana as cited in the article and not \"Adana City.\"", "ebr_raw": "50", "ebr_num": 50, "workerid": "0"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Info from sentences 8, 9, and 13 can be used to piece together that the location is indeed Turkey.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}, "4": {"question_id": "2", "question": "How are the workers searching? ", "system": {"davinci": {"answer": "They are searching by visiting the places where the children were found.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Key information from the article is missing", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 29 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.  It missed the information in sentences 26,27,28.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer may be correct but is not contained in the document", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article does not provide specific details on how the hospital workers are searching for the families of unidentified children. However, it does mention that health officials have been trying to match unidentified children to addresses, and surviving relatives and health ministry officials have been traveling between medical centers in an effort to find them.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They have also been using social media to get information by posting the children's pictures on social media platform. ", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Health officials are also searching social media which is filled with pictures of children and address information.", "ebr_raw": "60", "ebr_num": 60, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "More information could have been included", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the information in sentence 28.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and correct based on the article information. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "5": {"question_id": "2", "question": "Why can her parents not be found?", "system": {"davinci": {"answer": "The parents of the baby girl are dead.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer misrepresents the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They don't know who the baby is so they don't know who the parents are or where they are.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer is partially correct, drawing on sentence 3. Not all are dead however as sentence 3 also lists some are untraceable. Reasons are also listed in 23, 26, and 27. 23 indicates that the children cannot talk due to shock and cannot therefore give info that would be helpful. 26 and 27 note that in some cases, they have addresses, but there is often nothing left there but ruins.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 3.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "gpt4": {"answer": "Her parents cannot be found because they are either dead or untraceable due to the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a brief but reasonable answer", "ebr_raw": "Score: 95", "ebr_num": 95, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The info in the answer is correct to include, but it needs the addition of info in sentences 8 and 9 (many of the children were rescued from other regions and then brought to Adana), 23 (many cannot talk due to shock), and 26/27 (attempts to match with address often leads to nothing more than ruins)", "ebr_raw": "30", "ebr_num": 30, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Her parents are either dead or untraceable.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 corresponds with the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "Her parents cannot be found because they may have been killed in the earthquake or are untraceable due to the destruction caused by the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Homes have been destroyed, hence the reason why parents cant be found ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 3 contains the info used in the answer. The answer misses an additional reason given in sentence 23 however which lists that these children cannot really talk at the present due to shock, thus implying they cannot give the info needed to find their parents. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}}}}}, "6": {"question_id": "3", "question": "How long has the search been going on?", "system": {"davinci": {"answer": " The search is continuing.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article does not mention how long the searches have been going on.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article only states that the search is continuing, it did not state how long the search has been going on for. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not useful and the article provides no specific answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The text does not provide specific information on how long the search has been going on.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article only states that the search is continuing. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information, which the respondent acknowledges", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document doesn't how long the search has been going on.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "7": {"question_id": "3", "question": "Why are the children unidentified?", "system": {"davinci": {"answer": "The children are unidentified because their parents are dead or untraceable.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "When the children were found, there were not identifying records on them. They need to do some social media and place of discovery searches.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not valid as it is merely a repetition of sentence 3. 23, 27, and 28 hold the useful info, explaining that the children cannot talk and that the addresses they are able to get to help ID the kids are not mostly rubble.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "gpt4": {"answer": "The children are unidentified because their parents, who would know their names and other identifying information, are either dead or untraceable due to the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 and 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is essentially a duplicate question, but it is again answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer sounds correct, but the document does not talk about the children's parents knowing their names in the way described by the answer. This info is drawn from outside the document. The other parts of the answer are repeating what was said in sentence 3 of the document. The info needed comes from 23, 26, and 27, which explains that the children cannot talk due to shock and that the addresses that they have lead to ruins most of the time.", "ebr_raw": "Score: 0. The response did not include relevant information from the article to answer the question.", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 4.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Their parents are dead or untraceable, so they have no way of knowing their names.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The children are unidentified because their parents cant be found according to sentence 2", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Some key information is missing", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response, and because their parents cant be found, the children cannot be identified", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The children are unidentified because their parents are either dead or untraceable. The earthquake has caused destruction in the disaster zone, making it difficult to locate the parents of these children.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 corresponds to the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "THis is a valid answer, but could be more detailed", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is speaking more to why their parents cannot be found than it is addressing the question. That info is found in sentence 23 which lists that the children cannot talk due to shock. Additionally, address matches, which is implied to help in identification, are failing due to addresses being nothing more than ruins. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "8": {"question_id": "4", "question": "Where are the children coming from?", "system": {"davinci": {"answer": "The children are coming from the buildings that collapsed in the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It missed the information in sentences 12 and 13.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is correct but may be missing some relevant information", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}}}, "gpt4": {"answer": "The children are coming from regions affected by a devastating earthquake, where their buildings have collapsed.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the information in sentence 13.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and sourced from the article ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "9": {"question_id": "4", "question": "Why are the parents dead or untraceable? ", "system": {"davinci": {"answer": "The parents of the children are dead or untraceable because they are in the same situation as the children. They are homeless. They are in the streets. They are in the hospitals. They are in the morgues. They are in the cemeteries. They are in the mass graves. They are in the refugee camps. They are in the forests. They are in the mountains. They are in the seas. They are in the deserts. They are in the forests. They", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This incoherent answer made no attempt to cite facts from the article.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They don't know who the baby is so they don't know who the parents are or where they are.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, there was an earthquake that destroyed homes and lives, so the parents are likely dead and can't be traced due to the earthquake. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The document does not list all of the reasons given, but the first sentence does align with 4 which talks about the earthquake breaking homes. 12 explains newborn babies being rushed to another area, 23 notes that the children cannot talk due to shock, and 26 and 27 note that addresses they do have are often just ruins now.", "ebr_raw": "30", "ebr_num": 30, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and unsupported. It missed the information in sentence 4.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The parents are dead or untraceable due to the earthquake that struck Turkey. Many people died, and many more were left homeless or displaced, making it difficult to locate and identify family members.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by 4 and 14.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The earthquake broke their homes and many medical centres in the disaster zone have fallen or are damaged, making it difficult to locate the parents.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is strangely worded, but it fits with the article text", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Only \"The earthquake broke their homes\" in the response is supported by the passage and the last part isn't.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "As seen in the document, the earthquake was very intense, and many people were consumed with the earthquake and homes have been destroyed", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The parents of the unidentified children are dead or untraceable due to the earthquake that struck the region. The earthquake caused massive destruction and displacement, leaving many people dead or unable to locate their families.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The damage of the earthquake was massive, homes were destroyed, hence the parents cant be traced", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The second sentence of the answer is not drawn from the document. The first sentence draws from sentence 4. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}}}}}, "10": {"question_id": "5", "question": "How are adults doing after the earthquake? ", "system": {"davinci": {"answer": " The death toll from Turkey's earthquake has risen to at least 250, with more than 1,000 people injured. The 7.2-magnitude quake struck the country's south-east on Sunday, sending tremors through buildings in Istanbul and Ankara. The epicentre was close to the town of Van, which is home to many Kurdish people. The quake was felt in the capital, Ankara, and in Istanbul, about 100km (60 miles) away. The Turkish Red Crescent has", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer may be factually correct, but it was not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention anything that is said in the answer. There is no update on the adults.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 31 and 35 states that the adults are exhausted and shocked. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The  response answer is irrelevant and not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentence 3.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response uses outside information, while the article does not focus on adults", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article does not specifically mention the condition of adults after the earthquake, but it does describe earthquake survivors lying on trolleys and being treated for injuries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article states that the doctors are shocked and exhausted, and they are just trying to be good. This doctors are assumed to be adults. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but it could have been interpreted differently and/or expanded upon with more information from the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the information in sentence 3.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and consistent with the article  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "11": {"question_id": "5", "question": "How are the doctors helping the unidentified children?", "system": {"davinci": {"answer": " The doctors are trying to find the parents of the children. They are trying to find the addresses of the children. They are trying to find the parents of the children. They are trying to find the addresses of the children. They are trying to find the parents of the children. They are trying to find the addresses of the children. They are trying to find the parents of the children. They are trying to find the addresses of the children. They are trying to find the parents of the children.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is rambling and repetitive, but it matches key facts in the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The health officials are searching wherever they can for clues or pictures of missing children to try to find their parents. Meanwhile, they are taking care of the children's medical needs.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 and 28 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The info in the answer is supported by 6 and 7 and 38 and 39. All the repetition is not relevant however.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is repetitive and some parts are unsupported. It missed the information in sentence 26 and 28.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The doctors are helping the unidentified children by treating their injuries, stabilizing their condition, providing emotional support, and attempting to locate their families or learn their identities.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article supports the information given by the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer is reasonably inferred from the text, even though not all of it is stated explicitly in the article.", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The information in the answer is backed up by information and examples included in 2, 4, 5, 6, and 19.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentences 26 and 28.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The doctors are trying to find an address for the children and match them to their families. They are also providing medical care and trying to talk to the children to get their names.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 26 confirms the machine response, as well as some other parts of the document also confirms that the doctors have been providing medical assistance to the children", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 26 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The doctors are helping the unidentified children by trying to match them to addresses, taking them into care, and providing medical treatment. They are also trying to find out the children's names by talking to them once they are stabilized.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All parts of the document addresses medical aids given to the children and sentence 6 says the medical team is also trying to look for addresses", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The final answer sentence draws on sentences 23-25. Sentences 18-19 support medical treatment being given. Sentence 6 supports trying to match with addresses. Sentence 28 talks about children being taken into care, but it does not list doctors as being the ones to do it so the answer is not correct in that attribution as the context makes this appear more like a foster care situation. Sentence 2 notes them feeding the children and sentence 4 notes them comforting the children.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}}}}}, "12": {"question_id": "6", "question": "How many children have been found under the collapsed buildings?", "system": {"davinci": {"answer": "The answer is:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not say how many children have been found but it does say more than 260 wounded children are still unidentified.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Over 260 children have been found. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentence 13.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is incomplete and misses all the relevant information contained in the article", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article states that more than 260 wounded children have not been able to be identified across the country's disaster zone, but it does not provide the total number of children found under the collapsed buildings.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and sourced from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "13": {"question_id": "6", "question": "How was the baby injured? ", "system": {"text-davinci-003_few_shot": {"answer": "The baby was injured in an earthquake and was found with multiple fractures, a black eye, and bruises on her face.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 5 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The baby was injured in an earthquake that caused her home to collapse.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The baby was injured in a collapsed building as a result of the earthquake according to the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes use of the implied information given in sentence 4 to answer the question. Although it is never expressly said in the document it can be pieced together by what is in sentence 4.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}}}}}, "14": {"question_id": "7", "question": "How are the doctors trying to find an address?", "system": {"text-davinci-003_few_shot": {"answer": " Health officials have been trying to match unidentified children to addresses. But often the addresses are nothing more than ruins. In at least 100 cases, nameless children have already been taken into care. Turkish social media has been filled with posts showing missing children, giving details of which floor they lived on in collapsed buildings, expressing hope they may have been rescued and taken to hospital. Surviving relatives and health ministry officials have been travelling between medical centres trying to find them.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 28 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 26-29 corresponds to the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The doctors are trying to match the unidentified children to addresses, and are also using social media posts to try and locate missing children. They are also travelling between medical centres to try and find them.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 28 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer implies that doctors are traveling between medical centres, which is inaccurate", "ebr_raw": "30", "ebr_num": 30, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 28 indicates it was others who are posting on social media as they are hoping the children have been rescued. Doctors would know that they were rescued. Further, sentence 29 notes that it was relatives and health ministry officials who are travelling, not doctors. It is never stated how the doctors are trying to find the mentioned addresses. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Machine response added information not present in the article. The doctors were not the ones trying to match children to addresses and traveling between medical centers. Article states it was relatives and health ministry officials who were doing that and not doctors.", "ebr_raw": "70", "ebr_num": 70, "workerid": "0"}}}}}, "15": {"question_id": "8", "question": "How are the workers searching? ", "system": {"text-davinci-003_few_shot": {"answer": " Health officials have been trying to match unidentified children to addresses. But often the addresses are nothing more than ruins. In at least 100 cases, nameless children have already been taken into care. Turkish social media has been filled with posts showing missing children, giving details of which floor they lived on in collapsed buildings, expressing hope they may have been rescued and taken to hospital. Surviving relatives and health ministry officials have been travelling between medical centres trying to find them.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 28 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The duplicate question is answered sufficiently", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 26-29 confirms how the workers are searching", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The workers are searching for the parents of the unidentified children by trying to match them to addresses. They are also travelling between medical centres to try and find them, and using social media to post details of missing children.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 26-28 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 28 indicates that it was not doctors posting on social media as the posts expressed hopes that the children were rescued. Doctors would know the kids were rescued. Further, sentence 29 notes that it was relatives and health ministry officials traveling around, not doctors. The first answer sentence merely repeats sentence 6 information, that there is a search, it does not expand on how that is being done. The document does not provide that information for this context.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "16": {"question_id": "9", "question": "How long has the search been going on?", "system": {"text-davinci-003_few_shot": {"answer": " The search has been going on since the earthquake occurred and the children were brought to Adana City Hospital.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The search started just after the earthquake according to the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "\"The search has been going on since the earthquake occurred\" in the response is unsupported by the passage, which does not mention how long the search has been going on.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The search began immediately the earthquake occurred according to the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The search has been going on since the earthquake occurred and the children were brought to Adana City Hospital.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The search started immediately the earthquake happened", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The machine response missed the question and provided an irrelevant response. The article does not state how long the search has been going on.", "ebr_raw": "0", "ebr_num": 0, "workerid": "0"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is making an assumption, but that assumption is not contained in any of the sentences of the document. The document does not provide a time frame for the search. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "17": {"question_id": "10", "question": "Where are the children coming from?", "system": {"text-davinci-003_few_shot": {"answer": "The children are coming from collapsed buildings in other regions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms that the children are rescued from collapsed buildings", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is sufficient as written, but could have included a bit more information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 matches the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The children are coming from collapsed buildings in other regions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The machine response missed the question and provided an irrelevant response by restating the source sentence. Line 12 cites a particular city in which the children came from and line 13 indicates that this is taking place in Turkey.", "ebr_raw": "20", "ebr_num": 20, "workerid": "0"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The question is looking for specifics, but the answer just repeats the info given in sentence 8. The document does not provide further info on what other regions were involved, so the proper answer would be that such info is not available.", "ebr_raw": "20", "ebr_num": 20, "workerid": "6"}}}}}, "18": {"question_id": "11", "question": "How are adults doing after the earthquake? ", "system": {"text-davinci-003_few_shot": {"answer": "The essay does not mention how adults are doing after the earthquake, but it does mention that many medical centers in the disaster zone have fallen or are damaged, and that Adana became a rescue hub.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The adults who are the medical teams are exhausted", "ebr_raw": "10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response failed to mention parents are dead or untraceable, as per sentence 3.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The medical team are assumed to be adults, so the sentences listed above says they are shocked and exhausted", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The adults affected by the earthquake have been facing a difficult situation. Many have lost their homes and loved ones, and are struggling to cope with the aftermath of the disaster. The Turkish health ministry has reported that there are currently more than 260 wounded children who have not been able to be identified, and the number may rise as more areas are reached and the scale of homelessness is revealed.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The adults are shocked and exhausted, and trying to be good", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't address the problems the adults are having.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The machine response answered the question correctly but missed some relevant information. Line 3 states that parents have died and line 38 states that some have been reunited with their families.", "ebr_raw": "70", "ebr_num": 70, "workerid": "0"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The final answer sentence addresses children and not adults making it not relevant to the question. The first two sentences are not drawn from the document. Sentences 33-35 describe the experience/feeling of one adult, Dr. Kreskin, so that would be appropriate to include, though it should be noted that is the experience of one person. Further sentences 28-29 describe the actions of other adults only within the context of looking for children.   ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "19": {"question_id": "12", "question": "How many children have been found under the collapsed buildings?", "system": {"text-davinci-003_few_shot": {"answer": " Turkish health officials say across the country's disaster zone there are currently more than 260 wounded children who they have not been able to identify.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article, though it does not directly (or sufficiently) answer the actual question", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer says 260 but those are the children officials cannot identify. The article does not give any type of estimate as to how many children have been found.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "It is not specified how many children have been found under the collapsed buildings, but it is stated that there are currently more than 260 wounded children who have not been able to be identified across the disaster zone.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response correctly identified that the article did not have the relevant information to fully answer the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "0"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct in that it identifies that the requested info is not given in the document. It should not include the info from sentence 13 however, as that info is just stating the number who haven't been identified, not those found under buildings and not identified. That number could include children from many types of situations and thus gives an inaccurate picture of the events.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}}}}}}}, "13": {"dataset": "inq-ext", "article_num": 6, "question_details": {"0": {"question_id": "0", "question": "How do teams reach the Superbowl?", "system": {"davinci": {"answer": " The NFL playoffs are a single-elimination tournament, meaning that each team must win one game to advance to the next round. The two teams with the best records in each conference are seeded 1 and 2 for the playoffs. The two teams with the worst records in each conference are seeded last and play each other in the first round. The two teams with the best records in each conference then face the two teams with the worst records in each conference in the second round. The two teams with the", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer may be factually accurate, but it was not taken from the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The information in the answer is not in the article. The Super Bowl teams are the two team left standing from the games leading to the Super Bowl.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but  is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. Sentence 1 contains the answer, indicating that you have to be one of the two teams left standing after the season.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Teams reach the Super Bowl by advancing through the NFL playoffs, which are a series of single-elimination games that occur after the regular season. Each conference (NFC and AFC) sends their best teams, determined by regular season win-loss records and tiebreaker rules, to compete in the playoffs, culminating in a conference championship game. The winners of each conference championship game then advance to the Super Bowl to compete for the championship title.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from sentence 1.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the answer is correct, the article does not provide any of this information.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This seems to be an excellent answer, but it was not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not define how teams reach the superbowl making the question not answerable. The answer is created using an outside source.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Teams reach the Superbowl by being the last two teams standing from a punishing NFL season.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct based on the document, and isn't missing anything else relevant to the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, if concise", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer uses sentence 1 to answer the question. The sentence seems a bit flippant at first glance, but it does indeed answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": " Teams reach the Super Bowl by being one of the two teams left standing from a punishing NFL season.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and correct based on the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Teams basically compete for the superbowls, the best teams are chosen after undergoing the punishing NFL season.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}}}, "1": {"question_id": "1", "question": "Why are there only two teams left?", "system": {"davinci": {"answer": "The Super Bowl is the championship game of the National Football League (NFL) and the conclusion of the NFL playoffs. The game is played between the champions of the National Football League (NFL) and the champions of the American Football Conference (AFC). The winner of the game is awarded the Vince Lombardi Trophy.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "While it has a lot of outside information, this answer is at least consistent with the theme of the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "None of the information in the answer is from the article. The article does not go into detail as to how there are only two teams left.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There are only two teams left mainly because they were the only teams that pulled through the NFL punishing season. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is irrelevant and not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The question can be answered via info in 3 and 4, which note that this is the Super Bowl, the pinnacle of the game.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "There are only two teams left because the Super Bowl is the championship game of the NFL season, where the winners of the National Football Conference (NFC) and the American Football Conference (AFC) face each other to determine the overall league champion. Throughout the regular season and playoffs, other teams are eliminated until only the champions of each conference remain to compete in the Super Bowl.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 backs up the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the answer is correct, the article does not provide any of this information.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer appears to be factually accurate, but it was not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer draws on an outside source to answer the question as the document does not explain why there are only two teams left. Sentences 3 and 4 however imply that the super bowl is the pinnacle of the game of football and there are two teams involved in battling for the Lombardi Trophy at the event.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " The two teams left are the last two teams standing from a punishing NFL season.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "There is no other information that would answer the question, however, the answer itself basically just restates the question and does not actually answer it.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is correct and complete.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a concise, but valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer merely states that there are two left standing, it does not explain why there are only two. The question is not answerable as the document does not discuss this detail.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question because the passage doesn't mention why there are only two teams left.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The two teams left standing are the last two teams remaining from the NFL season. The NFL season is a long and punishing one, and only the two teams that have been able to make it through the entire season and reach the Super Bowl are left standing.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer includes all relevant info from the document and does its best to answer it", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is partially correct because the word \"long\" in the second sentence of the response is unsupported.", "ebr_raw": "80", "ebr_num": 80, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "There are only two teams left because it is a punishing NFL season and only two teams were able to scale through to the end.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}}}, "2": {"question_id": "2", "question": "Why is the Superbowl the biggest game in American sport?", "system": {"davinci": {"answer": "The Superbowl is the biggest game in American sport because it is the final game of the NFL season.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable inference that can be made from the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The question is not answerable given the info in the document. There would need to be information that explains the NFLs popularity in America. Such info is not present. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "gpt4": {"answer": "The Super Bowl is considered the biggest game in American sport mainly because of its immense cultural and social impact, as well as the large audience it attracts. It is the championship game of the NFL (National Football League), which is the most popular professional sports league in the United States. The Super Bowl also typically commands the highest television ratings of any sporting event in the country, attracts major advertising revenue, and often features high-profile musical performances during the halftime show. The event has become synonymous with American sports culture and is celebrated as a national spectacle.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on how well talked about the Super Bowl was in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the answer is correct, the article does not provide any of this information.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer was taken entirely from outside sources", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not provided in the document. There would need to be info in the document that speaks to the overall popularity of football compared to other sports in the US within the document, and that info is not present. As such, the question is not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The Super Bowl is the annual championship game of the National Football League (NFL), the highest level of professional American football in the United States. It is the culmination of a regular season that begins in the late summer of the previous calendar year.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct but contains information not contained or stated explicitly within the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention any of the facts mentioned in the answer. It does not explain why the game is the biggest in the US.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is mostly outside information, but it fits with the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not supported by any of the sentences in the document. The document does not address why the Superbowl is the biggest game in American sport. Sentence 4 says this is the pinnacle of the game, but there is no explanation of how that compares to other American sports.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is unsupported as the passage doesn't mention the reason.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The Super Bowl is the biggest game in American sport because it is the culmination of a long and punishing NFL season. It is the final game of the season and the winner is crowned the champion of the NFL. It is also one of the most watched sporting events in the world, with millions of viewers tuning in to watch the game each year.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is okay, but uses outside information not in the document.", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is valid, but it includes outside information", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention why the NFL is so popular. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The super bowl is one of the most watched game in America, and the season is always an interesting one. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}}}, "3": {"question_id": "3", "question": "Why is it significant that there are two Black quarterbacks battling it out for the trophey?", "system": {"davinci": {"answer": " The question is not about the significance of the two Black quarterbacks battling it out for the trophy. The question is about the significance of the fact that there are two Black quarterbacks battling it out for the trophy.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This incoherent answer made no attempt to cite facts from the article.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It's the first time two black quarterbacks have played each other in the history of NFL football.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is important because it helps bring about equality and representation of black people in the upper echelons of the sport. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is nonsensical and therefore not relevant. The info needed to answer the question comes in 4, which indicates that the NFL has struggled with equality and representation in the sport's top positions but now in the most important game there are two black players operating in the most important positions.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It missed the information in sentences 4, 5 and 7.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The presence of two Black quarterbacks battling it out for the Lombardi Trophy is significant because it represents a breakthrough in diversity, equality, and representation at the highest levels of American football, especially in the most important position on the field\u2014the quarterback. Historically, the NFL has struggled with these issues, and having two Black quarterbacks in the Super Bowl shatters stereotypes and serves as an inspiration for the next generation of aspiring Black athletes.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "It is an excellent answer, but it appears to have been written from outside information, and missed some key info in the article", "ebr_raw": "Score: 80. The response provides relevant information from the article, but it appears to have been supplemented with outside information, and missed some key details from the article.", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document provides a correct summary answer drawing on information from 3, 4, 5, 7, 8.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "It is a defining moment for the NFL, which has struggled with equality and representation in the upper echelons of the sport, as now finally at the very pinnacle of the game we have two black players operating in the most important position in the most important game there is. It is also a historic moment that gives hope of inspiring the next generation and showing that they can do it too.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is accurate and contains relevant information from the document, but misses some useful information too", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentences 5, 15, and 18 are used to craft the answer which provides a good overview answer to the question. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the reasons as per sentences 7 and 12.", "ebr_raw": "75", "ebr_num": 75, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "It is significant that there are two Black quarterbacks battling it out for the Lombardi Trophy because it is a landmark moment for the NFL, which has struggled with equality and representation in the upper echelons of the sport. This moment shows that black quarterbacks can have sustained success and can reach the pinnacle of the game. It is also a proud moment for both Mahomes and Hurts, as they are inspiring the next generation of black quarterbacks to follow their dreams and prove that they can do it too.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3-5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "4": {"question_id": "4", "question": "How has the NFL struggled with equality and representation?", "system": {"davinci": {"answer": "The NFL has struggled with equality and representation in the upper echelons of the sport, as now finally at the very pinnacle of the game we have two black players operating in the most important position in the most important game there is.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Key information is missing", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "In addition, sentence 25 states how and why the NFL has struggled with equality and representation.", "ebr_raw": "Score: 75", "ebr_num": 75, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question and missed the information in sentences 5 and 7.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not relevant as it merely repeats sentence 4. The info needed comes from sentences 7 (there is a stereotype that black quarterbacks cannot have sustained success), 13 and 14 (only seven black quarterbacks have previously made the big game with only two winning), and 23 (only 25 black quarterbacks have been first round picks)", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The NFL has struggled with equality and representation, particularly in regard to black quarterbacks. Historically, black players have been underrepresented in the quarterback position, often due to stereotypes and myths about their ability to lead a team and handle the demands of the role. Additionally, a lack of diversity among NFL owners and head coaches may have contributed to this underrepresentation. Though progress has been made in recent years, with more black quarterbacks being drafted and given opportunities to lead teams, this Super Bowl appearance of two black quarterbacks signifies a significant milestone in addressing the inequality and underrepresentation in the sport.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 25 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The final answer sentence is not relevant as it goes beyond the question's scope by talking about the meaning of the players in the super bowl. The info which comes before that sentence in the answer serves to answer the question, drawing on 25 for information.", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The NFL has struggled with a lack of diversity among owners and head coaches, which has led to a lack of trust and the myth building that a black QB could struggle with the leadership role needed to play the position.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and relevant and answers the question with info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer uses sentence 25 which is highly relevant. Info is missed from sentences 23 which gives stats on draft picks realted to black quarterbacks, sentence 21 which describes how black QBs positions are changed, and sentences 13-15 giving stats on superbowl black quarterbacks.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The NFL has struggled with equality and representation in the upper echelons of the sport by having a lack of diversity among NFL owners and head coaches, which has led to a lack of trust and the myth building that a black QB could struggle with the leadership role needed to play the position. This has resulted in only 25 black quarterbacks being first-round draft picks, and only six more black QBs making the Super Bowl since Doug Williams became the first in 1988.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is good and pretty complete, accurate and uses info from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7, 13,14 and 16 summarizes the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}}}}}, "5": {"question_id": "5", "question": "Why is quarterback the most important position in the game?", "system": {"davinci": {"answer": " The quarterback is the leader of the team. He is the one who calls the plays. He is the one who makes the decisions. He is the one who has to make the throws. He is the one who has to make the decisions. He is the one who has to make the throws. He is the one who has to make the decisions. He is the one who has to make the throws. He is the one who has to make the decisions. He is the one who has to", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is overly repetitive and badly written, but it at least fits with the theme of the article", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why the quarterback is the most important position.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is very repetitive and some of them are unsupported. It  missed the essence in sentences 35 and 43.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is supported by 29 and 30. The 2nd answer sentence is supported by 36. The rest of the answer is not drawn from the document and its repetitive elements are not relevant.", "ebr_raw": "Score: 55", "ebr_num": 55, "workerid": "6"}}}, "gpt4": {"answer": "The quarterback is considered the most important position in the game because they are responsible for leading the team, making key decisions, and executing offensive plays. This includes not only throwing the ball but also reading the defense, adjusting plays at the line of scrimmage, and making split-second decisions during the play. The quarterback is often the most visible and scrutinized player on the field, and their performance can have a significant impact on the outcome of the game.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the answer is correct, the article does not provide any of this information.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer, though comprehensive, was not taken at all from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Sentences 27, 29, 30 and 31 speak to the quarterback being the leaser of the team. The portion of the answer which indicates that the quarterback is the leader is correct. The rest of the information in the answer is not correct as it is not drawn from the document.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Quarterback is the most important position in the game because they are the ones who lead the team and make the decisions on the field. They are responsible for calling the plays, reading the defense, and making the throws that can win or lose the game.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The document doesn't really answer the question so the response doesn't miss anything, but it does use outside information to answer the question, even though it is accurate", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain the importance of the quarterback position. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is mostly outside information, but it fits with the article, and is the kind of background that a reader would already understand", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not backed up by any sentences in the document. it is never stated within the document why the quarterback is the most important position. There are various references to leadership that would be relevant (sentence 3, 25, 30, 35), but it is never stated that it is that aspect which makes the position the most important.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\"make the decisions on the field. They are responsible for calling the plays, reading the defense\" is unsupported by the passage.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Quarterback is the most important position in the game because they are the ones who lead the team on the field. They are responsible for making the decisions that will determine the outcome of the game, such as calling plays, reading defenses, and making accurate throws. They are also the face of the team, and their performance can have a huge impact on the team's success.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer does not miss anything relevant from the document but contains outside information not in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is a valid answer, but it is entirely outside information.  The article does not address this question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is partially correct because \"calling plays, reading defenses\" are not supported by the passage.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why the quarterback is so important.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 25 and 27 corresponds with the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "6": {"question_id": "6", "question": "How are they hoping these events inspire change?", "system": {"davinci": {"answer": "The answer is in the following sentence:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They hope kids will see how far they've come and if they can do it, the kids can do it.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 7 and 8 states that they hope more chances will be given to blacks to showcase their talents. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer fails to provide relevant information. The info needed to answer the question is contained in 8, 11, and 18, which note that it is is a chance to prove what was possible the whole time (8) and the events can inspire others and encourage them that they can do it too (11) and that demonstrating greatness can continue opening doors for other kids to follow their dreams to be quarterbacks too.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentences 7,8,11,15.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Mahomes and Hurts are hoping that their historic appearance as two black quarterbacks in the Super Bowl inspires change by showcasing their successes and breaking stereotypes. By playing at such a high level and being at the pinnacle of the sport, they aim to pave the way for the next generation of black players to be recognized and pursue their dreams of becoming successful quarterbacks in the NFL. They want to change perceptions about black quarterbacks' leadership abilities and inspire young athletes to believe they can achieve their goals, regardless of their race.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer summarizes the relevant information contained within 7, 8, 4, and 18 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " They are hoping that by breaking new ground and showing that black quarterbacks can be successful at the highest level, they will inspire the next generation of black quarterbacks to pursue their dreams and show that they can be successful.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "the answer is generally relevant and useful and taken from information in the document", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Info from sentences 11, 17, and 18 is relevant to answering the question, and the answer incorporates the info well. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "Mahomes and Hurts are hoping that their success in the Super Bowl will inspire a widespread change for the next generation of black quarterbacks. They are hoping that by showing that black quarterbacks can consistently be successful, it will open doors for other kids to follow their dreams and pursue a career in the NFL.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and comes from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5-9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "7": {"question_id": "7", "question": "Who are some other great Black quarterbacks who haven't been recognized?", "system": {"davinci": {"answer": " Doug Williams was the first Black quarterback to win a Super Bowl. He was the MVP of Super Bowl XXII. He was the first Black quarterback to start in a Super Bowl. He was the first Black quarterback to be drafted in the first round. He was the first Black quarterback to be named NFL MVP. He was the first Black quarterback to be named NFL Offensive Player of the Year. He was the first Black quarterback to be named NFL Comeback Player of the Year. He was the first Black", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article, though the answer includes some outside information", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The article only mentions three other black quarters: Marlin Briscoe, Eldridge Dickey, and Doug Williams.", "ebr_raw": "20", "ebr_num": 20, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not give names of the great Black quarterbacks who haven't been recognized", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Most parts of the response are unsupported and repetitive. It missed the information in sentences 14,20,21,22,41.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first 3 answer sentences are all supported by 13, 22 with the exception of the the naming of which super bowl Williams won his MVP. That was drawn from outside the document. The second sentence is not correct, as it was Marlin Brisco who was the first drafted in that spot (sentence 21). The rest of the info in the answer is not drawn from the document.", "ebr_raw": "Score: 63", "ebr_num": 63, "workerid": "6"}}}, "gpt4": {"answer": "The article does not mention specific names of other great Black quarterbacks who haven't been recognized. However, it highlights the journey of Black quarterbacks from Marlin Briscoe to Patrick Mahomes and Jalen Hurts, indicating that there have been others who faced challenges and stereotypes in the NFL but did not achieve wide recognition.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information in sentences 13,20,21,41.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Relevant information in the article was missed", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does in fact talk about a great black quarterback in 13 and 22 who was never recognized, Doug Williams.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Doug Williams was the first black quarterback to play in, and win, the Super Bowl back in 1988 with Washington. Since then, just six more black QBs have made the big game, with only Mahomes and Russell Wilson lifting the Lombardi Trophy. Marlin Briscoe is considered the first black QB of the modern era, having suited up for the Denver Broncos in 1968, with a decade passing until Williams became the first to be selected in the first round of a full modern-day NFL", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is relevant and sourced from the document but still misses some relevant info to further answer the question", "ebr_raw": "75", "ebr_num": 75, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Specific examples of those who are unrecognized is not given in the document. The answer lists a series of players, but it appears that these players have been recognized for their achievements in that they are remembered as record holders. To be unrecognized the players would have to be specifically named as such and the document fails to do that.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": " Some other great black quarterbacks who have not been recognized include Marlin Briscoe, Eldridge Dickey, Robert Griffin III, and Doug Williams. All of these players have made significant contributions to the NFL, but have not received the recognition they deserve.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and covers all the relevant info from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 20-22 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "8": {"question_id": "8", "question": "How long did Hurts train to reach the Super Bowl?", "system": {"davinci": {"answer": " He has been training since he was a child.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer was not taken from the text (which does not directly address this question, either)", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention how long Jalen Hurts has been training.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The machine response can be deduced from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not indicate the duration of time Hurts has spent training.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide information on how long Jalen Hurts trained to reach the Super Bowl.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not provide information on how long Hurts trained to reach the Super Bowl", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information, which the respondent acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document doesn't mention how long Jalen Hurts trained to reach the Super Bowl.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct, The info needed to answer the question is not contained within the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " It is not stated in the article how long Hurts trained to reach the Super Bowl.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct, the document does not specifically state the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "No more specific information is given in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the information is not given in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "It is not known how long Jalen Hurts trained to reach the Super Bowl, as his career path has not been publicly documented. However, Hurts has been playing football since high school and has been in the NFL since 2020, so it is likely that he has been training for many years to reach this point.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is generally good but contains some outside information not in the document.", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given, but the assumptions made in the latter part of the answer are not supported by the text", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is partially correct as only the first sentence is supported by the passage.", "ebr_raw": "Score: 18", "ebr_num": 18, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article doesn't mention when Jalen Hurts was in high school or even mention 2020.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct as the document did not specify the training duration of Hurts up to the Superbowl", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}}}, "14": {"dataset": "inq-ext", "article_num": 7, "question_details": {"0": {"question_id": "0", "question": "How is pay equity and lack of funding compromising their performances?", "system": {"davinci": {"answer": "The sentence is a bit confusing. I think the writer meant to say that the lack of funding and pay equity is compromising the team's performance.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question, but the article does not provide a real answer", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The disrespect by Canada Soccer is deeply frustrating to the players. Canada Soccer has also cut training camp days and full camp windows which is detrimental to their preparations. They've been told to make do with less. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer corrects grammar rather than giving an answer.  The info needed comes in 22, which says that training times were cut, which would impact preparations for the world cup.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not useful and contains none of the relevant information in the document ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The pay equity is making them feel frustrated and unmotivated to work.  ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "Pay equity and lack of funding can compromise the women's team performances by creating a demotivating work environment, limiting their access to necessary resources, such as proper training facilities and coaching, and preventing them from being able to prepare and compete at their best. Additionally, financial strain can lead to stress and distractions, further impacting their performances.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The description of events in the article corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The text does not directly address this question, but the answer uses the text to draw reasonable conclusions", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response contains a bit of speculation not directly from the article while missing a bit of relevant information", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Canada Soccer cut the training camp days and full camp windows which directly impacts the player's preparations.", "ebr_raw": "60. The response does mention that Canada Soccer cut training camp days and full camp windows, which impacts players' preparations, but it does not provide any further information about how this impacts the team or how the team is responding to the situation. Therefore, some important information from the document is missing.", "ebr_num": 60, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but most parts are unsupported. It missed the information in sentence 22.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The players are concerned that the lack of funding and pay equity issues will impact their ability to adequately prepare for the Women's World Cup, as Canada Soccer has cut training camp days and full camp windows. They also feel disrespected and mistreated by Canada Soccer, which has led to a feeling of frustration.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The whole of the document talks about how the cut will make them feel disrespected and frustrated", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and essentially correct, based on the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The players believe that the lack of funding and pay equity issues are compromising their performances because they are not receiving the support they need to adequately prepare for the Women's World Cup. This includes cuts to training camp days and full camp windows, which the players feel will negatively impact their ability to compete at the highest level.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The players have showed their concern that they are deeply outraged because of their funding cut, they cannot function in their full capacity because they feel disrespected and are not motivated ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "1": {"question_id": "0", "question": "How long have the Canadian athletes been dealing with this situation?", "system": {"davinci": {"answer": "The Canadian athletes have been dealing with this situation for a long time.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "There is no answer available in the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The team has been negotiating with Canada Soccer for more than a year.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is reasonable given the information contained in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They have been dealing with the situation for a very long time, and have been negotiating for more than a year. ", "ebr_raw": "25", "ebr_num": 25, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 3.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}, "gpt4": {"answer": "The Canadian athletes have been \"patiently negotiating\" with Canada Soccer for more than a year.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 23 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and sourced from the article ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the information in sentences 14 and 16.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}}}, "2": {"question_id": "1", "question": "How long does the team plan to go on strike? ", "system": {"davinci": {"answer": " The team is not going on strike. They are taking job action.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This is an incoherent answer that misses the relevant information in the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The team will not play, train, or attend meetings until there is resolution to the issues.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is making a semantic argument that is not a relevant response. The document does not give a set time timeline for the strike, rather saying in 6 that they will not be participating in any activities until the dispute is resolved.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is not helpful and misses useful information in the article.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article also suggests that the team is going in strike with the job action they want to take, and how long it will last is not stated in the article. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}}}, "gpt4": {"answer": "The team has not specified an exact duration for the strike, but they stated that they will not participate in any Canada Soccer activities until their concerns are resolved. They will not take part in any training, meetings, or games, which includes the SheBelieves Cup match against the United States on 17 February if the issues are not resolved by then.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is backed up by the information in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "As much information as is present in the article is included in the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and sourced from the article ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The team has not specified how long they plan to go on strike, but they have stated that they will not participate in any Canada Soccer activities until their demands are met.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant, answers the question and is consistent with the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The team has not specified how long they plan to go on strike, but they have stated that they will not participate in any Canada Soccer activities until their demands are met.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms that they intend to continue the strike until the issues is resolved", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "3": {"question_id": "1", "question": "How many times has the Canadian women's soccer team won gold at the Olympics?", "system": {"davinci": {"answer": "Canada has won gold at the Olympics twice. The first time was in 1996 in Atlanta, and the second time was in 2012 in London.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "This answer may be factually accurate, but it was not taken from the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They are sixth in the world rankings and won Olympic gold in 2021.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response appears to be incorrect and is unsupported by the article", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The Canadian women's soccer team won gold at the Olympics in 2021", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The Canadian women's soccer team has won gold at the Olympics once, in 2021.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears to be correct and consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "4": {"question_id": "2", "question": "How will the continued funding cuts affect the national team?", "system": {"davinci": {"answer": "The continued funding cuts will affect the national team in the following ways:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Canada Soccer has cut training camp days and full camp windows which is detrimental to their preparations. They've been told to make do with less. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and misses all useful details in the story", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The funding cuts will not make them train, they have decided not to attend meetings because they are not motivated. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentences 22 and 26.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The continued funding cuts could adversely affect the national team by impacting their training facilities, coaching staff, and overall development programs. This could potentially compromise their performances, World Cup preparations, and ability to compete effectively at an international level. The players might also feel frustrated and disrespected, which could affect their motivation and commitment to the team.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response provides answer that is supported by the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and a reasonable combination of supported facts and inferences", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response appears to be correct but the effect on \"coaching staff\" and \"their motivation and commitment to the team\" is not mentioned in the document.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}}}, "5": {"question_id": "2", "question": "What is the team hoping to achieve through the strike?", "system": {"davinci": {"answer": "The team is hoping to achieve equal pay and equal treatment.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The team is also wanting full funding for their program.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is supported by 2 and 12. There is missing info however from 12, which notes that they want their program and world cup preparations to be funded appropriately as well.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is essentially correct and consistent with the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "The team is hoping to achieve pay equity, fair treatment, and adequate funding for their program and World Cup preparations to ensure they can perform well and maintain their progress as a soccer nation. They are demanding immediate change and want to be treated equally and fairly by Canada Soccer.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 and 12 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and consistent with the facts in the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The team is hoping to achieve equal and fair treatment, and to have their program and World Cup preparations funded appropriately. They are also demanding immediate change from Canada Soccer.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on the article, the answer is correct, and is sourced from the document itself", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The team is hoping to achieve fair and equal treatment, and for a program that will give them a chance to achieve what they know the team is capable of achieving for Canada. They are also demanding immediate change and adequate funding for their World Cup preparations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 and 12 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "6": {"question_id": "3", "question": "How has Canada soccer failed to support its other national teams?", "system": {"davinci": {"answer": " The answer is simple. Canada soccer has failed to support its other national teams in the same way it has failed to support the women's national team.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer misses what little information is available in the article", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Canada Soccer doesn't give the team equal pay or adequately fund the program.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not align with the document which notes in 14 that the men's national team's dispute was over World Cup Prize Money.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is reasonable but could be more detailed and misses some relevant information", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "Canada Soccer has allegedly failed to support its other national teams by cutting funding, reducing training camp days and full camp windows, and not treating players fairly or equally. According to the players, this lack of support has led to frustration and has negatively impacted the teams' preparations for major tournaments like the Women's World Cup.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and essentially correct, and consistent with the article info", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Canada Soccer has cut training camp days and full camp windows, which would impact the team's World Cup preparations. They have also been told to \"make do with less\" and have been \"patiently negotiating\" with Canada Soccer for more than a year. The team also feels \"frustrated and deeply disrespected\" by the \"unacceptable burden\" placed on their shoulders.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 22-24 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is somewhat relevant but doesn't answer the question well, contains some mostly irrelevant info, and misses some relevant info about the other teams referenced in the question", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": " Canada Soccer has failed to support its other national teams by cutting training camp days and full camp windows, which would impact the team's World Cup preparations. They have also been accused of disrespecting the men's team in relation to negotiations around prize money for the 2022 World Cup in Qatar. Additionally, the players have stated that they have been \"patiently negotiating\" with Canada Soccer for more than a year but were being told to \"make do with less\".", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 30-34 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "7": {"question_id": "3", "question": "What do the athletes want as a result of their strike?", "system": {"davinci": {"answer": "They want Canada Soccer to change its policies and practices.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is technically correct, but it ignores details in the article.", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The women's team wants equal pay and adequate funding for their team.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is reasonable but misses some details", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 12 states what the athletes are demanding for. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported. It missed the information in sentence 12.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The athletes want to be treated equally and fairly, and they are demanding appropriate funding for their program and World Cup preparations. They seek immediate change in response to the reported funding cuts at the national body, which they believe will compromise their performances.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 and 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and complete, and uses information from the document to answer the question", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "8": {"question_id": "4", "question": "How are the athletes hoping the situation will change?", "system": {"davinci": {"answer": "They are hoping that the situation will change by the Canadian Soccer Association (CSA) to give them what they want.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is technically correct, but ignores all of the detail in the article", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The women's team wants equal pay and adequate funding for their team.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is correct but misses useful information in the article.", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the scenarios in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.Also the name of the association is incorrect as per sentence 10. It also missed the information in sentence 12.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The athletes are hoping the situation will change by being treated equally and fairly, and having their program and World Cup preparations funded appropriately. They want Canada Soccer to address the funding cuts and adjustments to training camp days and full camp windows that impact their performance and preparations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears relevant and consistent with the article facts", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "9": {"question_id": "4", "question": "What is the SheBelieves Cup?", "system": {"davinci": {"answer": "The SheBelieves Cup is a four-team invitational tournament hosted by the United States women's national soccer team. The tournament is held in the United States and features the United States, England, France, and Germany. The United States is the defending champion.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer may be valid, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't give any background on the SheBelieves Cup.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document fails to define what the event is which makes the question not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer uses outside information or speculation though it may be correct, and the actual answer isn't in the document", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no detailed information in the article about what the SheBelieves cup represents. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The SheBelieves Cup is an annual women's soccer tournament featuring national teams from different countries. It was established in 2016 and generally takes place in the United States. The tournament invites top-level teams, such as the U.S., as a showcase of competitive matches in the lead-up to major international tournaments like the World Cup and the Olympics.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although there is no detailed information about what the SheBelieves Cup is in the article, but I presume that the machine response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This appears to be an excellent, and factually accurate, answer, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response may be correct but utilizes outside information", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not give any background on SheBelieves.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but  is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The SheBelieves Cup is an international women's soccer tournament held annually in the United States. It was first held in 2016 and features some of the top women's national teams in the world.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document only made mention of the SheBelieves Cup in sentence 4 but did not further give detailed explanation of what this cup is. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The only information given on SheBelieves is that Canada is set to face the US on February 17. It does not say when it was created or about the rankings of its participants.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is relevant and likely correct but uses outside information not in the article itself", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is apparently valid, but it has a lot of outside information", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer appears to be valid, but it is not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The SheBelieves Cup is an international women's soccer tournament held annually in the United States. It features some of the top teams in the world, including the United States, Canada, England, Brazil, and Japan. The tournament was founded in 2016 and is hosted by the U.S. Soccer Federation.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer appears to be valid, but it is not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain anything about the SheBelieves Cup. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not give a full details of what the SheBelieves cup is", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer is apparently accurate, but it is not taken from the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain anything about the SheBelieves Cup. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}}}}}, "10": {"question_id": "5", "question": "How does the team hope the situation will be resolved?", "system": {"davinci": {"answer": "The team is not sure how the situation will be resolved. They are hoping that the situation will be resolved by Canada Soccer.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The key information from the article is ignored", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first answer sentence is not relevant, as the question is asking what they hope will happen. The second answer sentence is supported by 11-13. 11 also contains relevant information that should be included in the answer as it indicates that they are wanting immediate change.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not very helpful and misses information in the article", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "The team hopes the situation will be resolved by achieving pay equity, getting fair and equal treatment, and receiving appropriate funding for their program and World Cup preparations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and correct based on the article information. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The players are demanding immediate change and expect and deserve nothing less than to be treated equally and fairly and to have their program - and their World Cup preparations - funded appropriately. They are hoping that Canada Soccer will meet their demands and provide them with the necessary funding and support.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "In the document, the teams have laid down what they want and hoping that Canada Soccer grants their request.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and relevant and answers the question with info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The team hopes that Canada Soccer will provide them with equal and fair treatment, as well as appropriate funding for their World Cup preparations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "11": {"question_id": "6", "question": "How long have the Canadian athletes been dealing with this situation?", "system": {"text-davinci-003_few_shot": {"answer": "The players have been \"patiently negotiating\" with Canada Soccer for more than a year but were being told to \"make do with less\". The men's team went on strike last year in a dispute over World Cup prize money.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 23 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is essentially correct and relevant, using info from the article to answer the question", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The Canadian athletes have been negotiating with Canada Soccer for more than a year, but have been told to \"make do with less\".", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 23 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "12": {"question_id": "7", "question": "How many times has the Canadian women's soccer team won gold at the Olympics?", "system": {"text-davinci-003_few_shot": {"answer": " The Canadian women's soccer team has won gold at the Olympics once, in 2021.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on the document, the answer is correct and relevant, and there isn't any additional info given that should need to be added.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The Canadian women's soccer team has won gold at the Olympics once, in 2021.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 affirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "13": {"question_id": "8", "question": "How will the continued funding cuts affect the national team?", "system": {"text-davinci-003_few_shot": {"answer": "The funding cuts will impact the team's World Cup preparations, as they have been cut training camp days and full camp windows. This lack of support threatens to reverse the progress the team has made and could send them back to obscurity.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "It will affect their operations as they have refused to partake in any activities", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is good, helpful, relevant, accurate and based on the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The continued funding cuts will affect the national team by compromising their performances and limiting their ability to adequately prepare for the Women's World Cup. The players are concerned that the lack of funding and resources will prevent them from achieving their full potential and competing at the highest level.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 12 confirms the machine response, the team is not motivated to partake in any competition.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "14": {"question_id": "9", "question": "What do the athletes want as a result of their strike?", "system": {"text-davinci-003_few_shot": {"answer": "The athletes are demanding immediate change from Canada Soccer, including equal and fair treatment and appropriate funding for their program and World Cup preparations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is clear and concise, accurate and uses info from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid, if concise, answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The athletes want Canada Soccer to provide them with equal and fair treatment, as well as appropriate funding for their World Cup preparations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "15": {"question_id": "10", "question": "How are the athletes hoping the situation will change?", "system": {"text-davinci-003_few_shot": {"answer": "The athletes are hoping that Canada Soccer will provide them with equal and fair treatment and appropriate funding for their program and World Cup preparations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant, answers the question and is consistent with the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "text-davinci-003_zero_shot": {"answer": "The athletes are hoping that Canada Soccer will provide them with equal and fair treatment, as well as appropriate funding for their World Cup preparations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer to a duplicate question", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The athletes are hoping that they will be given equal and fair treatment according to sentence 12.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is mostly redundant, but it is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}}}, "15": {"dataset": "inq-ext", "article_num": 8, "question_details": {"0": {"question_id": "0", "question": "How are the women detected for not coming to Argentina for tourist activities?", "system": {"davinci": {"answer": " The women are detected by the immigration officers at the airport. The immigration officers are trained to detect people who are not tourists.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is at least partly incorrect and misses important info in the article.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fairly reasonable assumption to make, based on the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The officers found problems with some women's documentation.", "ebr_raw": "75", "ebr_num": 75, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some of them carry fake documents.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not supported by the document. The document never speaks to the training of the immigration officers. It is not stated in the document how they were detected. Sentence 7 notes that the women acknowledged it themselves, but it is not clear if that was after they were detected or if that was the method of detection.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide specific details about how the women were detected for not coming to Argentina for tourist activities.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The anser correctly identifies that the info needed to answer the question is not present in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "At least a partial answer could have been taken from the article", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It was detected through their fake documents and they also acknowledged themselves. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. It missed the information in sentences 5 and 7  in the document.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}}}, "1": {"question_id": "0", "question": "Why are pregnant Russian women going to Argentina?", "system": {"davinci": {"answer": "The answer is simple.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The women are coming to Argentina to have their babies so the babies will be Argentenian and have a passport. This speeds up the process for the parents to become citizens.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and misses all the relevant information contained in the article ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 3 states the reason why pregnant Russian women are going to Argentina", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentence 3.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Pregnant Russian women are going to Argentina to ensure their babies are born there and obtain Argentinian citizenship. This is because an Argentinian passport provides more freedom, allowing holders to enter 171 countries visa-free, while Russians can only travel visa-free to 87 countries. Additionally, having an Argentine child speeds up the citizenship process for the parents.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8, 11, and 13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears to be correct and complete and based on article information.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " It is believed the women want to make sure their babies are born in Argentina to obtain Argentinian citizenship. Having an Argentine child also speeds up the citizenship process for parents. It is also suggested that they are fleeing the war in Ukraine and attracted by the right of visa-free entry to Argentina, as well as by the high-quality medicine and variety of hospitals.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but it could have had more detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentences 17, 3, and 12 are used to appropriately answer the question.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "It is believed that the pregnant Russian women are going to Argentina to make sure their babies are born in Argentina to obtain Argentinian citizenship. Having an Argentine child also speeds up the citizenship process for parents, as it allows passport-holders to enter 171 countries visa-free, compared to the 87 countries Russians can travel to visa-free. Additionally, it is suggested that the increase in arrivals is a result of the war in Ukraine.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence of the answer uses relevant info from doc. sentence 3. The 1st part of the second answer sentence is relevant as it indicates a reason why pregnant women would want to go to Argentina, but the second part of the sentence is more a follow up of that introduced info, not really answering the question at hand making it not relevant. The final sentence uses info from sentence 17, but misses the part about high quality medicine and variety of hospitals.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "2": {"question_id": "0", "question": "Why has the war in Ukraine caused more pregnant Russian women to seek Argentinian citizenship for their babies?", "system": {"davinci": {"answer": "The answer is simple.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not provide any information despite saying the answer is simple. The info needed to answer the question comes in 17, which indicates that they are fleeing the war and have the right of visa fee entry to Argentina.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Having an Argentinian child speeds up the process for the parents to become citizens. An Argentian passport is allows the holders to enter many more countries than a Russian passport.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is bad and misses all relevant info", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The above sentences suggests that the Argentina citizenship gave more freedom than a Russian passport. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The article does not provide a direct explanation for why the war in Ukraine has caused more pregnant Russian women to seek Argentinian citizenship for their babies. However, it does mention that these women are fleeing the war and their country's health service, and are attracted by visa-free entry to Argentina, high-quality medicine, and variety of hospitals in the country. It is possible that the war has created a sense of instability or uncertainty in Russia, leading these women to seek more secure or stable futures for their children by obtaining Argentinian citizenship.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 17 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is overall relevant and correct, and consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct in its identification of the specific information not being given in the document. The first part of the second answer sentence which notes \"However, it does mention that these women are fleeing the war\" is correct as well. The info which follows in that answer sentence is not relevant however as it talks about reasons why they would leave other than the war. The final answer sentence could require outside information to construct.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}}}}}, "3": {"question_id": "1", "question": "How does the Argentinian passport give more freedom than a Russian passport?", "system": {"davinci": {"answer": "The Argentinian passport gives visa-free entry to 171 countries. The Russian passport gives visa-free entry to 87 countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and sourced from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 and 13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by 11 and 13.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "gpt4": {"answer": "An Argentinian passport gives more freedom than a Russian passport because it allows passport holders to enter 171 countries visa-free, whereas Russians can travel visa-free to only 87 countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by 11 and 13.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 and 13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "4": {"question_id": "1", "question": "How has the Argentine government responded to the influx of pregnant Russian women?", "system": {"davinci": {"answer": "The Argentine government has responded by cracking down on the practice.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They are detaining women who enter the country under false pretenses and carry out raids on companies that help the women with fake documents and other services.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "This is a technically-correct answer, but it was not citing any text", "ebr_raw": "30", "ebr_num": 30, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response seems reasonable based on the article but misses some info", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not state how the Argentine government responded to the influx of pregnant Russian women", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is not specific enough and missed the information in sentences 5,23,24,25.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}, "gpt4": {"answer": "The Argentine government has detained some pregnant Russian women who arrived in Argentina due to \"problems with their documentation.\" Officials have also investigated a possible \"million-dollar business and illicit network\" reportedly providing pregnant Russian women and their partners with fake documents. Argentinian police have conducted raids to seize laptops, tablets, immigration papers, and cash in connection to this network.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 and 23 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is complete and correct and sourced from the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " The national migration agency has detained some of the women due to \"problems with their documentation\". The Argentine police have also been carrying out raids as part of an investigation into a \"million-dollar business and illicit network\" that allegedly provided pregnant Russian women and their partners with fake documents.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The machine response answered the question correctly but missed some relevant information. The machine response could clarify what the fake documents were used for (\"allow them to settle in Argentina\") as well as the exorbitant cost for the service ($35,000) which hints at its illicit nature and gives context for why the police are involved.", "ebr_raw": "65", "ebr_num": 65, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Answer sentence 1 correctly notes that some women were detained, but the agency which did the detaining was not specified. Answer sentence 2 correctly uses info from doc. sentence 23", "ebr_raw": "95", "ebr_num": 95, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The Argentine government has responded to the influx of pregnant Russian women by detaining some of them on suspicion of being \"false tourists\". Three women were detained on Thursday due to \"problems with their documentation\", and three more were detained the previous day. The government is also carrying out raids as part of an investigation into a \"million-dollar business and illicit network\" that allegedly provided pregnant Russian women and their partners with fake documents.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Doc. sentence 5 is correctly used in answer sentence 1. It should be noted in the answer however that they were released (sentence 16) as that was part of the government's action in the matter. The second answer sentence is not relevant to the question as it is essentially repeating the info in the sentence immediately before it. The final sentence of the answer accurately uses info from sentence 23.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "5": {"question_id": "1", "question": "What kinds of problems with documentation do the pregnant women face?", "system": {"davinci": {"answer": "The pregnant women are detained because they are suspected of being \"false tourists\".", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer notes why they were detained, but does not speak to the documentation issue the question asks about. That info comes in sentence 6, which indicates that they initially claimed to be tourists, but it was detected that they did not come to engage in tourism activities. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "This answer misrepresents the article text", "ebr_raw": "20", "ebr_num": 20, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and consistent with the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 23 states that the women are issued false documents that allows them to settle in Argentina. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "gpt4": {"answer": "The article does not provide specific details about the documentation problems the pregnant Russian women faced. It only mentions that three were detained due to \"problems with their documentation.\"", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "There is an illicit business network that issues the women fake documents.", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the limited information available in text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in noting that the document does not state what the problems are.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}, "6": {"question_id": "2", "question": "How are the pregnant women able to travel to Argentina?", "system": {"davinci": {"answer": "The pregnant women are able to travel to Argentina because they are not pregnant when they arrive in Argentina.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Pregnant Russian women can buy a package from company that offers them fake documents along with other services. The women then say they are there as a tourist so they don't get deported for being there to give birth.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to use the text to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not useful and misses all important info  ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The above sentences provides answers to how the pregnant women are able to travel to Argentina. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 6,19,20,21,22.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The pregnant women are able to travel to Argentina because Russians have the right to enter Argentina visa-free.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be assumed to be true because Russian people will only be able to enter Argentina with their Russian passport.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 6.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and all facts are supported by the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The Russian women had initially claimed they were visiting Argentina as tourists, and they are able to travel visa-free to Argentina.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 and 17 are used to craft the answer.", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The pregnant women are able to travel to Argentina by obtaining Argentinian citizenship for their babies. They are also attracted by the right of visa-free entry to Argentina, as well as by the high-quality medicine and variety of hospitals. Additionally, some of the women may have obtained fake documents issued in record time to allow them to settle in Argentina.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The only info in the answer relevant to answer the question comes from sentence 17 which notes that they have the right to visa free entry into Argentina. That is the reason they can travel to Argentina.", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer does not address the \"How\" question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not explain HOW the women were able to travel to Argentina.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer misinterpreted the question, and gave a false impression of how the initial travel is made", "ebr_raw": "20", "ebr_num": 20, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer doesn't explain HOW the women were able to travel to Argentina, only why they would. The part about getting fake documents is not mentioned as happening in the article only that it's available.", "ebr_raw": "15", "ebr_num": 15, "workerid": "3"}}}}}, "7": {"question_id": "2", "question": "How common is it for women to be stopped at the Argentinian border when trying to enter?", "system": {"davinci": {"answer": " I have no idea. I have never heard of it happening before.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as there are stats regarding commonality given in sentence 5. It is stated there that 3 out of 33 were stopped. Broader stats were not given.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the article does not mention how common it is to be for a Russian woman to be stopped, it does mention that  the arrivals have increased. There have been over 5,000 have entered Argentina in recent months. There were 3 women stopped on one flight that 33 pregnant Russian women were on.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article does not provide an answer to this question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is not useful, while the article doesn't provide a specific answer", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no mention of women been stopped at the Argentina border in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The article does not provide specific information on how common it is for women to be stopped at the Argentinian border when trying to enter. However, it does mention that three women were detained on one flight, joining three more who arrived the previous day, due to problems with their documentation. The increase in pregnant Russian women arriving in Argentina might lead to more cases of border authorities stopping and checking their documentation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is overall correct and consistent with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the info needed is not present in the document. It also mentions the relevant info in 4 and 5 appropriately.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}, "8": {"question_id": "2", "question": "How could the mothers also obtain citizenship? ", "system": {"davinci": {"answer": "The mothers could obtain citizenship by naturalization.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response may be correct but it isn't directly addressed in the article", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer is not taken directly from the text, but it is a reasonable inference to make", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "If their child has Argentine citizenship, it speeds up the process for their parents to become citizens.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "They could obtain citizenship through their children.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document never uses the term naturalization. Sentence 12 does contain relevant info however as it notes that having an Argentine child speeds up the citizenship process for parents.", "ebr_raw": "Score: 0.", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Mothers can obtain citizenship more quickly if their child has Argentine citizenship.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 12.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "9": {"question_id": "3", "question": "Why do the Russian women travel to Argentina during their final weeks of pregnancy?", "system": {"davinci": {"answer": "The answer is simple.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The women want their babies to born in Argentina, making them Argentinian citizens and speeding up the process for the parents to become citizens.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and misses all useful details in the story", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They travel so that they can make sure their babies are born in Argentina to obtain Argentinian citizenship", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentence 3.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Russian women travel to Argentina during their final weeks of pregnancy so that their babies can be born there and obtain Argentinian citizenship, which provides more freedom and travel benefits compared to a Russian passport.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 and 9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 12.", "ebr_raw": "75", "ebr_num": 75, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is accurate and mostly complete but misses some relevant info", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " It is believed the women want to make sure their babies are born in Argentina to obtain Argentinian citizenship. Having an Argentine child also speeds up the citizenship process for parents. As it stands, Russians can travel visa-free to only 87 countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but it is missing a key fact", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The machine response answered the question correctly but missed some relevant information. It would be useful to include the fact that Argentinian passport holders can enter 171 countries visa-free since the machine response mentions Russians can enter only 87 countries visa-free. Including that fact in the machine response without additional context does would not make much sense.", "ebr_raw": "80", "ebr_num": 80, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Info from doc. sentence 3 is used to appropriately make the first sentence of the answer. The second sentence draws from doc. sentence 12. The final answer sentence is not relevant as the question is not asking how they are able to travel there. Info from sentence 17 which discusses fleeing the war and getting access to better health care.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "It is believed the women want to make sure their babies are born in Argentina to obtain Argentinian citizenship. Having an Argentine child also speeds up the citizenship process for parents. As it stands, Russians can travel visa-free to only 87 countries, whereas an Argentinian passport allows passport-holders to enter 171 countries visa-free. Additionally, the website seen by the BBC offers various packages for expecting mothers who wish to give birth in Argentina, ranging from \"economy class\" to \"first class\".", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first two sentences in the answer are supported by doc. sentences 3 and 12. The 3rd answer sentence is not relevant as it is speaking about visa rules and does not address the question. The 4th sentence speaks more to how they travel vs why they would travel to Argentina. Info from sentence 17 is missing that notes the war in Ukraine and the high quality medical care/variety of hospitals which all serve as additional reasons", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The question is answered sufficiently, though some additonal information was missed", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some information was missing", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}}}}}, "10": {"question_id": "3", "question": "Why do the women need to lie about why they are entering Argentina?", "system": {"davinci": {"answer": " The answer is simple. If they say they are coming to give birth, they will be detained and deported. If they say they are coming to visit, they will be allowed to enter.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as it contradicts what is in the document. Sentences 13-15 note that what they did was not a crime, and that there is no things as false tourists in the legislation of Argentina. As such, there is no reason to lie.as sentence 17 indicates that they have the right of visa free entry into Argentina.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article, though it is unclear if a real crime is being committed.", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and consistent with the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based in how the events is described in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "gpt4": {"answer": "The women need to lie about why they are entering Argentina because their actual intention is to give birth in Argentina to obtain Argentinian citizenship for their babies, which is not a valid reason for entering the country as tourists. If they reveal their true intentions, they might face issues related to immigration as they could be considered for \"birth tourism\" purposes rather than genuine tourism.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and fairly complete, consistent with the article", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not align with the information in the document. 13-15 indicate that what the women were doing was not a crime. Further, 17 indicates that they have the right to visa free entry into Argentina. As such, there is no need to lie.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "11": {"question_id": "3", "question": "Why do the women not stay in Argentina?", "system": {"davinci": {"answer": " The answer is simple. They are not interested in staying in Argentina. They are interested in getting their children Argentinian citizenship.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and consistent with the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a simplified but valid answer, though it could have included more detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document never addresses why specifically they leave. The information about wanting their children to have Argentinian citizenship is supported by 8, but that explains why they come, not why they then leave.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not explicitly state why the women do not stay in Argentina, but it can be inferred that they return to their home country after obtaining Argentinean citizenship for their children, as it provides more freedom than a Russian passport, allowing visa-free entry to 171 countries.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The portion of the answer that indicates the information needed is not present in the document is correct. The inference however talks about the reason why they came to Argentina, not why they left and it is therefore not relevant. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, given the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 6 in the document, the women claimed themselves as tourists in Argentina.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "12": {"question_id": "4", "question": "How do the women pretend to be tourists?", "system": {"davinci": {"answer": "They buy a ticket to Argentina, and then they buy a ticket to another country, and then they buy a ticket to Argentina again.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not elaborate on how they pretend to be tourists, making the question not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Pregnant Russian women can buy a package from company that offers them fake documents along with other services. The women then say they are there as a tourist so they don't get deported for being there to give birth.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer is incoherent, but the article does not provide any information about how women pretend to be tourists", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is wrong, while the document provides no further details.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They acknowledge it themselves that they are tourists. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "gpt4": {"answer": "The Russian women initially claimed they were visiting Argentina as tourists and probably engaged in tourist activities or followed tourist itineraries to give the appearance of being ordinary tourists. However, they were actually in Argentina to give birth and obtain Argentinian citizenship for their babies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "They get fake documents and claim that they are tourists. ", "ebr_raw": "50. The response includes relevant information from the article but does not provide any context or additional information. The sentence \"They get fake documents and claim that they are tourists\" is directly taken from the article at sentence number 6.", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable given the article information", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not state how the women pretended to be tourists. As such, the question is not answerable. The speculation in the answer is not supported by any of the document's sentences. Further, mentioning the actual reason they were in Argentina (to have their babies)  goes beyond the scope of the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "13": {"question_id": "4", "question": "How has obtaining Argentinian citizenship and leaving the country become a problem? ", "system": {"davinci": {"answer": " The reader is asking how the Argentinian government can stop the Russian women from leaving the country with their children.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not useful, and the question is not addressed directly.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question, but the article does not describe why it is a problem for Argentina", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't go into detail why the women signing up their children as Argentinean and then leaving is a problem.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not provide detailed information to answer the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not provide information that is relevant to answering the question, as it is summarizing what it thinks the question is asking. The document does not explain why leaving after getting citizenship for the children is a problem.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Obtaining Argentinian citizenship and leaving the country has become a problem because it seems that pregnant Russian women are entering Argentina with the sole purpose of giving birth to children who will have Argentinian citizenship, which comes with greater freedom than a Russian passport. This exploit of Argentina's nationality laws can lead to abuse of the system and may create difficulties for genuine tourists or immigrants.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first answer sentence is repeating the action that is alleged to be a problem. This info is already known and it is therefore not relevant to mention it again. The info which follows in the answer is not drawn from the document. The document does not provide info that explains why the situation presented is a problem.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The text does not directly address this question, but the answer uses the text to draw reasonable conclusions", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The high arte of pregnant women entering into Argentina has become worrisome according to information in the article, ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last part of the response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}}}, "14": {"question_id": "4", "question": "Why do the women want their babies to have Argentinian citizenship?", "system": {"davinci": {"answer": "The answer is simple.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Having an Argentinian child speeds up the process for the parents to become citizens. An Argentian passport is allows the holders to enter many more countries than a Russian passport.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and misses all the relevant information contained in the article ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 8 provides answer to the question. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentences 8,10,11,12.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The women want their babies to have Argentinian citizenship because it offers more freedom than a Russian passport. Argentinian passport holders can enter 171 countries visa-free, whereas Russians can only travel visa-free to 87 countries. Furthermore, having an Argentine child also speeds up the citizenship process for the parents.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8, 11, and 13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 17.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is pretty good, but misses some information that is relevant.", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "Having an Argentine child speeds up the citizenship process for parents. As it stands, Russians can travel visa-free to only 87 countries. Argentine passport-holders can enter 171 countries visa-free.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Info from Sentence 8 should be used in the final answer sentence as it is more relevant than the info from sentences 11 and 13 which is currently there. That info explains why Argentinian citizenship  gives more freedom.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The women want their babies to have Argentinian citizenship because it gives them more freedom than a Russian passport. Having an Argentine child also speeds up the citizenship process for parents. As it stands, Russians can travel visa-free to only 87 countries, whereas an Argentinian passport-holder can enter 171 countries visa-free.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 of the answer makes use of doc. sentence 8. The second answer sentence is supported by doc. sentence 12. The third answer sentence uses doc. sentences 11 and 13. This does provide a reason showing why Argentinian citizenship could be desired in this situation.  ", "ebr_raw": "85", "ebr_num": 85, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "15": {"question_id": "5", "question": "How has the Argentine government responded to the Russian women wanting citizenship?", "system": {"davinci": {"answer": " The Argentine government has responded by saying that it is not illegal to give birth in Argentina.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Other than detaining women for entering the country under false pretenses, it does not mention any other response to the women.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is not correct and misses relevant details in the article", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "How the Argentine government responded is not stated in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 5,23,24,25.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The Argentine government has been detaining some Russian women with \"problems with their documentation\" as they are suspected of being \"false tourists.\" Although the women were released, Argentinian police have been carrying out raids and investigating a \"million-dollar business and illicit network\" that allegedly provided pregnant Russian women and their partners with fake documents issued in record time, enabling them to settle in Argentina.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 and 23 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentences 24 and 25.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and all facts are supported by the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " The national migration agency has detained some of the women due to \"problems with their documentation\". The lawyer for the three women who were detained said that they are being \"falsely imprisoned\". The women have since been released. The Argentinian police has been carrying out raids as part of an investigation into a \"million-dollar business and illicit network\" that allegedly provided pregnant Russian women and their partners with fake documents issued in record time to allow them to settle in Argentina.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The final answer sentence correctly uses info from sentence 23. The first answer sentence is correct in that some women were detailed, but it is never specified which agency did the detaining. The info about the lawyers statement in answer sentence 2 is not relevant as that is not a government response. The info in sentence 3 comes from doc. sentence 16 is relevant as it relates to the government.", "ebr_raw": "Score: 73", "ebr_num": 73, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The Argentine government has allowed the Russian women to enter the country and has not taken any action to prevent them from obtaining citizenship for their babies. However, the government has recently begun to investigate a \"million-dollar business and illicit network\" that allegedly provided pregnant Russian women and their partners with fake documents issued in record time to allow them to settle in Argentina. Police said the gang charged up to $35,000 (\u00a329,011) for the service. No arrests were made, but police were said", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first section of the answer is incorrect in that doc. sentence 5 notes that six women were detained with sentence 16 indicates that they were released. There was no mention of either taking or refraining from actions to prevent citizenship for the babies. Moving to the second part of the answer, the info from sentence 23 is relevant but that which follows is not as that info serves as follow up information to what was in sentence 23 and does not directly related to the question asked.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer was cut off, and could have included more information from an earlier section", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is valid, but cut off", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "16": {"question_id": "5", "question": "How have Argentinians responded to this situation?", "system": {"davinci": {"answer": " Argentinians are not happy with this situation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on the totality of information in the article, the answer is reasonable.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not quote any members of the public, but other relevant information was still missed", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Argentina's are skeptical about the influx of the migration into the country. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 5 lists one particular Argentinian (the immigration official) who is not happy with the situation, but 14 lists another (the women's attorney) who thinks their treatment is wrong. The overall opinions felt by regular people are not discussed within the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide information on how Argentinians have responded to this situation of Russian women coming to Argentina to give birth.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question appears to be looking to find out about the opinions or regular people in Argentina. The answer is correct in identifying that this info is not present in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not give information about how the people of Argentina have responded", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. It missed the information in sentences 23,24,25 in the document.", "ebr_raw": "30", "ebr_num": 30, "workerid": "1"}}}}}, "17": {"question_id": "6", "question": "Why has the war in Ukraine caused more pregnant Russian women to seek Argentinian citizenship for their babies?", "system": {"text-davinci-003_few_shot": {"answer": "Having an Argentine child speeds up the citizenship process for parents. As it stands, Russians can travel visa-free to only 87 countries, while Argentinian passport-holders can enter 171 countries visa-free. Additionally, the Argentinian passport is seen as more secure than a Russian passport.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The way the question is written doesn't directly address the question, but it includes the relevant information", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Machine response did not directly answer the question and did not focus on the crux of the issue -- the war in Ukraine. Line 17 in the article provides an explanation, namely that Russians want to flee war. This was not mentioned in the machine reseponse.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "None of the info in the answer is related to the war, rather the answer is just giving general reasons. There is no info that would address the question in the document except for the brief mention of \"fleeing the war\" in sentence 17.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The war in Ukraine has caused more pregnant Russian women to seek Argentinian citizenship for their babies because it gives them more freedom than a Russian passport. Having an Argentine child also speeds up the citizenship process for parents, as Russians can travel visa-free to only 87 countries, whereas an Argentinian passport allows them to enter 171 countries visa-free. Additionally, the high-quality medicine and variety of hospitals in Argentina may be attractive to pregnant Russian women.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "None of the info in the answer addresses specifically why the war has caused an increase. All the reasons listed are just general reasons. The document does not contain any further information either.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "All necessary information is included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "18": {"question_id": "7", "question": "What kinds of problems with documentation do the pregnant women face?", "system": {"text-davinci-003_few_shot": {"answer": " The women had initially claimed they were visiting Argentina as tourists, but the migration agency head Florencia Carignano said that it was detected that they did not come here to engage in tourism activities. They acknowledged it themselves.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some information was missed", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentences 6 and 7 are used to appropriately answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The pregnant women face problems with their documentation such as not having the correct visas or not having the necessary paperwork to prove their identity. Additionally, they may not have the necessary documents to prove that they are eligible to stay in Argentina.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "None of the info in the answer was stated within the document. The only info available was in sentence 6, which noted that it was detected they did not come to engage in tourism, but there was no more info regarding documentation there.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is apparently valid, but it has a lot of outside information", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention any of the problems indicated in the answer. The only problem mentioned is the women lying about being there for tourism.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}}}}}, "19": {"question_id": "8", "question": "How common is it for women to be stopped at the Argentinian border when trying to enter?", "system": {"text-davinci-003_few_shot": {"answer": " It is not clear how common it is for women to be stopped at the Argentinian border when trying to enter. The article states that three women were detained out of 33 who arrived on one flight on Thursday, and three more were detained from the previous day. It is not clear if this is a common occurrence or not.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response correctly identified that the article did not have the relevant information to fully answer the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "0"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that it is not stated. Info from sentence 5 is used as well.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "It is not common for women to be stopped at the Argentinian border when trying to enter. However, in this case, the three women were detained due to problems with their documentation. According to the national migration agency, the number of arrivals has increased recently, which could suggest that more women are being stopped and questioned at the border.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info is not given within the document making the question not answerable. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "20": {"question_id": "9", "question": "Why do the women need to lie about why they are entering Argentina?", "system": {"text-davinci-003_few_shot": {"answer": " It is believed the women want to make sure their babies are born in Argentina to obtain Argentinian citizenship. Having an Argentine child also speeds up the citizenship process for parents. As it stands, Russians can travel visa-free to only 87 countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but it is missing a key fact", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "It is not legal to travel to Argentina only to get citizenship for a child. ", "ebr_raw": "10", "ebr_num": 10, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The machine response answered the question correctly but missed some relevant information since it included the fact that Russians can enter 87 countries visa-free. It would be helpful to include the fact that Argentinians can enter 171 countries visa-free, as stated in the article, as a point of comparison since it is relevant to the question.", "ebr_raw": "80", "ebr_num": 80, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated why they felt the need to lie. Sentence 15 indicates they did not commit a crime and sentence 16 indicates that they were released.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The women need to lie about why they are entering Argentina because they are actually coming to the country to give birth so that their children can obtain Argentinian citizenship. This is a form of \"birth tourism\" which is not allowed under Argentinian law. By claiming they are visiting as tourists, the women are able to enter the country without raising suspicion.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated within the document why they felt the need to lie. Sentence 17 indicates that they had the right to free entry into Argentina.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "21": {"question_id": "10", "question": "How do the women pretend to be tourists?", "system": {"text-davinci-003_few_shot": {"answer": "The women had obtained fake documents issued in record time to allow them to settle in Argentina. They had also signed up their children as Argentinean and left.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The women did nothing else but claim to be there for tourism activities.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Machine response added information not present in the article. It is not clear if all the Russian women had utilized fake documents as stated at the end of the article. Additionally, the machine response states that the women had signed up their children as Argentinian. This is not possible since the women the article was talking about were just recently caught and were still detained.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer info is not correct in that these things were not ascribed to have been done by these women. It was not stated what was done to pretend that they were tourists beyond claiming they were tourists in sentence 6.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They get papers from the birth tourism website and say they are tourists instead of there to give birth.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "It is unclear how the women are pretending to be tourists. It is possible that they are using false documents or providing false information to immigration officials in order to gain entry into Argentina. It is also possible that they are using legitimate documents, such as tourist visas, to enter the country.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence of the answer is correct in that the document does not provide this info. The sentences which follow are not supported with what is written in the document. The only mention of false documents comes in sentence 23, but those are said to be settlement documents. ", "ebr_raw": "Score: 5", "ebr_num": 5, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the limited information in the text", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article, along with some reasonable speculation", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "22": {"question_id": "11", "question": "How are the women detected for not coming to Argentina for tourist activities?", "system": {"text-davinci-003_few_shot": {"answer": "The national migration agency detected that the women did not come to Argentina to engage in tourism activities. They acknowledged it themselves.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article doesn't specifically mention how the women were caught, just that they were identified as lying.", "ebr_raw": "60", "ebr_num": 60, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The machine response answered the question correctly but missed some relevant information. The issues with documentation as mentioned on line 5 likely was the reason why the women in particular were flagged and then later admitted that they were not tourists.", "ebr_raw": "70", "ebr_num": 70, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first answer sentence is not relevant as it does not explain how they were detected. It is not specifically stated within the document how they were detected. It is noted in sentence 7 and correctly picked up by the answer however that it was acknowledged by the women. That should be left in then along with a note that the specific info is not given as to how the initial detection took place.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain how they detected the non-tourism, just that it did.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The women are detected for not coming to Argentina for tourist activities by the national migration agency. The agency checks the documentation of the women and if they are found to not have the necessary documents for tourism activities, they are detained.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not specify how this was detected about the women.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "23": {"question_id": "12", "question": "How does the Argentinian passport give more freedom than a Russian passport?", "system": {"text-davinci-003_few_shot": {"answer": "The Argentinian passport allows passport-holders to enter 171 countries visa-free, while Russians can travel visa-free to only 87 countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response answered the question correctly and with all relevant information presented in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "0"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly uses info from sentences 11 and 13.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The Argentinian passport allows its holders to travel visa-free to 171 countries, while the Russian passport only allows visa-free travel to 87 countries. This means that holders of an Argentinian passport have more freedom to travel to different countries without having to apply for a visa.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence of the answer uses doc. sentences 11 and 13 to answer the question. The second answer sentence lists a summary of the info, but that summary was never stated within the document. Further, as it just repeats info already known from sentences 11 and 13, it is not relevant. ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "24": {"question_id": "13", "question": "How could the mothers also obtain citizenship? ", "system": {"text-davinci-003_few_shot": {"answer": "Having an Argentine child also speeds up the citizenship process for parents. As it stands, Russians can travel visa-free to only 87 countries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered by the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The machine response answered the question correctly but added irrelevant information. Visa-free travel is not pertinent to answering the question.", "ebr_raw": "90", "ebr_num": 90, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The info in answer sentence 1 from doc. sentence 12 is relevant to answer the question. The second answer sentence is not as it does not address the topic of Argentinian citizenship.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The mothers could obtain citizenship by having their children born in Argentina, as Argentinian citizenship is granted to those born in the country. This would also speed up the citizenship process for the parents, as having an Argentine child would allow them to enter 171 countries visa-free, compared to the 87 countries that Russians can travel to visa-free. Additionally, the mothers could also purchase a package from a Russian-language website that offers services such as personalised birth plans, airport pick-ups, Spanish lessons", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The steps needed to take for citizenship are not given in the document. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer misinterpreted the question, but there is no directly relevant information in the article with which to answer the real query", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}}}, "25": {"question_id": "14", "question": "Why do the women not stay in Argentina?", "system": {"text-davinci-003_few_shot": {"answer": " It is believed the women want to make sure their babies are born in Argentina to obtain Argentinian citizenship. Having an Argentine child also speeds up the citizenship process for parents. As it stands, Russians can travel visa-free to only 87 countries. The women may also be returning to Russia to take advantage of the higher quality health services and to escape the war in Ukraine.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The second half of this answer is not supported by the text", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The machine response missed the question and provided an irrelevant response. The article does not fully explain why Russians do not stay in Argentina but it is likely because the parents are not Argentinian citizens and cannot stay long term.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is never stated in the document specifically why they do not stay. The first 3 sentences in the answer do not explain why they would leave right away. The final answer sentence contradicts what is in sentence 17.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer nor the article explains why they don't stay in Argentina.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The women are likely not staying in Argentina because they want their children to have Argentinian citizenship, which gives them more freedom than a Russian passport. Additionally, having an Argentine child speeds up the citizenship process for the parents. The women may also be taking advantage of the \"birth tourism\" packages offered by a Russian-language website, which range from \"economy class\" to \"first class\" and offer services such as airport pick-ups, Spanish lessons, and discounts on the cost of stays in", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not specifically stated within the document why the women leave. However sentence 23 indicates that not all do given that some are getting fake documents that allow them to settle in the country.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The question was misinterpreted in this case, but the information given fully matches that which is included in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't mention why the women leave Argentina.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer does not directly address the \"why\" question, though the text provides no specific answer either", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain why the women don't stay in Argentina.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}}}, "26": {"question_id": "15", "question": "How has obtaining Argentinian citizenship and leaving the country become a problem? ", "system": {"text-davinci-003_few_shot": {"answer": "The influx of pregnant Russian women has increased recently, which local media suggests is a result of the war in Ukraine. It is believed the women want to make sure their babies are born in Argentina to obtain Argentinian citizenship. This has caused concern as the women are signing up their children as Argentinean and leaving, which could be seen as a form of birth tourism.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer is supported by the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The machine response missed the question and provided an irrelevant response. The article does not mention the drawbacks of birth tourism. However the machine response did not recognize this and provided an irrelevant response to the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "0"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated in the document why this is a problem.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "Obtaining Argentinian citizenship and leaving the country has become a problem because the Russian women are entering Argentina under false pretenses, claiming to be tourists when they are actually coming to give birth in order to obtain Argentinian citizenship for their children. This is illegal and has caused the Argentinian government to investigate the situation and detain some of the women.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not state where exactly the problem lies. The answer is also incorrect in that according to doc. sentence 13, 14, and 15 there is no law against what they are doing.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why the Russian women leave or why it's a problem.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why the women leaving is a problem.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}}}, "27": {"question_id": "16", "question": "How have Argentinians responded to this situation?", "system": {"text-davinci-003_few_shot": {"answer": " Argentinians have expressed concern over the influx of pregnant Russian women, with the national migration agency head Florencia Carignano saying that the women had initially claimed they were visiting Argentina as tourists. The lawyer for the three women who were detained on Thursday said that they are being \"falsely imprisoned\". Additionally, Argentinian police has been carrying out raids as part of an investigation into a \"million-dollar business and illicit network\" that allegedly provided pregnant Russian women and their partners with fake documents issued", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is valid and sufficient", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The machine response answered the question correctly but had its last sentence in its response cut off and missed relevant information. The response should include that the fake documents were issued to allow the women to settle in Argentina.", "ebr_raw": "80", "ebr_num": 80, "workerid": "0"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentences 23, 5, and 13 are used to appropriately answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_zero_shot": {"answer": "The Argentinian government has expressed concern about the influx of pregnant Russian women entering the country. The national migration agency has detained some of the women who have arrived with questionable documentation. The police have also been carrying out raids as part of an investigation into a \"million-dollar business and illicit network\" that allegedly provided pregnant Russian women and their partners with fake documents. The public has also expressed concern about the situation, with some questioning the motives of the women and the potential impact on the country's resources.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The 1st answer sentence is not supported. The second is supported by sentence 5. It should be noted that their lawyer said they were wrongly detained and they were released. Sentence 23 supports answer sentence 3. The final answer sentence is not supported.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information is taken from the article, but the final sentence of this answer (while a reasonable assumption) was not.", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "There is no mention in the article about how the public views the situation.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}}}}}, "16": {"dataset": "inq-ext", "article_num": 9, "question_details": {"0": {"question_id": "0", "question": "How will a deficit affect Hong Kong's economy?", "system": {"davinci": {"answer": "A deficit is a shortfall in the government's budget. It means the government has spent more than it has received in revenue.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer defines a deficit but it does not explain how it will affect the economy. Sentence 8 provides useful information however by noting that the deficit would increase taxes. ", "ebr_raw": "30", "ebr_num": 30, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article does not really address how a broader budget deficit would affect the economy, but does hint at how it would affect the clubs", "ebr_raw": "30", "ebr_num": 30, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain how the deficit will affect the economy.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Investments will not be made and the economy will get worse if there are no investments that is bringing returns. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer doesn't directly answer the question, and the article doesn't either.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "A deficit of more than HK$100 billion (US$12.7 billion) in Hong Kong's economy could have multiple consequences. It may lead to reduced government spending on public services, infrastructure, and social programs, potentially dampening economic growth. Additionally, the government might have to increase taxes or introduce new revenue-generating measures to cover the shortfall, which could burden businesses and individuals, potentially affecting their spending and investment decisions. A prolonged deficit could also deter investors and weaken Hong Kong's credit rating, making it more expensive for the government and companies to borrow money.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the situation of things described in the article. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This appears to be an excellent, and factually accurate, answer, but it was not taken from the text (which does not discuss the effects of a deficit on the public)", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although correct, none of the information found in the answer is in the article. The article does not detail how a deficit affects the economy.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer sounds correct, but it is drawn from a source outside the document. The document does not explain the specific effects of a deficit.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "1": {"question_id": "0", "question": "How will the government strive to maintain low tax rates?", "system": {"davinci": {"answer": "The government will strive to maintain low tax rates by increasing the tax rates.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer isn't helpful, but the question isn't addressed any further in the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The government will take into account fiscal position of government, economic environment of Hong Kong, and the impact on the people in the community.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 5 states how the government will strive to maintain low tax rate. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not align with the document which lists planned steps to be taken before an increase. There are four items mentioned.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide specific details about how the Hong Kong government will strive to maintain low tax rates. However, it does mention that the government will consider adjusting the betting duty rates and evaluate the effects on areas such as people's livelihoods if any adjustments are made. This indicates that the government will be cautious in making changes to tax rates to ensure minimal impact on people's lives.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable inference that can be made from the article text", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is correct in identifying that the needed info is not present in the document. The information that follows in answer sentence 2 is just repeating already known information however and is therefore not relevant. The finals answer sentence is not drawn from the document.", "ebr_raw": "Score: 33", "ebr_num": 33, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "2": {"question_id": "0", "question": "Why is the proposal controversial? ", "system": {"davinci": {"answer": "The proposal is controversial because it is a controversial proposal.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The government promised it would maintain a low tax rate but is now wanting to do a tax rate adjustment.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the e article, the proposal is controversial because the government has said it will put more prudent consideration into the proposal. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is making an invalid argument without providing additional information, which makes it not relevant. The proposal's controversy is never specifically defined, but sentence 5 indicates that the proposal has the potential to affect the fiscal position of the government, Hong Kong's economic environment, and people from different walks of life in the community.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed all the information in sentences 9,10,11,12,13.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The proposal is controversial because the Hong Kong Jockey Club, which would be directly affected by the increase in betting duty, has criticized the proposal. The Club argues that the party behind the proposal fails to understand the betting business and that, if approved, it could create irreversible damage to Hong Kong and give rise to illegal or offshore gambling. The Club also warned that the increased levy would drastically reduce its operating revenue, resulting in a potential deficit and preventing it from making investments and charitable donations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 and 11 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly summarizes the relevant info included in 10-12.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The proposal has been met with criticism from the Hong Kong Jockey Club, who argue that the increased levy would drastically reduce its operating revenue and prevent the club from making investments and charitable donations. It has also raised concerns that an increase in betting duty might reduce the club\u2019s ability to fund charitable causes.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct because it missed the arguments as stated in sentences 10 and 12.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The proposal is controversial because it would increase the betting duty rate, which could have a negative impact on the Hong Kong Jockey Club's operating revenue and its ability to make investments and charitable donations. It could also lead to an increase in illegal or offshore gambling.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms the machine response ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the reasons stated in sentences 9 and 10.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}}}, "3": {"question_id": "1", "question": "How could the proposal potentially change the economic environment of Hong Kong?", "system": {"davinci": {"answer": "The proposal could potentially change the economic environment of Hong Kong in the following ways:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and misses all the relevant information contained in the article ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not say how the economic environment would change.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The tax adjustment on football betting will generate more revenue for the government. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not provide any of the ways that the environment could change. Some of the ways are talked about in 10-12 which notes that illigal or offshore gambling could arise, a deficit in revenue to the Jockey Club could also arise which could affect investments and charitable donations.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The proposal to raise Hong Kong's betting duty could potentially change the economic environment of Hong Kong in several ways. The increased revenue from the higher betting duty could be used to fund government projects and programs, potentially contributing to economic growth. However, opponents of the proposal, like the Hong Kong Jockey Club, argue that the increased betting duty could drastically reduce the club's operating revenue, create irreversible damage to Hong Kong, and give rise to illegal or offshore gambling. This could negatively impact the economic environment by decreasing legitimate business and increasing criminal activities related to illegal gambling.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response for both arguments is evident is the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The second answer sentence requires outside information. The 3rd answer sentence is correct, drawing on info in 10. The final answer sentence is drawn from an outside source. Info in 12, which notes that the Jockey club having less money preventing it from making investments and charitable donations is left out.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Most parts of the response are correct except the last sentence, which is unsupported by the document.", "ebr_raw": "75", "ebr_num": 75, "workerid": "1"}}}}}, "4": {"question_id": "1", "question": "How do citizens feel about the government wanting to generate more revenue?", "system": {"davinci": {"answer": "The government is a public service organization. It is supposed to serve the public. It is not supposed to be a profit-making organization.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer is not based on the text, but the article does not cite any response from the broader public", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention the citizens' reactions.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no information provided to answer the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The opinions of citizens are not given by the document in relation to this matter making the question unanswerable. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and unsupported. The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide information on how citizens feel about the government wanting to generate more revenue. However, it does mention that the Hong Kong Jockey Club has criticized the proposal to increase betting duty, arguing that it could create irreversible damage and encourage illegal or offshore gambling.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "As much relevant information as can be taken from the article is included in this answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly identifies that the info needed is not present in the document. The included info about the Jokey Club is supported by 10 and serves to speak to the opinions of a non government entity.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " It is not clear how citizens feel about the government wanting to generate more revenue. The essay does not mention any public opinion on the matter.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Citizens in Hong Kong have mixed reactions to the government's proposal to raise the betting duty to generate more revenue. Some are in favor of the proposal, arguing that it could help the government address the current fiscal deficit and fund public services. Others are concerned that the increase in betting duty could lead to a decrease in the Jockey Club's operating revenue, which could in turn reduce its ability to make investments and charitable donations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention citizens at all.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-12 corresponds with the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The citizens' reactions are not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "5": {"question_id": "1", "question": "Why does the proposal target football betting? ", "system": {"davinci": {"answer": "The proposal targets football betting because the party believes it is the most popular form of betting in Hong Kong.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. It is not specified in the document why football specifically was targeted, however, sentence 16 may provide a hint in saying that it is government policy not to  encourage gambling.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer may be factually valid, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is not supported by the article. ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "2"}}}, "gpt4": {"answer": "The proposal targets football betting because the New People's Party believes that increasing the football betting duty from 50% to 80% would help raise government revenue. The party suggested this idea to Financial Secretary Paul Chan Mo-po to address the budget deficit of more than HK$100 billion that Hong Kong might face for the current financial year due to the Covid-related economic slump.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Relevant information is included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer provides general information from the document without explaining why football betting specifically was targeted. The document does not explain why this type of betting was targeted over other forms.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "6": {"question_id": "2", "question": "How will ordinary citizens be impacted by the proposal? ", "system": {"davinci": {"answer": "The proposal will increase the betting duty.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is limited but correct based on the article info, but may miss some relevant info.", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Although it was clearly not based on a reading of the article, the answer is technically correct", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The government says the tax would not affect the livelihoods of the citizens.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The ways in which the ordinary citizens will be impacted is not stated in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not specifically state how ordinary citizens will be affected. Info from 12 is relevent however as it explains where could be less money for investment or charities. Sentence 19 indicates that the Jockey Club is charged a betting Duty, it can be assumed that is passed on to the people betting, but that info is not provided in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article suggests that any increase in betting duty would not affect ordinary people's livelihoods. The government will take into account the fiscal position of the government, the economic environment of Hong Kong, and the impact on people from different walks of life in the community while considering the proposal of adjusting the betting duty.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 5.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, though the article does not say it directly", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by 4 and 5 which sum up the situation well.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "7": {"question_id": "2", "question": "Why does the government want more revenue?", "system": {"davinci": {"answer": "The government wants more revenue to spend on public services.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is technically correct, but it ignores details in the article.", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The government needs to ensure fiscal responsibility. Due to the pandemic, Hong Kong could have a deficit of over $100 billion. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 7 answers the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not align with the document which states in 7 that Hong Kong could face a deficit of 100 billion because of the covid related slump in economic activity.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 7 and 8.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The government wants more revenue in part due to the deficit it is facing as a result of the Covid-related slump in economic activity. Financial Secretary Paul Chan Mo-po has hinted that Hong Kong could face a deficit of more than HK$100 billion (US$12.7 billion) for the current financial year.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its summarization of 6 and 7.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The government is facing a deficit of more than HK$100 billion (US$12.7 billion) for the current financial year because of the Covid-related slump in economic activity.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The government wants more revenue to ensure fiscal sustainability and to address the current financial year's deficit of more than HK$100 billion (US$12.7 billion) due to the Covid-related slump in economic activity.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "8": {"question_id": "2", "question": "Why has the idea caught media attention? ", "system": {"davinci": {"answer": "Because the party is headed by Regina Ip Lau Suk - yee , a lawmaker and convenor of the Executive Council \u2013 Hong Kong \u2019s de facto cabinet.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer repeats what is in sentence 9, but this does not answer the question as the question is asking why the fact that the party is headed by this person would case media attention. The document does not further elaborate, making the question not answerable. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The question isn't directly answered in the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The idea of increasing the betting duty caught media attention because the party proposing it, the New People's Party, is headed by Regina Ip Lau Suk-yee, who is a lawmaker and the convenor of the Executive Council, which is Hong Kong's de facto cabinet. This association with such political prominence lends the proposal more weight and visibility.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 and 10 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first answer sentence merely repeats already known information from 9. The second answer sentence requires outside information about politics in general in order to be written. The document never expands on why the the person described would cause media attention.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "9": {"question_id": "3", "question": "How does the betting business in Hong Kong work? ", "system": {"davinci": {"answer": "The Hong Kong Jockey Club is the only legal bookmaker in Hong Kong. It is a non-profit organisation and is the only legal bookmaker in Hong Kong. It is a non-profit organisation and is the only legal bookmaker in Hong Kong. It is a non-profit organisation and is the only legal bookmaker in Hong Kong. It is a non-profit organisation and is the only legal bookmaker in Hong Kong. It is a non-profit organisation and is the only legal", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first answer sentence is correct and is supported by 17. The answer sentences which follow are both repetitive and contain info not drawn from the document. Info from 18-22 further expains the process. ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer may be factually accurate, but it is overly repetitive and only loosely based on the text", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct but there is a lot of garbage text in it that needs to be removed.", "ebr_raw": "70", "ebr_num": 70, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": " The above sentences points out how the betting business in Hong King works.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response has serious problems and misses important information", "ebr_raw": "10", "ebr_num": 10, "workerid": "2"}}}, "gpt4": {"answer": "The betting business in Hong Kong operates under limited and regulated means, with the Hong Kong Jockey Club being the only licensed betting operator in the city. It conducts betting on horse races, football matches, and lotteries. Government charges a betting duty on these activities. The duty rates currently stand at 72.5% to 75% (on a progressive basis) for betting on horse races, 50% for football matches, and 25% for lotteries, calculated on the amount of proceeds or net stake receipts, depending on the betting type.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 17-22 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer summarizes 17-19 and 21-22 to appropriately answer the question in a complete manner.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}, "10": {"question_id": "3", "question": "What kinds of alternate measures is the government looking into?", "system": {"davinci": {"answer": "The government is looking into different measures to ensure fiscal sustainability.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The government said it is looking into various measures but only mentions a tax rate adjustment to betting as one of them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not detail any of those measures and is therefore not relevant. The info needed to answer the question is not given in the document as it never elaborates on what those other measures would be.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is not stated in the document anywhere. This makes the response incorrect and unsupported. It missed the information in sentence 3.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details about the alternate measures the Hong Kong government is considering to ensure fiscal sustainability.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "According to the article, the government is looking at making tax rate adjustment by increasing tax on football betting. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. The document does not further expand on the topic.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The government is looking into a \"tax rate adjustment\" to ensure fiscal sustainability. They will take into account the fiscal position of the government, the economic environment of Hong Kong, and the impact on people from different walks of life in the community.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer could have included more detail", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The government is looking into different measures to ensure fiscal sustainability, such as a \u201ctax rate adjustment\u201d. This could include increasing the betting duty on horse races and football matches, as well as increasing the duty rate for lotteries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more detail could have been included.", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8, 18, 19 and 20 all corresponds to the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer to a broad question", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "11": {"question_id": "3", "question": "Why does the government want to increase betting duty, in particular?", "system": {"davinci": {"answer": "The government wants to increase betting duty to generate more government revenue.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Response is accurate and based on article information", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "40", "ebr_num": 40, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not state why this particular aspect of the economy was targeted for receiving a duty increase. As such, the question is unanswerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The government wants to increase betting duty in order to generate more government revenue and ensure fiscal sustainability amid a potential deficit of more than HK$100 billion for the current financial year due to the Covid-related economic slump. The idea of increasing the betting duty was proposed as a means to raise revenue.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not address why betting in particular was targeted over other activities that could be taxed. The answer just speaks to why the government wants to increase revenue generally, not specifically why betting was chosen to do so.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It missed the information in sentence 16.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "12": {"question_id": "4", "question": "How is the government striving to ensure fiscal sustainability?", "system": {"davinci": {"answer": "The government is striving to ensure fiscal sustainability by increasing the betting duty.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a very brief answer, but it fits with the text", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 1.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is incorrect and it missed the information in sentence 5.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The government is striving to ensure fiscal sustainability by looking into different measures, including a possible \"tax rate adjustment.\" In particular, they are considering raising Hong Kong's betting duty. However, the government aims to maintain a low tax rate policy \"as far as possible\" and ensure that any increase does not affect people's livelihoods.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its use of information from 3, 4, and 1 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " The government is looking into different measures to ensure fiscal sustainability, including a \"tax rate adjustment\". They will take into account the fiscal position of the government, the economic environment of Hong Kong, and the impact on people from different walks of life in the community when considering the proposal of adjusting the betting duty.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some information is missing", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer could have been more specific", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The government is striving to ensure fiscal sustainability by looking into different measures, such as a possible tax rate adjustment. They will take into account the fiscal position of the government, the economic environment of Hong Kong, and the impact on people from different walks of life in the community when considering any adjustments.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Some more information could have been included", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 5 confirms the machine response, and the government is looking at taxing some businesses so as to increase revenue.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more information would have helped to answer the question", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "13": {"question_id": "4", "question": "How popular is football betting in Hong Kong? ", "system": {"davinci": {"answer": "According to the Jockey Club, the turnover of football betting in Hong Kong in the first half of this year was HK$1.5 billion, up from HK$1.3 billion in the same period last year. The turnover of horse racing betting was HK$3.5 billion, down from HK$3.6 billion in the same period last year.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not drawn from the document. The document does not directly speak to the popularity of football betting in Hong Kong.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "These figures may be valid, but they were not taken from the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't say how popular football is.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the football betting in Hong Kong generates high income. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response appears to use outside information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article does not provide information on the popularity of football betting in Hong Kong.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although the article did not provide information on how popular football betting is in Hong Kong, but it can be assumed that it is very popular and this has to be the reason why government wants to add tax duty so as to generate more revenue. ", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information, which the respondent acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification that the document fails to provide such information.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}, "14": {"question_id": "4", "question": "Why has Covid caused a slump in economic activity?", "system": {"davinci": {"answer": "The answer is simple. The slump in economic activity is caused by the slump in the stock market.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The question isn't really addressed further in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This incoherent answer made no attempt to cite facts from the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why COVID caused the slump, just that it was COVID-related.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not address why Covid created a slump, making the question not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Covid has caused a slump in economic activity because it led to lockdowns, travel restrictions, and social distancing measures, which significantly slowed down businesses, reduced consumer spending, and disrupted international trade. Many businesses faced temporary or permanent closures, and people lost jobs, which further contributed to reduced economic activity.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the turn out of events during the Covid lockdown.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is an excellent answer, but none of it was taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the answer is correct, the article does not provides any of this information.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not provide any information that would address this question. The info in the answer is drawn from an outside source. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "15": {"question_id": "5", "question": "How will tax rates change?", "system": {"davinci": {"answer": " The government will adjust the tax rates.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Key details from the article were missed", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The government wants to raise taxes to $31 billion versus the $25 billion being generated currently. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 3 confirms the machine response, and there will be betting duty on football. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not explain the change and is therefore not relevant. 8 contains the relevant info needed in the document. There is is stated that the football betting duty would increase from 50 percent to 80 percent.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is incorrect and unsupported. The correct answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details on how tax rates will change. It mentions that the government would look into different measures to ensure fiscal sustainability, including a \"tax rate adjustment.\" The New People's Party proposed increasing football betting duty from 50% to 80% as a means to raise revenue. However, the government has not confirmed or specified any exact changes to tax rates.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the information provided in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first two sentences of the answer are not relevant as they are contradicted by the info in answer sentences 3 and 4, which is supported by document sentences 8, 14, and 15.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " The government has not specified how tax rates will change, only that they will be adjusted.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The infromation from the article was skipped over", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "He did add that any increase would not affect people's livelihood.", "ebr_raw": "70", "ebr_num": 70, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article had much more detail", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response failed to mention the  proposed increase in football betting duty from 50 per cent to 80 per cent, as per sentence 8.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The Financial Services and Treasury Secretary Christopher Hui Ching-yu has stated that the government will look into different measures to ensure fiscal sustainability, which could include a tax rate adjustment. However, he also noted that the government would strive to maintain the policy of a low tax rate \u201cas far as possible\u201d and that any increase would not affect people\u2019s livelihood. The specifics of any potential tax rate adjustment have not yet been determined.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer could have included more detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3, 4 and 5 corresponds to the machine response, there will be a tax rate adjustment on match, and horse betting", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more information in the article is relevant to this question", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The last sentence in the response is unsupported and the response doesn't mention the rate change as per sentence 8. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "16": {"question_id": "5", "question": "Why would the proposal potentially give rise to illegal or offshore gambling?", "system": {"davinci": {"answer": "The proposal is to increase the betting duty from 50% to 80%.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer indicates what the proposal is, but it does not say how that would give rise to offshore gambling. Such information is not provided by the document, making the question not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The figures are valid, but they are only tangentially related to the question", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain why but it can be inferred that the people would go where they wouldn't have to pay such high taxes.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no given information as to why the proposal will potentially give rise to illegal or offshore gambling.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article provides no further explanation.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "gpt4": {"answer": "The proposal to raise Hong Kong's betting duty could potentially give rise to illegal or offshore gambling because the increased taxation could significantly reduce the profits and operating revenues of legal licensed operators like the Hong Kong Jockey Club. If these operators cannot operate profitably or competitively, gamblers may seek alternative illegal or offshore gambling options, which could lead to a loss of regulatory control over gambling activities in Hong Kong.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from a summary of the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer, based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not address why the proposal could give rise to the type of gambling described. The question is therefore not answerable. The answer uses outside information to construct its answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "17": {"question_id": "6", "question": "How will the government strive to maintain low tax rates?", "system": {"text-davinci-003_few_shot": {"answer": "The government will take into account the fiscal position of the government, the economic environment of Hong Kong, and the impact on people from different walks of life in the community and give prudent consideration to the proposal of adjusting the betting duty.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer to a vague question", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The government will strive to maintain low tax rates by evaluating the effects of any proposed tax rate increases on people's livelihoods and other factors such as the fiscal position of the government and the economic environment of Hong Kong. They will also take into account the impact of any tax rate adjustments on different sectors of the community.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 matches the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": " The response is partially correct because it doesn't mention that the government will give prudent consideration to the proposal of adjusting the betting duty , as per sentence 5.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}}}, "18": {"question_id": "7", "question": "How could the proposal potentially change the economic environment of Hong Kong?", "system": {"text-davinci-003_few_shot": {"answer": " If the proposal is approved, it could increase the betting duty from 50% to 80%, which could theoretically raise HK$25 billion to HK$31 billion in taxation from the Hong Kong Jockey Club. However, this could drastically reduce the club's operating revenue from HK$15 billion to HK$9 billion, resulting in either zero surplus or a deficit, and preventing the club from making investments and charitable donations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The proposal to increase the betting duty could potentially change the economic environment of Hong Kong by increasing government revenue, which could be used to fund public services and infrastructure projects. It could also reduce the amount of money available to the Hong Kong Jockey Club, which could lead to reduced investments and charitable donations. This could have a negative impact on the economy, as the Jockey Club is a major contributor to the local economy.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article doesn't say anything about Jokey Club being a major contributor to the local economy.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Major parts of the document talks about the tax adjustment on different businesses will help to raise revenue to improve the economy and at the same time it could also damage the economy according to sentence 11 and 12", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "\"which could be used to fund public services and infrastructure projects.\", \" reduce the amount of money available to the Hong Kong Jockey Club\" and \"as the Jockey Club is a major contributor to the local economy.\" in the response are unsupported.It missed to mention that  the increased levy would drastically reduce Jockey Club's operating revenue, as per sentence 11.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}}}, "19": {"question_id": "8", "question": "How will ordinary citizens be impacted by the proposal? ", "system": {"text-davinci-003_few_shot": {"answer": "The government will take into account the impact on people from different walks of life in the community when considering the proposal to adjust the betting duty. It is not clear yet how ordinary citizens will be impacted by the proposal.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Christopher Hui Ching said they would keep the tax rate as low as possible and that any increase would not affect people's livelihood.", "ebr_raw": "80", "ebr_num": 80, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response doesn't mention that the proposed increase in the betting duty might reduce the Jockey Club \u2019s ability to fund charitable causes, which in turn will affect some citizens, per sentence 13.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The government has stated that any increase in the betting duty would not affect people's livelihood. However, if the proposal is approved, it could reduce the Hong Kong Jockey Club's operating revenue, which could in turn reduce its ability to make investments and charitable donations. This could have a negative impact on ordinary citizens who rely on the club's charitable donations for support.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5, 11 and 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "20": {"question_id": "9", "question": "Why does the government want to increase betting duty, in particular?", "system": {"text-davinci-003_few_shot": {"answer": "The government wants to increase betting duty to generate more government revenue and ensure fiscal sustainability. The proposal was put forward by the New People's Party to Financial Secretary Paul Chan Mo-po for the 2023-24 budget.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Hong Kong's deficit was due, in part, to the slump in economical activity due to Covid.", "ebr_raw": "20", "ebr_num": 20, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, though the article does not really address the specific why question", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article does not explain why the government wants to increase betting duty, specifically.", "ebr_raw": "40", "ebr_num": 40, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "There is a little bit more information in the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. The passage doesn't mention why the government wants to increase betting duty in particular .", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The government wants to increase betting duty in order to generate more government revenue and ensure fiscal sustainability. The proposal was put forward by the New People\u2019s Party last month to Financial Secretary Paul Chan Mo-po for the 2023-24 budget. The increase in betting duty would theoretically raise HK$25 billion to HK$31 billion in taxation from the Hong Kong Jockey Club, which would help the government to offset the deficit caused by the Covid-19 pandemic.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 summarizes the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. The reason for the particular increase  is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "21": {"question_id": "10", "question": "Why has Covid caused a slump in economic activity?", "system": {"text-davinci-003_few_shot": {"answer": " Covid has caused a slump in economic activity due to the restrictions on travel, business operations, and social gatherings, as well as the decrease in consumer spending.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article does not give details on what economical problems was caused by COVID.", "ebr_raw": "20", "ebr_num": 20, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is mostly speculation, but it fits with the overall theme of the article", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not detail why COVID caused a slump in the economy.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This information is not included in the text, but it is alluded to, so the answer appears to be valid", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is unsupported because the passage doesn't mention the reasons.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The Covid pandemic has caused a slump in economic activity due to the restrictions and lockdowns imposed by governments around the world to contain the spread of the virus. These restrictions have led to a decrease in consumer spending, business closures, and job losses, all of which have had a negative impact on the global economy.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer seems valid, but it is mostly speculation based on the little relevant information in the article", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not state why covid caused a slump in the economic but the machine response is correct as the restrictions on businesses during the covid caused a huge drop in revenue and it affected the economy.", "ebr_raw": "75", "ebr_num": 75, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is an apparently valid answer, but it has some outside information", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "22": {"question_id": "11", "question": "How will a deficit affect Hong Kong's economy?", "system": {"text-davinci-003_few_shot": {"answer": " A deficit could lead to a decrease in government spending, which could have a negative impact on the economy. It could also lead to an increase in taxes, which could further reduce consumer spending and investment. Additionally, it could lead to an increase in borrowing, which could lead to higher interest rates and an increase in the cost of borrowing.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't detail any of the points made in the answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain how a deficit would impact the economy.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is unsupported because the passage doesn't mention how the economy is affected.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "A deficit in Hong Kong's economy could have a significant impact on the city's economic stability. It could lead to higher taxes, reduced government spending, and a decrease in public services. It could also lead to an increase in unemployment, as businesses may be forced to cut back on hiring due to the lack of funds. Additionally, it could lead to a decrease in consumer spending, as people may be less willing to spend money if they are worried about their financial security.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article, but has a lot of outside information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention anything about the long-term effects of the deficit.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 and 11 summarizes the machine response.  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer makes a lot of reasonable assumptions, but they were not based on any explicit information in the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "23": {"question_id": "12", "question": "Why does the proposal target football betting? ", "system": {"text-davinci-003_few_shot": {"answer": " The proposal targets football betting as it is seen as a potential source of revenue for the government. The party believes that increasing the betting duty from 50% to 80% would generate more government revenue.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why the government is targeting football betting, specifically.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence in the response is unsupported.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The proposal to increase football betting duty from 50 per cent to 80 per cent is an attempt to generate more government revenue. Football betting is a popular form of gambling in Hong Kong and the increase in duty would result in more money for the government. The Hong Kong Jockey Club, which is the only licensed betting operator in Hong Kong, conducts horse race betting, football betting and lotteries. The government charges the club a betting duty, and the duty rate for football matches is currently 50 per cent", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "It targets football betting because many people put their monies into bettings and much revenue is generated from football betting.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "As much information as is present in the article is included in the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}}}, "24": {"question_id": "13", "question": "Why has the idea caught media attention? ", "system": {"text-davinci-003_few_shot": {"answer": "The idea was proposed by the New People's Party, which is headed by Regina Ip Lau Suk-yee, a lawmaker and convenor of the Executive Council \u2013 Hong Kong\u2019s de facto cabinet.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer doesn't explain \"why\" the idea caught media attention.", "ebr_raw": "60", "ebr_num": 60, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Some more information would have helped to answer the question", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why the idea caught media attention.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article does not really address this specific question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The idea has caught media attention because the party is headed by Regina Ip Lau Suk-yee, a lawmaker and convenor of the Executive Council \u2013 Hong Kong\u2019s de facto cabinet. This has given the proposal more weight and legitimacy, as it is coming from a high-ranking official in the government.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The  last sentence in the response is unsupported, so the response is partially correct.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}}}, "25": {"question_id": "14", "question": "How does the betting business in Hong Kong work? ", "system": {"text-davinci-003_few_shot": {"answer": " In Hong Kong, it is government policy not to encourage gambling. But the city allows limited and regulated means of gambling and has authorised the Hong Kong Jockey Club as the only licensed betting operator in Hong Kong. The club conducts horse race betting, football betting and lotteries. The government charges the club a betting duty. At present, duties on betting on horse races and football matches are charged on net stake receipts. The duty rates for betting on horse races are from 72.5 per cent to", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid and comprehensive answer, but it was cut off", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is incomplete as it missed the duties for horse race betting and lotteries, as per sentences 21 and 22.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "In Hong Kong, it is government policy not to encourage gambling. But the city allows limited and regulated means of gambling and has authorised the Hong Kong Jockey Club as the only licensed betting operator in Hong Kong. The club conducts horse race betting, football betting and lotteries. The government charges the club a betting duty. At present, duties on betting on horse races and football matches are charged on net stake receipts. The duty rates for betting on horse races are from 72.5 per cent to", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 16-22 matches the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is valid, but it is cut off", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is partially correct because it missed the current football betting duty of 50 per cent, as per sentence 8.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}}}, "26": {"question_id": "15", "question": "How popular is football betting in Hong Kong? ", "system": {"text-davinci-003_few_shot": {"answer": "The government charges the Hong Kong Jockey Club, the only licensed betting operator in Hong Kong, a betting duty of 50% for football matches.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer doesn't explain how popular football betting is in Hong Kong.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The specific question is not really covered by the article, but the answer misses the point of the question entirely", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention how popular football betting is in Hong Kong.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This question was not directly addressed in the article, but the answer given is irrelevant", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question because the passage doesn't mention how popular football betting is in Hong Kong.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Football betting is a popular form of gambling in Hong Kong. The Hong Kong Jockey Club is the only licensed betting operator in the city and offers football betting with a duty rate of 50%. According to the Hong Kong Jockey Club, football betting accounted for around 40% of its total betting turnover in 2019.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Very little information (though apparently accurate) in the answer came from the text", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not give any numbers or even mention how popular football betting might be.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 17 and 21 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first and the last sentences in the response are unsupported.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "1"}}}}}, "27": {"question_id": "16", "question": "Why would the proposal potentially give rise to illegal or offshore gambling?", "system": {"text-davinci-003_few_shot": {"answer": "The proposal to increase the betting duty would drastically reduce the Hong Kong Jockey Club's operating revenue, which would prevent the club from making investments and charitable donations. This could lead to people seeking out illegal or offshore gambling options as an alternative.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Neither the answer or the article explain why the proposal could give rise to illegal or offshore gambling.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain WHY the proposal would give rise to illegal or offshore gambling.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question as the passage doesn't mention why the proposal potentially would give rise to illegal or offshore gambling.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The proposal to increase the betting duty could potentially give rise to illegal or offshore gambling because it would drastically reduce the operating revenue of the Hong Kong Jockey Club, which is the only licensed betting operator in Hong Kong. This could lead to people seeking out illegal or offshore gambling options as an alternative to the Jockey Club.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-12 summarizes the machine response. ", "ebr_raw": "95", "ebr_num": 95, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}}}, "17": {"dataset": "inq-ext", "article_num": 10, "question_details": {"0": {"question_id": "0", "question": "How long had Ong worked in as a superintendent?", "system": {"davinci": {"answer": "Ong was a superintendent for about a year.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not state how long Ong had worked as a superintendent", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article does not say how long Ong has been in the position", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not say how long Ong was a superintendent.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not state the length of time that he was working in that position, making answering the question not possible.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide information on how long Ong had worked as an assistant marine superintendent.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the information needed to answer the question is not present in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asked for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not provide the details about Ong duration of work.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document does not provide information on how long Ong was a superintendent for", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "human_1": {"answer": "It is not stated in the article how long Ong had been a superintendent.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not mention the duration Ong had worked for. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in stating that the needed info is not provided.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct and supported.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state the duration that Ong worked for", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "1": {"question_id": "0", "question": "Why is the tanker believed to be from North Korea? ", "system": {"davinci": {"answer": "The vessel is believed to be from North Korea because the vessel is flagged by North Korea.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The Why question was not addressed in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100.", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not explaining how it was known that the vessel was flagged in that way. Information to that effect comes in 10 and 11 together, which indicate that there was a report signed by a chief officer of Ansan Shipping Company, which is believed to be a North Korean Company. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "gpt4": {"answer": "The tanker is believed to be from North Korea because images taken from an employee's mobile phone showed that the Singapore-registered MT Sea Tanker II transferred oil to a vessel called \"MT An San I.\" The transfer was signed off by a chief officer from \"Ansan Shipping Company,\" which is thought to be a North Korean shipping company.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't really say why the MT An San I was suspected of being North Korean, and this answer does not address the question adequately", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 and 11 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer draws from sentences 10 and 11 to provide an answer the is clear and relevant, including all applicable info in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 and 11 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "During the transfer to the tanker, a report was signed off by an office from the \u201cAnsan Shipping Company\u201d, believed to be a North Korean shipping company, although the article does not make it clear why this company is believed to be North Korean. Furthermore, the article describes that Koh, Ong, and Tan collaborated to rewrite records. While it is not stated explicitly, such suspicious activity may indicate that they knew they had made an illicit transaction.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told why the company/ship was believed to be North Korean.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information is included to make a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first part of the response is correct but the later part starting from 'Furthermore...\" is irrelevant to the question.", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "human_2": {"answer": "The tanker was flagged as a North Korean tanker because of where the tanker had originated from. The report about the tanker was signed by the \"Ansan Shipping Company\", which is believed to be located in North Korea. From there, there was significant obstruction of information concerning the tanker's path, and falsified information was recorded in the logbooks. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "'which is believed to be located in North Korea' in the response is unsupported. Instead sentence 9 says 'believed to be a North Korean shipping company'. The last sentence of the response is also unsupported.", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is relevant and reasonable but misses some relevant info", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The prosecution said that the vessel was called \u201cMT An San I\u201d \u2013 believed to be North Korea-flagged \u2013 in October 2018. The report was signed off by a chief officer from one \u201cAnsan Shipping Company\u201d, believed to be a North Korean shipping company.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more detail could have been included.", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The tanker carried a North Korea flagged logo and was also signed off by a chief officer from a North Korea shipping company according to sentence 11", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article says the tanker has the North Korea flagged number and it was also signed off by a chief officer that works in a North Korea shipping company. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The tanker is believed to be from North Korea because it was identified as \"MT An San I\" - a vessel believed to be North Korea-flagged - in images of items taken from Koh's mobile phone. Additionally, the report was signed off by a chief officer from one \"Ansan Shipping Company\" - believed to be a North Korean shipping company.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 and 11 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and complete, and uses information from the document to answer the question", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentences 10 and 11 are used to appropriately answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}, "2": {"question_id": "1", "question": "How long had Tan and Koh been cargo officers? ", "system": {"davinci": {"answer": " Tan and Koh were cargo officers for Sea Hub Tankers.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not state how long Tan and Koh had been cargo officers", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "No attempt was made to answer the question, but the article does not provide an answer", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not say how long Tan and Koh had been cargo officers.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and incorrect. The answer is not stated anywhere in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not state how long they had been in that position, making it not relevant to answer the question. The question is not answerable however as there is no mention in the document of how long they had been serving in this position.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not mention how long Tan and Koh had been cargo officers.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct. This information is not stated in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asked for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The duration of Tan and Koh Cargo officer job is not stated in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not provide information about how long Tan and Koh had been cargo officers.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "human_1": {"answer": "It is not stated in the article how long Tan and Koh had been cargo officers.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not mention Tan and Koh work duration", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in stating the the needed info is not included within the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response correct and supported.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not mention the work duration of Tan and Koh", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't state the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "3": {"question_id": "1", "question": "Why does the UNSC prohibit transactions to North Korea? ", "system": {"davinci": {"answer": "The UNSC resolutions are based on the UN Charter. The UNSC resolutions are based on the UN Charter. The UN Charter is the fundamental law of the United Nations. It sets out the purposes and principles of the United Nations and establishes the powers and responsibilities of its main organs. The UN Charter is the fundamental law of the United Nations. It sets out the purposes and principles of the United Nations and establishes the powers and responsibilities of its main organs. The UN Charter is the fundamental law of the United", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Another worthless answer which ignores the entire article", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The UNSC prohibit transactions to North Korea so as to counter its nuclear weapons programme. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into why the UNSC prohibits such transactions with North Korea.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not address the question and draws from outside sources and is therefore not relevant. The information needed to answer the question comes from 9, which indicates that such are in place to counter North Korea's nuclear weapons program.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and repetitive. It missed the information in sentence 9.", "ebr_raw": "Score: 0. The response misses all the information from the document needed to answer the question and is irrelevant and repetitive.", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The United Nations Security Council (UNSC) prohibits transactions to North Korea because it has implemented sanctions and resolutions against the country to counter its nuclear weapons program. As a member state of the UN, Singapore is obliged to enforce these sanctions and resolutions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is documented in sentence 9 of the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Only the first answer sentence is needed to answer the question. Drawing from 9, the sentence effectively describes why the prohibition is in place. The second answer sentence deals with Singapore's role, which is beyond the question's scope.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not stated in the article why the UNSC specifically prohibits transactions of oil to North Korea, although it is stated that the resolutions are generally about countering North Korea's nuclear weapons program.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The AI contradicts itself, stating that it is not known and then tell us the known answer. The answer is to counter North Korea's nuclear weapons program.  ", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information was included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and it missed the part that these are UNSC sanctions, as per sentence 7.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "human_2": {"answer": "It is not explicitly stated, but can be inferred through the conditions Singapore must meet why the UNSC prohibits transactions with North Korea. Member states of the UNSC must implement sanctions on North Korea to control its trade. The aim of this is to prevent its nucular weapons programme from expanding and to prevent more funding coming into North Korea. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is coined out from sentence 7 and some other parts of the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "'to prevent more funding coming into North Korea.' in the response is unsupported by the document.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears correct and reasonable based on the article info", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " To counter its nuclear weapons programme.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The UNSC prohibits transactions to North Korea in order to counter its nuclear weapons programme. As a member state of the UN, Singapore is obliged to implement the UNSC sanctions and its respective resolutions on North Korea in order to prevent the proliferation of nuclear weapons.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Sentence 1 of the answer makes use of sentence 9 of the document to appropriately answer the question. The sentence which follows is not relevant as it is both repetitive and gets into why Singapore follows the requirement which is beyond the scope of the question.", "ebr_raw": "Score: 90", "ebr_num": 90, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant, answers the question and is consistent with the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "4": {"question_id": "2", "question": "How will the sanctions affect the economy?", "system": {"davinci": {"answer": "The sanctions will not affect the economy.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information about how the sanctions will affect the economy. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to find an answer in the text, but even if the answer writer had put in some effort, the article would not have provided any information on the effects of sanctions on North Korea", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into detail as to how the sanctions will affect the economy.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document as the document does not address the effect such sanctions will have on the economy.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The sanctions imposed by the United Nations Security Council (UNSC) on North Korea are aimed at countering the country's nuclear program. These sanctions can affect the economy by restricting trade, hampering investment, and limiting access to international financial and banking systems. Consequently, this could lead to reduced revenue from exports, decreased government spending, and a slowdown in economic growth. Additionally, sanctions may disrupt the operations of businesses that have dealings with North Korea, which would affect their profitability and ability to create jobs. However, the broader economic impact would depend on the scale of a country's trade links with North Korea and the extent to which it enforces these sanctions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete but contains a significant amount of speculation and information not directly contained in the document.", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This seems to be a very accurate and comprehensive answer, but it is not based on the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Other than the sanctions being place to counter North Korea's nuclear program, the article doesn't include any of the other parts of the answer.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information given by the machine response cannot be deduced form the article. ", "ebr_raw": "Score: 0.", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The way the sanctions will affect the economy is not stated in the article. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}}}, "human_1": {"answer": "It is not stated in the article how the UNSC sanctions affect North Korea's economy, nor the economy of other nations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question again asks for outside information, but the respondent does not try to go beyond what is given in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no evidence of how the sanction will affect the economy in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly states that the info is not stated in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document, but is in fact contained in sentence 1: The United Nations Security Council ( UNSC ) resolutions prohibit such transactions.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "As the purpose of the sanctions is to limit the expansion of the nucular weapons programme, it can be inferred that the sanctions would also weaken the economy. The sanctions would also likely affect how dependent North Korea is on international trade. The economy would have to be supported by other means, if sanctions were preventing open trade. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "A part of the machine response is evident in sentence 7", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid supposition, given the limited information in the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere and the response just made inferral. So the response is unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is relevant and mostly reasonable but contains a bit of speculation and/or outside information to answer the question", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}}}}}, "5": {"question_id": "2", "question": "Why did the three men create the false narrative and transfer the oil?", "system": {"davinci": {"answer": " The answer is simple. They did it for money.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "This answer is just an assumption that clearly ignored the article text (though it is technically accurate)", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, two of them likely did it because they were instructed while Ong did it because of a sense of misguided loyalty to Sea Hub tankers for putting him through school in the past.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "One man had a misguided loyalty to the company because they put him through school and there is no reasons stated for the other two.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. Sentence 24 indicates that the offences were commited out of a sense of misguided loyalty to Sea Hub Tankers.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incorrect and unsupported. it missed the information in sentences 3,4,15,16,17,18,24.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The three men created the false narrative and transferred the oil to a North Korea-flagged vessel in violation of United Nations Security Council resolutions, which prohibit such transactions. This transfer was not recorded in MT Sea Tanker II's official logbook, and the trio attempted to obstruct the course of justice by falsifying the logbooks when presenting them to authorities.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer does not address the \"why\" question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The major reason why the three men created the false narrative and transferred the oil is not specifically stated in the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is completely wrong. Ong committed the offence due to a sense of misguided loyalty to the company who put him through school. There is no information on the other two men involved.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct because it describes what the men did, not why they did it. Sentence 24 indicates that it was done out of a misguided loyalty to Sea Hub Tankers, at least in the case of Ong.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The above sentence states the reason why one of the workers got involved in the oil transfer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}}}, "human_1": {"answer": "It is not stated in the article why the men created the false narrative. However, since the transaction is prohibited by the UNSC, it can be inferred that they may have created the false narrative to avoid being implicated in wrongdoing. It is also not stated by they transferred the oil, but it can be hypothesized that they or their company may have benefitted monetarily.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They were also working under instructions by their boss, who instructed them to create the false narrative ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told why the three men created the false narrative, but it can be inferred from #19 that there was some loyalty to the company from at least one of the accused, so it may have been the company that ordered the transfer and the accused were just following orders.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "These are all reasonable assumptions, based on the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is not stated in the document anywhere. This makes the response incorrect and unsupported. The correct answer is stated in sentence 19.", "ebr_raw": "Score: 0.", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "human_2": {"answer": "One of the men, Ong, stated that he had a \"misguided sense of loyalty\" that caused him to construct the false narrative. He had worked for the Sea Hub company for several years, and they had even paid to put him through college education at a university in Singapore. Due to this, we can infer he likely felt some pressure from higher-ups to allow the North Korean tanker to enact a transaction, or that there were forces above him that assigned him the cover up job, if he is telling the truth. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "'He had worked... for several years' and 'at a university in Singapore' in the response are unsupported. Also, the last sentence of the response is inferred but not supported by the document.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 19 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is reasonable and likely accurate based on the information in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " The three men were working for a firm called Sea Hub Tankers that owned and operated MT Sea Tanker II. They were instructed to present a false narrative on the movement of the Singapore-registered MT Sea Tanker II by falsifying its logbooks, which were then presented to the authorities. The transfer of oil was likely done to avoid violating UNSC resolutions which prohibit such transactions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 15-18 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The three men were workers in the North Korea company and were instructed by their bosses to falsify the logbook and tranfser the oil. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "'They were instructed' and the last sentence in the response are unsupported, so the response is incorrect.", "ebr_raw": "30", "ebr_num": 30, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The three men created the false narrative and transferred the oil in order to conceal the fact that they had violated UNSC resolutions prohibiting such transactions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a brief but valid answer", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "In addition to the machine response, sentence 24 also talks about the reason one of the three had committed the crime ", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct in that they did not transfer oil to conceal that they were prohibited from taking such actions. Rather sentence 24 lists that it was all done out of a misguided loyalty to Sea Hub Tankers.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and based on information provided in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "6": {"question_id": "3", "question": "How did the three men become involved in the oil transfer situation with North Korea?", "system": {"davinci": {"answer": " The three men were working for a firm called Sea Hub Tankers that owned and operated MT Sea Tanker II. Ong was then an assistant marine superintendent, while Tan and Koh were cargo officers.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, for once", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer speaks to the general context of the situation but does not specifically address how that lead to them being involved with this particular transfer. Such information is not provided by the document, making the question not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "gpt4": {"answer": "The three Singaporean men were working for a firm called Sea Hub Tankers, which owned and operated the Singapore-registered MT Sea Tanker II at the time of the offenses.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more detail could have been included.", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as it describes the general situation of their employment at the time, but it does not provide info about how they came to be involved with North Korea. Such info is not provided within the document. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The three men were working for a firm called Sea Hub Tankers that owned and operated MT Sea Tanker II. It is not clear how they specifically got involved with the oil transfer beyond being aboard the ship.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They got involved as they were at the time working for the Sea hub company and they were made to alter information", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told the accused motives. We can only infer from #19 that it was company loyalty, but that is just speculation and does not satisfy the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "There is more relevant information in the article", "ebr_raw": "20", "ebr_num": 20, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response failed to mention the details on how the three men got involved in the oil transfer, as per sentences 13,14,15,16,17.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "human_2": {"answer": "The three men became involved in the oil transfer situation because of the company they worked for, Sea Hub Tankers. The article does not mention whether they were assigned the job, or whether it was something they decided to take on themselves, knowing it would be against the law. The three men worked together to falsify the records, deceive those investigating, and to carry out the transaction. ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "This response incorrectly states that the information is not in the document. However, sentences 13,14,15,16,17 in the document describe how they were involved in the oil transfer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Information from the text is used to create a reasonable answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is basically correct and a good summary of the information presented in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " The three Singaporean men - Ong Chou Hong, Benny Tan Chun Kiat and Jeremy Koh Renfeng - were working for a firm called Sea Hub Tankers that owned and operated MT Sea Tanker II. They were instructed to falsify the logbooks of the MT Sea Tanker II in order to present a false narrative on the movement of the vessel.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 15-18 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article says the three men were carrying out the instructions that was given to them by their bosses.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response missed a lot of details about how they are involved in the oil transfer situation, as per sentences 15,16,17,18,21 and 22.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The three Singaporean men became involved in the oil transfer situation with North Korea when they were instructed by their employer, Sea Hub Tankers, to falsify the logbooks of the Singapore-registered MT Sea Tanker II in order to present a false narrative on the movement of the vessel. This was done in order to hide the fact that the vessel had transferred oil to a North Korea-flagged vessel, which is prohibited by United Nations Security Council (UNSC) resolutions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer that fits with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The three men were workers in the North Korea company and were instructed to commit the crime that they committed.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not supported by the document. It was never stated that they were specifically directed to do this. When exactly they became involved was not discussed.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response seems accurate and complete, relevant and sourced from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "7": {"question_id": "3", "question": "How will the sanctions affect other countries?", "system": {"davinci": {"answer": " The sanctions are intended to cut off sources of revenue that North Korea uses to fund its nuclear and ballistic missile programmes. The sanctions are also intended to cut off North Korea's access to the international banking system.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms part of the machine response. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer would have been a good response to an earlier question, but it does not address the current one, and it was not taken from this article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into detail as to how the sanctions will affect other countries.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not address how other countries will be affected by the sanctions.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "UNSC sanctions on North Korea can affect other countries by prohibiting them from engaging in certain trade activities involving the country. This means businesses and individuals in other nations must adhere to the specified restrictions or risk facing penalties such as fines, imprisonment or both for violating international regulations. The sanctions aim to counter North Korea's nuclear weapons program and weaken its regime by limiting its access to necessary services, goods and funds.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response contains speculation and outside information, as it isn't directly addressed in the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer is loosely based on the article, but it is almost entirely outside information", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 corresponds slightly with the machine response. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention anything in the answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 corresponds to some part of the article. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "human_1": {"answer": "The UNSC sanctions are designed to counter its nuclear weapons program. Their actual impact is unstated, but it can be inferred that their intent is to improve national security for other countries that may be targets of North Korean attacks.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information is included, along with some reasonable inferences when the article asks for outside information", "ebr_raw": "Score: 90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct in stating that the information is not stated within the document. The rest of the information would need to come from outside the document however because the only mention of the sanctions comes in sentence 7, and there is not enough info there to draw the inferences that are drawn by the answer. Further the first answer sentence is answering why the purpose of the sanctions are, which is not what the question is asking about.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The inferral part from the response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not mention how the sanctions will affect other countries. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this question would have required outside information (or speculation)", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 1 in the document, the other countries are prohibited to transfer oil to North Korea.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct, the answer is not stated within the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "8": {"question_id": "4", "question": "How did the authorities find out about the situation?", "system": {"davinci": {"answer": " The prosecution said that three Singaporean men then worked together to present a false narrative on the movement of the Singapore - registered MT Sea Tanker II by falsifying its logbooks , which were then presented to the authorities.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Key information from the article is missing", "ebr_raw": "Score: 20. The response does not include key information from the article necessary to answer the question.", "ebr_num": 20, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They found out through Koh's mobile phone", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not address how the authorities were made known of the situation, which is what the question is looking to know. Such info comes in 13, which notes that authorities were given alleged information, the source of which was not named in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is partially correct and missed the information in sentences 12,13,14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The authorities found out about the situation when they retrieved images from Koh's mobile phone, which showed items such as a report indicating that MT Sea Tanker II had transferred oil to a vessel called \"MT An San I\" in October 2018.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as the answer currently notes proof of the situation rather than describing how authorities were originally alerted, which is what the question is asking about. 13 holds the needed information, indicating that authorities received info alleging the transfer (it is not stated who did the alerting)", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The Singapore authorities received information alleging that MT Sea Tanker II had engaged in a ship-to-ship transfer with a North Korea - flagged vessel, at which time they requested Sea Hub Tankers' documents, including MT Sea Tanker II \u2019s official logbook.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 8 also provides additional information as to how the authorities found out about the situation", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information is found in #11 and #12.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "The authorities became aware of the situation because of information that was shared with them, including suspicious photos of supplies that were being transferred on the vessel. From there, the authorities were able to investigate and uncovered more evidence. They found images on one of the three men's phone that proved their involvement in the cover-up. They were also able to trace the origin of the tanker to a North Korean shipping company. ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "'suspicious photos of supplies' in the response is unsupported. The response missed the information in sentences 11 and 12 about how the authorities found out about the situation.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article did not say how they first came to know about the tanker.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8-12 backs up the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_few_shot": {"answer": "The truth emerged after investigators retrieved images from Koh\u2019s mobile phone.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid, if concise, answer", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 matches the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The truth emerged after investigators retrieved images from Koh's mobile phone.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is valid, but could have included more details", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms the machine response.  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info is contained in sentence 13 which notes that authorities received info alleging the ship to ship transfer. The current answer indicates how the info was confirmed, not how the authorities found out. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is clear and concise, and answers the question with info from tehe document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "9": {"question_id": "4", "question": "How will Singapore combat North Korea's nuclear weapons programme?", "system": {"davinci": {"answer": " Singapore is a member of the United Nations and is obliged to implement the UNSC sanctions and its respective resolutions on North Korea to counter its nuclear weapons programme.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, with all relevant information from the text included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed that the prohibition of oil transfer, as per sentence 1.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is repeating sentence 9 of the document, but this sentence does effectively answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "gpt4": {"answer": "Singapore will combat North Korea's nuclear weapons program by implementing UNSC sanctions and its respective resolutions on North Korea. This includes actions such as prohibiting transactions that are forbidden by the Security Council resolutions, such as the transfer of oil to a North Korea-flagged vessel, as detailed in the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and a good summary of the relevant article information ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 and some parts of the article confirms the machine response. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "Singapore is combatting North Korea's nuclear weapons program through sanctions, but other measures are not stated in the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The small amount of relevant information from the article is included in this answer", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification and use of sentence 7.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document, but sentence 1 mentions prohibition of oil transfer by Singaporean tanker. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 7 states that there will be an implementation of the UNSC sanction", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer could at least have mentioned that Singapore is assisting with the implementation of sanctions against North Korea", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 1 in the document, This response incorrectly states that the information is not in the document. As per sentence 3 in the document, Singapore will combat North Korea's nuclear weapons programme by not transferring oil.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Response is correct, a specific answer is not contained in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "10": {"question_id": "5", "question": "What does Sea Hub Tankers do? ", "system": {"davinci": {"answer": "Sea Hub Tankers is a Singapore-based company that owns and operates MT Sea Tanker II. It is a subsidiary of Sea Tanker Pte Ltd, which is a subsidiary of Sea Tanker Holdings Ltd.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They operate oil tankers that transport oil to various locations.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "All the info in the answer is drawn from an outside source except for the phrase, \"owns and operates MT Sea Tanker II\". This information suffices to answer the question, albeit not well, but it gives all the info available in the document.", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The latter part of the response regarding subsidiary is incorrect and unsupported. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "gpt4": {"answer": "Sea Hub Tankers is a company that owns and operates vessels, such as the MT Sea Tanker II.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a concise, but valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 3", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer draws from doc. sentence 3 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It can be inferred that Sea Hub Tankers owns and operates tanker ships, but other details about their business and operations are not provided in the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 3 adds information to what Sea Hub tankers does.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The  response failed to mention that Sea Hub Tankers owned and operated MT Sea Tanker II, as per sentence 3.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "human_2": {"answer": "The Sea Hub Tankers company owns and operated the tanker that was used in the deal with the North Korean shipping company. It can be inferred that the company likely manages trade and tankers in Singapore. ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Though the inferral in last sentence of the response looks good, it is unsupported by the document.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Relevant information from the article is included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 3", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and reasonable based on the information in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " Sea Hub Tankers is a firm that owns and operates MT Sea Tanker II, a Singapore-registered tanker.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 3", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": " Sea Hub Tankers is a firm that owns and operates the Singapore-registered MT Sea Tanker II.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Info from sentences 2 and 3 is used to answer the question. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is as good and accurate as possible, given the information in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "11": {"question_id": "5", "question": "What happened to the chief officer from Ansan Shipping Company? ", "system": {"davinci": {"answer": "The chief officer from Ansan Shipping Company was not charged.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information about what happened to the chief officer, so he most likely was not charged.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "There is no information in the article that can either confirm or refute what the answer's author has given here", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer is correct but it could offer some more detail. The chief officer worked for a North Korean shipping company and so he would not be charged.", "ebr_raw": "60", "ebr_num": 60, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not state what happened to the chief officer in question, thus it is not possible to answer the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not mention what happened to the chief officer from Ansan Shipping Company.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct, this info is not stated in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asked for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not provide information about what happened to the chief officer from Ansan Shipping Company?", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not mention what happened to the chief officer", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "human_1": {"answer": "It is not stated what happened to the chief officer from the Ansan Shipping Company.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Again, the question asks for outside information, but the respondent does not provide false info or anything that is not included in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state what happened to the chief officer", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the effect of all this on the officer mentioned is unknown.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct and supported.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state what happened to the chief officer", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this question would have required outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct and supported..", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "12": {"question_id": "6", "question": "How many people were involved in the oil transfer? ", "system": {"davinci": {"answer": "The report was signed off by a chief officer from one \u201cAnsan Shipping Company\u201d, believed to be a North Korean shipping company.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Three other people were involved in the oil transfer. ", "ebr_raw": "10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The previous answer was just duplicated here, where it does nothing to address the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The prosecution said that three men worked together on this.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response doesn't answer the question directly and missed the other people involved , as per statement 2.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer correctly names one of the people. Sentence 2 of the document indicates hat there were three more, the three Singaporean men who the story centers around. ", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "6"}}}, "gpt4": {"answer": "The article does not specify the exact number of people involved in the oil transfer.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The question asks for outside information, but the answer should have at least included the names that were cited in the article", "ebr_raw": "Score: 30. The response mentions nothing about the names cited in the article and therefore does not include relevant information to answer the question.", "ebr_num": 30, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "From the article, four people were involved in the oil transfer. Koh, Ong, Tan and the chief officer who signed the report", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There were four people involved in the oil transfer based on the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "human_1": {"answer": "It is not clear how many people were involved in the oil transfer. The article names three Singaporean men and the chief officer from Ansan who signed off on the report, which makes at least four.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "A valid attempt is made at answering the question with the limited information available", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer properly uses info from sentences 2 and 9 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "According to the article, there were at least four people involved in the oil transfer. The chief officier of Ansan Shipping Company signed off for the tanker to go to Singapore. Three workers, Ong Chou Hong, Benny Tan Chun Kiat, and Jeremy Koh Renfeng, one of whom was a manager and the other two cargo workers, were also heavily involved in the illicit tanker situation. It is possible more people were involved and not caught.  ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in all part of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "All available information is included in the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant, good and accurate based on the info in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}}}, "18": {"dataset": "inq-ext", "article_num": 11, "question_details": {"0": {"question_id": "0", "question": "Why is inflation expected to remain elevated? ", "system": {"davinci": {"answer": "Inflation is expected to remain elevated because of the following reasons:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "the answer is incomplete and not helpful, misses all relevant info.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "According to the article, inflation is expected to remain elevated due to global and domestic factors such as potential shocks to global commodity prices", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Inflation is expected to remain elevated and will come as global inflation moderates and the tight domestic labor market eases.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is incomplete. It missed the information stated in sentences 4,7,8,12,13,14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Inflation in Singapore is expected to remain elevated in the first half of 2023 due to both external and domestic factors. Externally, global inflation is likely to stay firm in the near term as global energy and food commodity prices remain elevated despite coming off their peaks in 2022. Domestically, businesses are likely to continue passing on elevated import, utilities, and labor costs to consumers. Additionally, home rentals and car prices are projected to remain high due to strong demand for rental housing and tight certificate of entitlement quotas, respectively.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3, 4 and 8 confirms the machine response, ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 12 provides additional information to answer the question ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more information could have been included", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}}}, "human_1": {"answer": "The article reports on multiple inflation measures that are expected to remain elevated. Import, utilities, labor, home rentals, and cars are all expensive. Possible reasons are external factors, such as potential shocks to global commodity prices caused by worsening geopolitical tensions and a jump in demand following the reopening of China \u2019s economy, as well persistent manpower shortages.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Inflation is expected to remain elevated and will come as global inflation moderates and the tight domestic labor market eases.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first two answer sentences are not relevant as they are stating things that are inflated in price rather than why they are that way. The rest of the answer lists accurate reasons but misses global energy prices being higher (sentence 4), strong housing demand and tight entitlement quotas (sentence 8), and persistent manpower shortages (14).", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and failed to mention the domestic factors as per sentences 4,7,8,13,14.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}, "human_2": {"answer": "Inflation in Singapore is expected to remain elevated because of both domestic and global economic factors. The labor market is tight worldwide, and global inflation is an issue. The price of food and energy are elevated globally. Domestically, these issues stem from high utlity and labor costs.  ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article infers the machine response is correct, the inflation is caused by both domestic and global factors", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the other factors as per sentences 8 and 12.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The 1st sentence is supported by sentence 3. The 2nd is not needed as the first part is not drawn from the document and the second part is merely restating that inflation is elevated. The 3rd is supported by sentences 4/13. Info from sentence 12 is needed however as it more fully explains the causes. The final sentence is supported by 7. Also info from 8 is missed which talks about demand and quotas for homes and cars.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Inflation in Singapore is expected to remain elevated in the first half of 2023 due to global inflation moderating and the tight domestic labour market. Externally, global inflation is likely to stay firm in the near term, as global energy and food commodity prices remain elevated despite coming off their peaks in 2022. Domestically, businesses are likely to continue passing on elevated import, utilities and labour costs to consumers, while home rentals and car prices are projected to remain high due to the strong demand for rental", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed other factors as per sentences 8,12,13 and14.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a comprehensive answer, but it was cut off before it could finish supplying key information", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-8 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first sentence in the response is incorrect as per sentences 1 and 2. While the rest of the response is correct, it missed the other factors mentioned in sentences 8,12,13 and 14.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Inflation in Singapore is expected to remain elevated in the first half of 2023 due to external factors such as potential shocks to global commodity prices caused by worsening geopolitical tensions and a jump in demand following the reopening of China\u2019s economy, as well as domestic factors such as persistent manpower shortages which could lead to higher labour cost pressures. Additionally, home rentals and car prices are projected to remain high due to the strong demand for rental housing and tight certificate of entitlement quotas respectively.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response seems accurate and complete, relevant and sourced from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes good use of sentences 12, 14, and 8 to provide a full answer to the quesiton.", "ebr_raw": "95", "ebr_num": 95, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4-8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "1": {"question_id": "1", "question": "How will this inflation affect the economy? ", "system": {"davinci": {"answer": " The inflation will affect the economy in the following ways:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is bad, incomplete, misses all relevant information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Prices of food and energy will increase, there will also be an increase in labor cost", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Core inflation is expected to average 3.5 to 4.5 percent. Business are likely to pass the elevated import, utilities, and labor costs to consumers. Home rentals and car price will remain high.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentences 4,5,6,7,8,13,14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The elevated inflation in the first half of 2023 could affect the economy in multiple ways. It may lead to higher consumer prices and an increase in the cost of living, which could potentially lower consumer spending power and slow down economic growth. Businesses may also face higher import, labor, and utility costs, which may lead to lower profit margins and reduced investments. Furthermore, high inflation may prompt the Monetary Authority of Singapore to consider tightening its monetary policy to maintain price stability, which could affect interest rates and borrowing costs. However, the inflationary pressures are expected to ease in the second half of the year, which could provide some relief and stabilize economic conditions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently with information from the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the whole of the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer appears to be accurate, but most of it is outside speculation rather than facts from the article.", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "According to the Minister of State for Trade and Industry, businesses are likely to continue passing on elevated costs to consumers, and home rentals and car prices are projected to remain high. The Singapore dollar is strong and other economic factors are mentioned; however, it is not clear if these are caused by inflation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 and 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The info included from sentences 7 and 8 is relevant and accurately used to answer the question. The info which follows in the answer is not relevant as the question is just looking to know the known effects and by only alluding to other items, nothing of substance is actually being said.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the impact of other elevated costs to the economy, as per sentences 7 and 13.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "human_2": {"answer": "Inflation in Singapore will affect the economy by generally raising prices of a number of goods and services. Housing costs are expected to remain high, and food and energy prices are expected to rise. Additionally, the Singaporean dollar's value will fluctuate depending on geopolitical tensions and how the economy is faring. Businesses will require extra support from the government to deal with reduced labor and rising demand. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response summarizes the whole of the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\"the Singaporean dollar's value will fluctuate depending on geopolitical tensions \" in the response is unsupported. The response failed to mention other effects as per sentences 7 and 8.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Sentence 1 is supported by 5-7, which note a general price rise as the answer states. Sentence 2 is supported by 8, however the answer leaves out high car prices. 13 supports the higher food/energy costs. The 3rd sentence is not supported by any sentences in the document. The final sentence is not correct as 17 indicates businesses many need such support.", "ebr_raw": "Score: 57", "ebr_num": 57, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The inflation outlook for Singapore in 2023 is dependent on both external and domestic factors. Externally, global inflation is likely to stay firm in the near term, as global energy and food commodity prices remain elevated despite coming off their peaks in 2022. Domestically, businesses are likely to continue passing on elevated import, utilities and labour costs to consumers, which could lead to higher prices for food and energy-related items. The Monetary Authority of Singapore had policy in October 2021, when global inflationary pressures", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Only the domestic part of the response is relevant to the question, all others are irrelevant. The response also missed other effects on the economy, as per sentences 8,12,13 and 14.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "An otherwise excellent answer was cut off before it could finish supplying the available facts from the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12-14 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first sentence of the response is irrelevant. Also, it wrongly linked the cause and effect of the higher prices for food and energy - related items. Lastly it doesn't mention higher labour cost pressures, as per sentence 14.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The elevated inflation in Singapore is expected to have an impact on the economy, as businesses are likely to pass on higher import, utilities and labour costs to consumers. This could lead to higher prices for food and energy-related items, as well as higher home rentals and car prices due to the strong demand for rental housing and tight certificate of entitlement quotas respectively. The Monetary Authority of Singapore has implemented policy in October 2021 to strengthen the Singapore dollar, which could help to reduce imported inflation. The Government has also", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is generally good and accurate, but the last sentence is cut off and incomplete.", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Info from sentences 7 and 13 is used well to start. The info from sentence 8 is relevant but it should be its own sentence as it is a separate thought from what come before... The final two sentences are not relevant as the one containing info from sentence 9 is talking about what the government is doing about inflation which is beyond the question's scope, and the final sentence is cut off and not really saying anything.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer was sufficient, but the final sentence was cut off", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11-14 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "2": {"question_id": "2", "question": "What is cost of living like in Singapore in 2023?", "system": {"davinci": {"answer": " Cost of living in Singapore in 2023 is likely to be higher than in 2022.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The document doesn't directly address or answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is technically correct, but was probably not even based on a reading of the article, as key information is missed", "ebr_raw": "Score: 25", "ebr_num": 25, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is unsupported. It misseed the information stated in sentences 5 and 6.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details about the cost of living in Singapore in 2023. However, it mentions that inflation is expected to remain elevated in the first half of the year before softening in the second half. All-items inflation for 2023 is cent compared to 6.1% in 2022, while core inflation is expected to average between 3.5% and 4.5%. Factors affecting the cost of living include global energy and food commodity prices, elevated import costs, utilities, and labour costs, as well as high home rentals and car prices.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 8 also adds up to the machine response. ", "ebr_raw": "75", "ebr_num": 75, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Relevant information from the article is included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The article does not describe specific figures related to the cost of living, but home rentals and car prices are projected to remain high due to the strong demand for rental housing and tight certificate of entitlement quotas respectively.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information is taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in that it the answer is not specifically stated. That being said that article does discuss living cots. In that way, the answer is also correct in referring to  housing/car costs as those affect cost of living. Info from sentence 7 should also be included which talk about prices rising for consumers as those affect living costs too.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and the correct answer is stated in sentences 5 and 6.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The cost of living is expected to remain high due to high demand", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "At least a partial answer could have been taken from the article", "ebr_raw": "30", "ebr_num": 30, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. In fact, the answer is stated in sentences 5 and 6.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification that the needed info is not present. The document talks about rising prices, but never sets a baseline for what the overall cost of living it like, rather just noting that it is going up.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " All-items inflation in Singapore for 2023 is cent, compared with 6.1 per cent in 2022. Over the same period, core inflation \u2013 which excludes accommodation and private transport costs \u2013 is expected to average 3.5 per cent to 4.5 per cent, compared with 4.1 per cent in 2022, the Government said.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The amount per cents of All-items inflation in Singapore for 2023 has been missed in both the passage and the response.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but could be more detailed", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 and 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct except that the passage and the response missed the amount of the All-items inflation.", "ebr_raw": "80", "ebr_num": 80, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The cost of living in Singapore in 2023 is expected to remain elevated in the first half of the year before softening in the second half of the year. All-items inflation in Singapore for 2023 is expected to be around 6 per cent, while core inflation \u2013 which excludes accommodation and private transport costs \u2013 is expected to average 3.5 per cent to 4.5 per cent. Home rentals and car prices are projected to remain high due to the strong demand for rental housing and tight certificate of", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is pretty good and relevant, but the last sentence is incomplete and misses a bit of info from the document.", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer does a good job summarizing the situation using sentences 1, 5, 6, and 8. The final sentence should be shortened however as it got cut off at the end.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is valid, but cut off", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 and 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "3": {"question_id": "3", "question": "What is causing inflation in Singapore? ", "system": {"davinci": {"answer": " The answer is simple. It is the cost of living.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is not very helpful, and misses most important information.", "ebr_raw": "40", "ebr_num": 40, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 12 provides information that can answer the question. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The geopolitical tensions and jump in demand following the reopening of China's economy. This lead to higher prices for food and energy-related items. Also, labor shortages are leading to higher labor costs.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It missed the information in sentences 4,7,8,12,13,14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Inflation in Singapore is caused by both external and domestic factors. Externally, global inflation, elevated energy and food commodity prices contribute to inflation. Domestically, businesses are passing on elevated import, utilities, and labor costs to consumers, while home rentals and car prices remain high due to strong demand for rental housing and tight certificate of entitlement quotas. In addition, potential shocks to global commodity prices caused by geopolitical tensions or a jump in demand following the reopening of China's economy could lead to higher prices for food and energy-related items. Persistent manpower shortages could also cause higher labor cost pressures.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a redundant question, but it is answered again sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article backs up the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article supports the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer to a duplicate question", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The article cites internal factors such as strong demand for rental housing and tight certificate of entitlement quotas driving up prices of home rentals and cars. It also discusses external factors, such as potential shocks to global commodity prices caused by worsening geopolitical tensions and a jump in demand following the reopening of China \u2019s economy, as well persistent manpower shortages.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 and 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The first answer sentence is supported by sentences 3 and 8. Answer sentence 2 is supported by sentences 12 and 14. ", "ebr_raw": "95", "ebr_num": 95, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed other causes as stated in sentences 7 and 13.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}, "human_2": {"answer": "Inflation in Singapore is expected to remain elevated because of both domestic and global economic factors. The labor market is tight worldwide, and global inflation is an issue. The price of food and energy are elevated globally. Domestically, these issues stem from high utlity and labor costs.  ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 and 13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it failed to mention other causes as per sentences 8, 12 and 13.", "ebr_raw": "30", "ebr_num": 30, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence references info from 12 but does not include the examples of worsening geological tensions and the reopening of China. That into should be included for completeness. The second sentence is not relevant as the 1st part is not in the document and the second part is merely repetitive. The 3rd sentence is supported by 4/13. The final sentence is supported by 7. Info from 8 is missing however as that includes additional factors.", "ebr_raw": "Score: 45", "ebr_num": 45, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Inflation in Singapore is expected to remain elevated in the first half of 2023 before softening in the second half of the year. This will come as global inflation moderates and the tight domestic labour market eases. Externally, global inflation is likely to stay firm in the near term, as global energy and food commodity prices remain elevated despite coming off their peaks in 2022. Domestically, businesses are likely to continue passing on elevated import, utilities and labour costs to consumers. Home rentals and car", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed other factors mentioned in 8,12,13 and 14.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer was again cut off before it could capture all of the relevant information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 12 gives another reason for the inflation.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first two sentences in the response are irrelevant, and it missed other factors as per sentences 8,12,13 and14.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Inflation in Singapore is caused by both external and domestic factors. Externally, global inflation is likely to stay firm in the near term, as global energy and food commodity prices remain elevated despite coming off their peaks in 2022. Domestically, businesses are likely to continue passing on elevated import, utilities and labour costs to consumers, while home rentals and car prices are projected to remain high due to the strong demand for rental housing and tight certificate of entitlement quotas respectively.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer includes info from sentences 12, 4, 7, and 8. It misses info from sentences 12 and 14, While the info that is there currently is relevant and helpful to answer the question, the info from sentences 12 and 14 is needed to fully explain the situation.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4-14 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "4": {"question_id": "4", "question": "How could a stronger Singaporean dollar reduce inflation?", "system": {"davinci": {"answer": " The answer is simple. A stronger Singapore dollar means that the price of imported goods will be higher. This will reduce the inflation rate.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't directly address the question, answer is speculation/outside information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "A stronger local currency decreases the prices of imports.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the high price on imported goods, a stronger Singaporean dollar will help to reduce the price on importation", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain how a stronger dollars would reduce inflation.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is unsupported and nonsense. It missed the information in sentence 10, thought the answer is not explicitly mentioned in the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "A stronger Singaporean dollar could reduce inflation by making imported goods and services cheaper. When the domestic currency is strong compared to other currencies, it can purchase more goods and services from foreign countries for the same amount of money. This lower cost of imported goods and services can help keep overall prices in the economy lower, which in turn leads to reduced inflation.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is a valid answer to the question, but the text only hints at this outcome, and the answer itself is based on outside knowledge", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 and 15 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer appears to be factually accurate, but it was not taken from the article text.", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response provides answers that can be deduced from the article.", "ebr_raw": "85", "ebr_num": 85, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated how a stronger Singaporean dollar could reduce inflation. However, it can be inferred from the discussion of imports that a stronger Singaporean dollar may decrease the price of imported goods, thereby counteracting inflation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence of the answer is correct in stating that the info is not given. The second sentence of the answer would require outside info on how international trade works.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears correct but is not stated in the document anywhere. This makes the response incorrect and unsupported. The correct reason is stated in sentence 10.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "It will reduce inflation when the stronger dollar helps to reduce prices on importation", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer could at least have mentioned that a stronger currency will reduce inflation to some extent", "ebr_raw": "20", "ebr_num": 20, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 10 in the docuent,the Singapore dollar has since strengthened by 7.8 per cent against a basket of currencies of its major trading partners.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that this process is not explained in the document. Sentence 10 mentions their dollar strengthening against its trading partners, but it is not stated how that decreases inflation.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "A stronger Singapore dollar would reduce imported inflation by making imported goods cheaper. This would help to reduce the cost of living in Singapore.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Though the response sounds correct theoretically, it is unsupported by the passage, as per sentence 10.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 rallies round the machine response, a stronger Singaporean dollar will help to reduce the inflation caused by imported goods. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is unsupported by the passage.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "A stronger Singaporean dollar can reduce imported inflation by making imported goods and services cheaper. This in turn can help to reduce the cost of living and overall inflation in Singapore. Additionally, a stronger Singaporean dollar can also help to reduce the cost of imported raw materials and inputs used by local businesses, which can help to reduce the cost pressures faced by businesses and ultimately help to reduce inflation.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer seems good and is likely accurate but draws conclusions and/or uses information not explicitly stated in the document to answer the question", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question is never expressly stated in the document. The answer can be implied given the material that is given, but there is no explanation as provided in the answer expressly present within the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The text only hints at this answer, and the rest is all (reasonable) speculation from outside information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The above sentences points out what the Singaporean dollar will do", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "7"}}}}}, "5": {"question_id": "5", "question": "What are some exernal and domestic factors that could affect inflation in Singapore this year?", "system": {"davinci": {"answer": " The following are some of the external and domestic factors that could affect inflation in Singapore this year.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and misses all the relevant information contained in the article ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 12 states the external factors and the domestic factors are labor shortages, prices of house rent. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Geopolitical tensions, a jump in demand after China reopened, and manpower shortages are causing inflation.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentences 4,7,8,12,13,14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "External factors affecting inflation in Singapore this year include global inflation, energy and food commodity prices, potential shocks to global commodity prices due to geopolitical tensions, and the impact of China's economic reopening on demand for various goods. Domestic factors include import costs, utilities costs, labor costs, home rental prices, car prices, and tightness in the domestic labor market.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is another redundant question, but the answer is sufficient", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 and 15 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "In addition to the machine response, sentence 12 provides information on some factors that could affect inflation. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "human_1": {"answer": "The article cites internal factors such as strong demand for rental housing and tight certificate of entitlement quotas driving up prices of home rentals and cars. It also discusses external factors, such as potential shocks to global commodity prices caused by worsening geopolitical tensions and a jump in demand following the reopening of China \u2019s economy, as well persistent manpower shortages.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 8 and 12", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in stating the examples that it does as they address why inflation could be high. Given the question just asks about inflation in general and not how it could be higher, there also needs to be info which talks about how it could lower. Such into related to currency is included in sentences 3, 9, and 10. ", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed other factors as per sentences 7 and 13.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "human_2": {"answer": "Globally, lack of a strong labor force and the rising cost of food and energy are factors that are affecting inflation in Singapore. These issues also tie into domestic factors, and the cost of labor and supplies is inflated in Singapore. The strength of the Singaporean dollar is also fluctuating, which is affecting the economy and inflation. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 12 also adds up to the external factors that could affect inflation", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer to another duplicate question", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The last sentence of the response is unsupported. The response failed to mention other factors as per sentences 4,7,8,12,13,14.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The info from doc. sentence 4 in answer sentence 1 is correct, but the document does not talk about worldwide labor but rather domestic labor issues (15). For sentence 2, supplies were not talked about in the document, but the domestic labor market was in sentence 15. Other domestic factors are listed in 8. Sentence 12 lists external factors. Sentence 3 talks about the Singapore dollar, supported the final answer sentence.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Externally, global inflation is likely to stay firm in the near term, as global energy and food commodity prices remain elevated despite coming off their peaks in 2022. Domestically, businesses are likely to continue passing on elevated import, utilities and labour costs to consumers, while home rentals and car prices are projected to remain high due to the strong demand for rental housing and tight certificate of entitlement quotas respectively. Persistent manpower shortages could also lead to higher labour cost pressures.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The above sentences gives further details on factors that contribute to domestic and external inflation.", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed geopolitical factors causing higher prices for food and energy - related items, as per sentences 12 and 13.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": " External factors that could affect inflation in Singapore this year include global energy and food commodity prices, geopolitical tensions, and a jump in demand following the reopening of China\u2019s economy. Domestic factors that could affect inflation in Singapore this year include labour market shortages, higher labour costs, and higher prices for rental housing and car prices due to strong demand. The Monetary Authority of Singapore has also implemented policy to strengthen the Singapore dollar, which could help to reduce imported inflation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and relevant and answers the question with info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Info from sentences 4, 12, 14, 8, and 3 are used. The answer does miss elevated utilities cost (sentence 7).", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12-15 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "6": {"question_id": "6", "question": "Why are global energy and food prices currently elevated? ", "system": {"davinci": {"answer": "The answer is simple. The world is running out of energy and food.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is unhelpful speculation, mostly inaccurate. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer was not based on a reading of the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 12 provides answer to the question. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't go into detail as to why the food and energy prices remain high.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentences 12 and 13.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide a specific reason for the elevated global energy and food prices. However, such prices can be influenced by various factors, including supply chain disruptions, increased demand, geopolitical tensions, adverse weather conditions affecting production, and changes in global trade policies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 and 14 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer seems to be based on speculation, when at least a partial answer could have been taken from the article", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not stated why global energy and food prices are currently elevated.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 12 mentions why food prices are currently elevated", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. The info is not given.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. But the answer is stated in sentence 12 and 13.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "The labor market is tight worldwide, and global inflation is an issue. Domestically, these issues stem from high utlity and labor costs. Inflation is causing prices to rise worldwide, and businesses are raising prices to try and offset their costs. Consumers are now paying the price. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is another valid answer to a duplicate question", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It failed to mention the reasons for the price elevation, as per sentences 12 and 13.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Doc. sentence 12 contains the info needed to answer the question. The 1st answer sentence merely repeats that prices are high and references the world labor market which is not talked about in the doc. The second sentence talks about domestic issues, which are not what the question asked about. The rest of the info is not drawn from the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Global inflationary pressures are relatively modest and geopolitical tensions could lead to higher prices for food and energy-related items.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the other factor as per sentence 12.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer could have used more detail", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The above sentences shows the reason why there is an increase in food and energy prices. ", "ebr_raw": "25", "ebr_num": 25, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "'Global inflationary pressures are relatively modest' in the response is irrelevant. The response failed to mention a jump in demand following the reopening of China \u2019s economy as one of the causes, as per sentence 12.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Global energy and food prices are currently elevated due to a combination of factors, including geopolitical tensions, increased demand from China's reopening economy, and supply chain disruptions caused by the pandemic. These factors have caused a surge in global commodity prices, which has in turn led to higher prices for energy and food items.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is generally correct and uses info from the document", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The info from sentences 12 and 13 contained within the first 2 answer sentences is accurate. There is no mention in the document of supply chain disruptions however. The final answer sentence is also not supported by any of the document's sentences and rather is an implied summary of the information already given.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11-13 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "7": {"question_id": "7", "question": "Why do businesses pass costs on to consumers?", "system": {"davinci": {"answer": " Businesses pass on costs to consumers because they want to make more profit.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is speculative, and the question isn't directly addressed in the article.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "That answer is technically correct, and the article doesn't provide any additional insights", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the current situation in Singapore as described by the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into why businesses pass their costs on to customers, just that they do.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Businesses pass costs on to consumers because they need to maintain their profit margins and financial sustainability. When businesses face higher input costs, such as elevated import, utilities, and labor expenses, they often choose to increase the prices of their goods or services to cover these additional expenses. By passing these costs on to consumers, businesses can continue to earn sufficient returns, cover their operating costs, and remain competitive in the market.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is a reasonable inference, and a valid answer to the question, but it was not based on the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from a summary of the trend of occurrence in the article. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into why businesses pass their costs on to customers, just that they do.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the situations reported in the article. ", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This was a valid answer, but it was not taken from the article text (nor does it acknowledge that the article does not directly address the question)", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}}}, "human_1": {"answer": "It is not stated why businesses pass costs on to consumers. However, it can be inferred that higher costs require businesses to set higher prices in order to remain profitable.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information, but relevant passages from the article are still alluded to", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not specify the reason for passing of costs to consumers but it could be due to the high price on imported goods. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer is correct in noting that the info is not given. The info which follows in the answer is speculation that would require info from outside the document on how businesses generate profits.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response looks correct but this is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "It is not stated but can be inferred that businesses are passing extra costs onto consumers to try and reduce what they are paying. Due to global inflation and domestic issues that are contributing to this inflation, businesses in Singapore are having to pay more than they normally would for labor and supplies. Businesses are asking for additional support from the Singaporean government. As they are not receiving the support they need, they have likely begun to look to consumers to make up for these extra costs. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response summarizes the article.", "ebr_raw": "85", "ebr_num": 85, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the information in the text", "ebr_raw": "95", "ebr_num": 95, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response looks good but is unsupported by the document. The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence is correct in identifying the info is not given in the document. The rest of the answer is speculation which requires additional info not present in the document. ", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Businesses are likely to continue passing on elevated import, utilities and labour costs to consumers due to the strong demand for rental housing and tight certificate of entitlement quotas. Additionally, persistent manpower shortages could also lead to higher labour cost pressures.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not state why businesses may continue to pass cost on to customers. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is marked as incorrect, as the last sentence is irrelevant and the reasons are not explicitly stated in the passage.", "ebr_raw": "30", "ebr_num": 30, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Businesses pass costs on to consumers in order to maintain their profit margins and remain competitive in the market. By passing on costs, businesses are able to offset the rising costs of production and remain profitable. This helps them to remain competitive and provide goods and services at competitive prices.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is relevant and likely correct but draws conclusions using information not directly in the article", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer makes sense but it did not come from within the document. It is not explained why business pass on costs within the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not state why businesses pass costs on to consumers?", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}}}}}, "19": {"dataset": "inq-ext", "article_num": 12, "question_details": {"0": {"question_id": "0", "question": "How are foreign powers reacting to the election? ", "system": {"davinci": {"answer": " The US and EU are trying to influence the election.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no answer available in the text, and this was clearly either made up or taken from an external source", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't say what other countries are feeling about the election. And it certainly doesn't say the US and EU are trying to influence the election.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The above sentences states how the foreign powers are reacting to the elections. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document doesn't talk about foreign influence on the election.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not address the thoughts held by foreign powers. Paetongtarn believes that coups make other countries see the country in a different light and not want to trade with them, but the countries themselves did not say that within the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide information on how foreign powers are reacting to the upcoming election in Thailand.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The foreign powers don't want to trade according to the above sentences", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification. Such info is not given in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The foreign powers don't want to trade because of how they already perceive the election", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}}}, "human_1": {"answer": "It is not described how foreign powers are reacting to this election. However, Paetongtarn claims that coups have caused foreign powers to not want to trade with Thailand.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information is included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21-23 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the info is not given. It is also correct in identifying Paetongtarn's statements about the matter, drawing from sentences 21-23.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The foreign powers do not want to trade with the people", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. The document does not provide the opinions of foreign powers. Sentences 21-23 address Paetongtarn's opinion of how other countries see them, but those are not the stated opinions of the countries themselves.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " Thailand's election is shaping up to be another grudge match between warring elites in Southeast Asia's second-biggest economy. Paetongtarn said she remains concerned about the impact of the country's intractable power struggle involving her family, including coups, which she said makes Thailand \"go backwards\". She said it also makes the world see the country in a different light and reduces opportunities for everyone.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer missed the focus of the question, though the article provided no specific answers, either", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The above sentences provide additional information on how the foreign powers are reacting to the election.", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not state what foreign powers think. The info expressed in the answer was the opinion of Paetongtarn and what she thought other countries thought.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Only the last sentence in the response is relevant to the question, and the response also missed the foreign reaction as per sentence 23.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Foreign powers have been cautious in their reactions to the upcoming election in Thailand. They are concerned about the country's intractable power struggle and the potential for further coups, which could have a negative impact on trade and other opportunities.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not specifically address how other countries are reacting. The information in the answer is drawn from sentences 21-23, but it is info provided by Paetongtarn and not by foreign countries, so it does not answer the question. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer fits with the overall theme of the article, but it attributes Paetongtarn Shinawatra's concerns to foreign investors", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 20-24 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not accurate in that the opinions of foreign powers on the election specifically were never given in the document. Paetongtarn talked about coups in sentences 21-26 but that was what she was saying, not foreign powers themselves and it was about coups in the country, not the election itself.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "1": {"question_id": "0", "question": "How did these coups affect ordinary citizens in Thailand?", "system": {"davinci": {"answer": "The coups in 2006 and 2014 were not popular with the majority of Thai people. The 2006 coup was a bloodless coup. The 2014 coup was a violent coup.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer may be valid, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The above sentences states how the coup affects ordinary citizens. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Nothing in the answer is stated in the article. When the coups happened, the was much economic growth which would have been reversed under the conservative establishment.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response missed the information stated in sentences 21,22,23,24,25. The response answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The information in the answer is drawn from outside the document. The info needed to answer the question comes from 24, which notes that the opportunities of everyone are reduced.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The coups that ousted Thaksin and Yingluck Shinawatra in 2006 and 2014 would have affected ordinary citizens in Thailand in various ways, such as increased political instability, more government control over the media, heightened censorship, and restrictions on public gatherings and protests. The coups would also have disrupted the continuity of government policies and could have possibly affected economic progress and trade, as Thailand was seen as less stable by the rest of the world.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is reasonably inferred by the information available in the document. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer may be accurate, but it was not based on the article (which does not really address this question sufficiently)", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The examples given in the first answer sentence are not drawn from any of the document's sentences. The second answer sentence is supported by sentences 22-25 as all the information in the second answer sentence can be aligned to info from those sentences.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response may be correct but contains speculation and/or outside information not directly supported by the article ", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}}}, "human_1": {"answer": "It is not explicitly stated how these coups affect ordinary citizens in Thailand.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article mentions that the coup disrupted the ongoing work of Thaksin and Yingluck", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response misses all of the relevant information in the article that could at least partly answer the question, and instead uses outside information not contained in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100.", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does provide an example of how people are affected in sentence 24. There it states that opportunities are reduced for everyone. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "human_2": {"answer": "The coups lead to a lot of political instability. Though not explicitly addressed in the article, it is likely this led to economic crises and distrust in leadership for many citizens, which is affecting the current election. It also means many policies were overturned overnight, and promises that were made by leaders were not fulfilled. Ordinary citizens are likely the ones who suffered most because of the coups. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, and it fits with the theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response puts a good summary of the article. ", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't specifically address the question. The response contains speculation", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response is all inferral which is unsupported. The response doesn't mention the effects on the ordinary citizens in Thailand as per sentences 22,23,24,25,26,27.", "ebr_raw": "20", "ebr_num": 20, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The coups have frozen the country and reduced opportunities for citizens, making it difficult for them to have better livelihoods. The coups have also caused the world to view Thailand in a different light, making it difficult for the country to engage in international trade.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer was taken directly from the text, albeit a bit too literally", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21-25 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21-25 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response overall is relevant and accurate based on info in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "The coups had a negative effect on ordinary citizens in Thailand, as they disrupted the economic growth that had been overseen by Thaksin and Yingluck. This resulted in reduced opportunities for citizens, as foreign investors were less likely to trade with Thailand due to the instability caused by the coups. Additionally, the coups froze the country's progress, preventing citizens from having better livelihoods.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer includes some reasonable speculation, but it at least fits with the limited information in the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is good and complete, and based on information in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21-25 summarizes the machine response.  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "2": {"question_id": "0", "question": "Why is this candidate generating nostalgia among voters? ", "system": {"davinci": {"answer": " Nostalgia is a sentimentality for the past, typically for a period or place with happy personal associations. It is a sentimental yearning for the return of something or someone from the past, a wistful affection for the past, its personalities, and events, especially the \"good old days\" or the \"golden age\" of a bygone era.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 1 and 2 provides answer to the question. Paetongtarn is from a family that has been in power in the past. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer only gives a definition of what nostalgia is. Both Paetongtarn's father and aunt were were both prime ministers until being ousted by coups. They did a lot of the regular people which is why they were ousted. They were liked by the citizens and are feeling nostalgic now about those times since Paetongtarn is a part of that family.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is somewhat relevant but doesn't really answer the question and misses all important information.", "ebr_raw": "40", "ebr_num": 40, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as it is not drawn from the document. The info needed to answer the question comes in 1 and 2 where she is invoking her families legacy and comparing in places also visited by her relatives.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Paetongtarn Shinawatra is generating nostalgia among voters because she is the youngest daughter of Thaksin Shinawatra, Thailand's former prime minister who won power through landslides in the past. Her aunt Yingluck was also a former prime minister. Both Thaksin and Yingluck were largely popular, achieving massive election victories, which has created nostalgia among supporters.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and supported by facts in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "Paetongtarn Shinawatra is the daughter of Thaksin and niece of Yingluck, two previous prime ministers. The article is likely referring to these family connections. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer to a somewhat subjective question", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The first answer sentence is correct in using info from sentence 2. Nostalgia is based on invoking the past, so that sentence is fitting. The second answer sentence is supported by sentence 1 in that it makes mention of her family legacy. Info from sentence 8 however is missing which talks about the same type of policies also being Paetongtrarn as were used by her family before her.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article.  ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is good and relevant and uses information from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "The candidate's father and aunt both held office in the past. However, these leaders were ousted by a coup and have 'unfinished business' in office. The candidate is relying on previous support and an emphasis on completing this unfinished business in office to generate support among voters. The candidate is using the same 'playbook' that her aunt and father used to reinstate similar policies that were popular during their years as leaders. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1,2, and 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and complete and uses information stated in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Paetongtarn is campaigning hard in the vote-rich rural strongholds of the Shinawatra family\u2019s Pheu Thai political juggernaut, hoping to reignite the kind of fervour that swept father Thaksin and aunt Yingluck to power in unprecedented landslides. She is promising Pheu Thai will complete unfinished business from three stints in office since 2001, all of which were cut short by court rulings and military coups that it says were orchestrated by Thailand", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer was excellent, but it was cut off", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is good and includes the relevant information from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "She is from a family of past leaders according to sentence 1 and 2", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "This candidate is generating nostalgia among voters because she is part of the Shinawatra family, which has a legacy of populism and massive election victories. The Shinawatras have long been popular among the working class, and Paetongtarn is hoping to reignite the fervor that swept her father Thaksin and aunt Yingluck to power in unprecedented landslides. She is also promising to complete unfinished business from the Shinawatras' three stints in office since 2001, which were", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 summarizes the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is valid, but cut off", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer combines information from sentences 1-4 of the document to appropriately answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 2 conforms to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "3": {"question_id": "1", "question": "How do these policies have the power to change lives? ", "system": {"davinci": {"answer": "The policies are designed to change the lives of the people.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The policies include minimum wage increases, utilities subsidies, high-speed rail systems, and better infrastructure. All of these would enrich the lives of the citizens.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 8 lists the kind of changes that the policy will bring. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response missed the information stated in sentence 8.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer merely repeats that the policies have the power to change lives without explaining why, which is what the question wants to know. The document never specifically says how the policies will change lives. It can be assumed that the list of things in sentence 8 will improve the quality of life, but that is not stated within the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The policies have the potential to change lives by offering minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The policies the answer lists from sentence 8 have the potential to change lives, but the document does not state how these things will do that. For example, the policy raise minimum wage would likely have the affect of people being able to afford more necessities, improving health and well being. The document does not go into that kind of detail however, so the answer is not answerable given the lack of info in the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not clear what specific policies are being referred to. Paetongtarn seems to be making a general claim about the stability of the politics in Thailand and its benefits.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_all", "feedback": "Paetongtarn has mentioned minimum wage increases, utilities subsidies, and high-speed rail systems and infrastructure to manage floods and droughts.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response missed the answer which is stated in sentence 8.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "A relevant answer could have been taken from a different part of the article", "ebr_raw": "20", "ebr_num": 20, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The policies can bring about a better minimum wage, utilities subsidies and infrastructure to manage floods ", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first answer sentence is not quite correct in that the policies being referenced are the policies her family sought to install in the past (sentences 4 and 5). It is more accurate to say that it is not stated how the policies would change lives. The second sentence is an accurate summarization of the main point of sentences 5 and which is relevant to the question at hand as it discusses policies and changing lives.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "human_2": {"answer": "Paetongtarn wants to provide minimum wage increases, utilities subsidies, high-speed rail systems and infrastructure to manage floods and droughts, and to descale military power in Thailand (which increased through the conservative coup). She is focusing her campaign on a people-centered approach that will have direct impact in the lives of millions. Economic growth also provides more work and wage improvements for citizens around the country. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is never stated in the document how these things will change lives. Rather the document just repeatedly states that Paetongarn asserts that they will, The questions is thus not answerable given the info in the document. The first two answer sentences just list ideas but do not state how these things will change lives. It can be implied that increased wages will improve lives, but this is never stated directly. Additionally, the final sentence of the answer is not drawn from the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is overall good and accurate in regard to information it uses from the article, but it contains some inference or speculation", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The policies include minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts. These policies are designed to improve the lives of the working class and provide them with more economic opportunities.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 and 8 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first answer sentences lists policies, but the question was not asking what the policies were and is therefore not relevant. The second answer sentence is not backed up by any of the sentences in the document. It is never expressly stated within the document how exactly the policies can change lives. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence in the response is unsupported.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The policies proposed by Paetongtarn and Pheu Thai include minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts. These policies have the potential to improve the lives of working class people by providing them with more money to spend, access to better infrastructure, and more reliable utilities.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer uses sentence 6, but the politics mentioned were related to coups and not further expanded on. The policies given in the portion of the answer that follows are are all discussed in the article (sentence 8 of document) but the document does not state the info used in the last sentence of the answer. So, the policies are given, but their significance is not explained and they do not match up to what is being asked by the question. The info is not available.", "ebr_raw": "30", "ebr_num": 30, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5,6 and 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in sentence 1 of the answer (drawn from sentence 8 of the document) lists the policies but that is not what the question is looking to know. Further, sentence 2 of the answer does answer the question but the information is not drawn from the document. The document does not specify how the policies are expected to change lives.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "4": {"question_id": "1", "question": "Why are Thaksin and Yingluck avoiding prison convictions?", "system": {"davinci": {"answer": " Because they are guilty.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The respondent completely ignores the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The prison convictions will prevent their comeback according to sentence 7.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the article does not explain why Thaksin and Yingluck are avoiding prison convictions, it's reasonable to infer that it would not be safe for them to become political prisoners of some of the very people who orchestrated the coup.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct, but it is unsupported. The correct answer is because they want to have their  political comebacks, as per sentence 7.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not backed up via any of the document's sentences. The document does not state why they want to avoid prison convictions. Common sense would indicate that they would want to avoid them as they do not want to go to prison, but that info is not stated in the document. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Thaksin and Yingluck are avoiding prison convictions because their allies believe that the convictions were designed to prevent their political comebacks.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is reasonable to infer that Thaksin and Yingluck are avoiding prison convictions because they don't want to go to prison.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not state the reason they want to avoid prison convictions. The natural implication would be that they want to avoid going to prison, but this is not stated in any of the document's sentences. The answer merely repeats what is in sentence 7 without effectively answer the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is contained in the article but not really relevant and doesn't answer the actual question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "human_1": {"answer": "It is not explicitly stated, but it can be inferred that because these two former leaders were ousted in coups, they would be arrested if they returned to the country and they would rather live in exile than be imprisoned.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "A valid attempt is made at answering the question with the limited information available", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and seems to accurately summarize the answer based on the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The very first portion of the answer is correct. It is not stated why they want to avoid convictions. The middle portion of the answer about them being former leaders and being arrested if they come back is just restating already known info and is therefore not relevant. The Final portion of the answer makes common sense, but it is not stated within the document.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not give specific reasons as to why they are avoiding prison convictions. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "While the article does not explain why Thaksin and Yingluck are avoiding prison convictions, it's reasonable to infer that it would not be safe for them to become political prisoners of some of the very people who orchestrated the coup.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 7 in the document, prison convictions were designed to prevent their political comebacks, so they wanted to avoid them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "Their allies say the prison convictions were designed to prevent their political comebacks.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer essentially gives as much information as is available regarding the \"why\" question", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is clearly stated in sentence 7", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 affirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is based on the article, but doesn't really answer the question. The question itself is rather poor, and the article doesn't provide a direct answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "Thaksin and Yingluck are avoiding prison convictions because their allies believe that the convictions were designed to prevent their political comebacks.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer isn't really complete or relevant, though it is from the document. The question itself is bad.", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer could be a bit more complete, but is relevant and useful given the limited information about it provided in the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "5": {"question_id": "1", "question": "Why are rural areas the Shinawatra family's political strongholds? ", "system": {"davinci": {"answer": " The answer is simple. The rural areas are the Shinawatra family's political strongholds because the rural areas are the Shinawatra family's political strongholds.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the rural areas are the Shinawatra family's political strongholds mainly because they had done gone things for the rural areas while they were in power in the past.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not useful, but the document itself doesn't contain a direct answer", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain the reasons for the rural popularity of the family. However, the article does talk about the family's policies which include minimum wage increases, utilities subsidies, high-speed rail systems and better infrastructure. The rural communities would benefit from these programs.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is incoherent, but the article does not really give an answer to the \"Why\" question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not provide a reason, instead it just repeats information. The document does not specifically address the topic, but sentence 18 notes that the Shinawatras support populist policies. It can be assumed that these policies are appealing to those in the areas in question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide a direct reason for why the rural areas are the Shinawatra family's political strongholds. However, it could be related to their history of offering programs and policies that benefit poorer, working-class citizens, such as minimum-wage increases and utilities subsidies, which may have garnered them more support in rural areas.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes a lot of assumptions, but they are based on factual information in the text", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the article. ", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct, sourced from the article and makes reasonable inferences", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from how loved the Shinawatra family was and the good things they had done in the past in the article. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}}}, "human_1": {"answer": "It is not stated why the rural areas are the Shinawatra family's political strongholds.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information, and the answer does not resort to unfounded speculation", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. The info needed to answer the question is not given. Sentence 17 notes that appeal to the poor with populist policies but the connection is not well enough made to link this to document sentence 2. Any connection would rely on outside info.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They brought changes to the rural areas in their first year", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "We can infer from the article that development and changes were brought to the rural areas during Thaksin and Yingluck regime", "ebr_raw": "10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The explicit answer to this question was not given in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 1 in the document, the Shinawatra family had legacy of populism and massive election victories in these areas.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The Shinawatra family has a long history of offering populist policies such as minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts, which have been popular among rural voters.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The rural area is a vote rich stronghold, and Shinawatra family have met the need of the rural area to some extent during their previous time in power.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears accurate and is based on information provided in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-4 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The Shinawatra family has a long history of populism and has been able to win the support of the working class in rural areas. They have implemented policies that have improved the lives of the rural population, such as minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts. This has earned them the loyalty of the rural population, making them a powerful political force in these areas.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 5 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence can be drawn from information in sentences 1 and 2 of the document. Answer sentence 2 can be drawn from information in sentence 8. The final sentence of the answer is a summery which is not drawn from the document however, as there is no specific info that says this within the document.", "ebr_raw": "Score: 83", "ebr_num": 83, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-8 summarizes the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "6": {"question_id": "2", "question": "How does the candidate hope to change lives in a sustainable way?", "system": {"davinci": {"answer": " The candidate is a billionaire. She is a member of the family that has been in power for 15 years. She is a member of the family that has been in power for 15 years. She is a member of the family that has been in power for 15 years. She is a member of the family that has been in power for 15 years. She is a member of the family that has been in power for 15 years. She is a member of the family that has been in power for 15", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "By sustainable she means things like minimum wage increases, utilities subsidies, and high-speed rail systems, and infrastructure to manage floods and droughts.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 8 provides answer to the question. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incorrect and the last part is unsupported and repetitive. The response missed the information stated in sentences 8,21,22,23,24,25,26,27.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is repetitive and discusses background facts that are not relevant to answering the question. The question can be answered however by listing the policies given in sentence 8. Those are the things that Paetongtarn wants to enact.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The candidate, Thailand Paetongtarn Shinawatra, hopes to change lives in a sustainable way through stable politics.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 8 lists out what Paetongtarn is offering the people, which will change the lives of the people.", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "While technically correct, the answer misses a lot of relevant information", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer seems rather flippant, drawing as it does from sentence 6, but it does suffice to answer the question. It is further backed up by sentences 26-27, where it is said that coups should be avoided so the country can progress.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The article does not describe what sustainable means in this context. Some descriptions of policies such as minimum wage increases are mentioned.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "By sustainable she means things like minimum wage increases, utilities subsidies, and high-speed rail systems and infrastructure to manage floods and droughts.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response failed to mention utilities subsidies , and long - promised high - speed rail systems and infrastructure to manage floods and droughts, as mentioned in sentence 8.", "ebr_raw": "20", "ebr_num": 20, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "More detail could have been given ", "ebr_raw": "30", "ebr_num": 30, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 8 mentions how the candidate hopes to change lives. ", "ebr_raw": "10", "ebr_num": 10, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the specific into needed is not in the document. There are no sentences which specifically state how exactly all this will be accomplished in detail. That being said, policies are listed in sentence 8, and the inference can be drawn that those will change lives in the manner described given the context where they are included in the document.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. It is not expressly defined how lives will change in this way. Sentence 8 lists policies that Paetongtarn hopes to implement, but there is no info given as to how these policies will actually affect the people. The answers can be implied, but the document does not specifically provide the details.", "ebr_raw": "85", "ebr_num": 85, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 8 talks about how the candidate intends to change life in a sustainable way", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "At least a partial answer could have been taken from the article", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response misses the information that may be relevant to the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The candidate is promising Pheu Thai will complete unfinished business from three stints in office since 2001, all of which were cut short by court rulings and military coups that it says were orchestrated by Thailand\u2019s conservative establishment. She is campaigning hard in the vote-rich rural strongholds of the Shinawatra family\u2019s Pheu Thai political juggernaut, hoping to reignite the kind of fervour that swept father Thaksin and aunt Yingluck to power in unprecedented landsl", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "A lot of information is missing", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The candidate is also promising an increase in minimum wage, infrastructure to manage flood and drought among others. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The 1st answer sentence gives background information which is not relevant to what the question asked. The second answer sentence is not relevant either as it is talking about the campaign process and not how she is hoping to change lives. Info needed to answer the question can be drawn from sentence 8 would answer the question, as a list of the policies would suffice in this instance. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. The response failed to mention the stable politics as per sentence 6.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The candidate is campaigning on a platform of minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts. She believes that these policies will help to create a more stable political environment, which will in turn lead to more sustainable changes in people's lives.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer drawn from sentence 8 provides an answer to the question. The second answer sentence however is not drawn from the document as there is no sentence within it expressing this, and it therefore should not be in the answer.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is essentially a duplicate question, but it is again answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 and 8 corresponds to the machine response. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence, drawn from document sentence 8, serves to answer the question. The second sentence of the answer contains info not stated within the document. It is not expressly said in any of the sentences that she believes this.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}}}, "7": {"question_id": "2", "question": "How would political comebacks from Thaksin and Yingluck affect the country?", "system": {"davinci": {"answer": " The answer is simple. It would be a disaster.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This respondent isn't even trying to answer the questions", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the good things they did while in power, their political comeback will most likely bring about positive changes to the country.  ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain what would happen if Thaksin and Yingluck came back to power.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported. The effect on the country is stated in sentence 8.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. 17 notes that they are both loathed and loved, Beyond that it is not possible to say as the document does not address the future in this respect.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Political comebacks from Thaksin and Yingluck could lead to increased unity and voter turnout among the working class for the Pheu Thai party, as their family has overseen much economic growth in the country. However, both leaders are opposed by many within Thailand and have faced accusations of cronyism, and of creating wasteful populist policies to buy off the poor. Their return could also escalate the ongoing power struggle involving the Shinawatra family and lead to further setbacks, including the risk of more coup attempts which have in the past caused the country to \"go backwards\".\n", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is a reasonably inferred by the information in the article. ", "ebr_raw": "90", "ebr_num": 90, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "A bit more detail would help the answer", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Everything in the machine response can be deduced from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly looks at both sides of the issue as presented in document sentences 1, 2, 6, 17, and 21.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and seems to be reasonably inferred from the information in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_1": {"answer": "It is not explicitly stated, but the fact that Thaksin's daughter Paetongtarn is running for office indicates that the Pheu Thai party is still a political force. Therefore, political comebacks from former leaders would probably strengthen it further.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article backs up the machine response, their comeback would have brought about the continuation of their unfinished business and bring more development to the country", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Relevant information is taken from the article, but the question cannot be answered without outside information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response contains some speculation and misses some relevant information from the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first portion of the answer is correct in picking up on the info not being included within the document. That is the only info that need be included in the answer as the speculation which follows is not entirely in line with the document. In fact, there is contradictory info contained in sentence 17 which notes they are loathed by some in addition to being popular with others. So, it could be inferred that it would divide the country.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate, the article doesn't state a specific answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "A lot of information from the article is relevant, at least as a partial, indirect answer", "ebr_raw": "65", "ebr_num": 65, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The political comebacks will most likely bring about more positive improvement to the country since they brought some noticeable changes during their past tenure.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 8 in the document, their comebacks can continue their previous policies. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "Thaksin and Yingluck were both popular and had overseen much economic growth during their time in office. Their political comebacks could reignite the fervour that swept them to power in unprecedented landslides and could complete unfinished business from their three stints in office since 2001.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't explain what would happen if they came back to power.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more detail could have been included", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document talks about the positive impact Thaksin and Yingluck had on the country when they were in power, so their comeback will bring more improvement to people's lives. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article talks about the good things Thaksin and Yingluck did when they were in power, and because of that their comeback will most likely bring further improvement to the country.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is relevant but misses some information that could help further answer the question", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "Thaksin and Yingluck's political comebacks would likely have a significant impact on Thailand. They have a long history of electoral success and their policies have been popular with the working class. If they were to return to power, they would likely continue to implement policies that benefit the working class, such as minimum wage increases, utilities subsidies, and infrastructure projects. This could potentially lead to increased economic growth and improved living standards for many in the country. However, their return to power could also be met", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is valid, but cut off", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article does not explain what would happen if Thaksin and Yingluck came back to power.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is good and generally accurate, though the final sentence is cut off and incomplete therefore not 100% relevant", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer seems accurate and good overall, but the final sentence is cut off and incomplete, therefore not relevant.", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "8": {"question_id": "2", "question": "What unfinished business does Pheu Thai have?", "system": {"davinci": {"answer": " The unfinished business is the unfinished business of the Thaksin regime.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The unfinished business can be deduced from sentence 8.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "the response is relevant but misses some relevant info that might be used to answer the question", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The unfinished business from both the Taksin and Yingluck regimes. They were able to fix many things but then were ousted. Paetongtarn wants to remedy that with all of the same promises made by her family.", "ebr_raw": "10", "ebr_num": 10, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is supported by 8, but is missing a lot of information from 8 as well. Minimum wage increases, utilities subsidies, rail systems, and flood and drought infrastructure are all being promised.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "6"}}}, "gpt4": {"answer": "Pheu Thai is promising to address unfinished business from their three previous times in office since 2001, by offering minimum wage increases, utilities subsidies, and infrastructure enhancements such as long-promised high-speed rail systems and flood and drought management.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and supported by the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The nature of the unfinished business is not stated, but this is likely referencing the fact that the previous candidates from Pheu Thai to be in office had terms that were cut short.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "It's a valid answer, but it could have mentioned why the previous terms were cut short", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct in that there is information in sentence 8 which lists the business that Paetongtarn is looking to finish: minimum wage increases, utilities subsidies, high speed rail systems, and infrastructure to manage floods/droughts. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article talks about the coup which took the previous leaders out of power when they were yet to complete their intending projects", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is only partially correct, and misses some relevant information", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}}}, "human_2": {"answer": "Pheu Thai's goal is to bring a stable political state to Thailand. Being ousted by the coup meant that policies that had been enacted were immediately reversed by the conservative government that took over. Pheu Thai would like to see the policies reinstated and carried out so citizens can truly reap the benefits. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed from the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "A lot of relevant detail was missed in this answer", "ebr_raw": "Score: 35", "ebr_num": 35, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is pretty good but may miss some details about the specifics of the unfinished business", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the unfinished business as per sentences 8, 9 and 10.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "Pheu Thai is promising minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts. They are also aiming to address long-standing problems such as cronyism to enrich business friends and of buying off the poor with wasteful populist policies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partly good and accurate but misses some relevant info from the document.", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4, 5 and 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": " Pheu Thai has unfinished business from three stints in office since 2001, which includes minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer is not relevant as it is merely repeating what was said in document sentence 4 which the asker would already know. They are asking about what the unfinished business is. That is appropriately given in the lines of the answer which follow however and draws from document sentence 8", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 8 matches the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "9": {"question_id": "3", "question": "How are people reacting to the potential of Thaksin's daughter winning the election?", "system": {"davinci": {"answer": " The answer is in the following article.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This respondent isn't even trying to answer the questions", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 10 states that Thaksin's daughter is far ahead in the opinion polls, meaning she has many supporters ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Paetongtarn Shinawatra is from a very popular family. Her father and aunt won elections by landslides and she's becoming the candidate to beat in the current election. This means she is very popular with the people of Thailand.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 11 in the document, Pheu Thai is expected to win most votes.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not provide the answer despite setting up to do so. The info needed to answer the question comes from 10, which notes that despite not yet being named as a candidate, Paetongtarn is far ahead in opinion polls.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not give a direct indication of public opinion on Paetongtarn Shinawatra, the daughter of Thaksin Shinawatra, and her prospects of winning the elections. However, it does mention that while she hasn't yet been named as Pheu Thai's prime ministerial candidate, Paetongtarn is far ahead in the opinion polls, with twice the support of existing Prime Minister Prayuth Chan-ocha.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence of the answer is not relevant because it is contradicted by the the second sentence which goes on to provide an example of how people are reacting and thereby answers the question. The second sentence is supported by 8 (identifies the current PM) and 10 (which talks about the poll).", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and correct based on the article information.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_1": {"answer": "The broad opinions of the public are not made clear. However, the article mentions the military\u2019s influence in the Senate and implies that Paetongtarn would not have the support of the military. It is also mentioned that the Shinawatra family has opponents who accuse them of cronyism and wasteful populist politics.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "In addition to the machine response, there are also people who are voting for Paetongtarn as it is said in the article that she is far ahead in the opinion polls.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "A valid attempt is made at answering this question, and relevant information is retrieved from the article.", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable and basically correct based on the article", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence of the answer contradicts with what is contained in sentence 10, which notes that Paetongtarn is far ahead in the opinion polls, by double. This is a good indicator of opinion.The second answer sentence is supported by sentence 11 and the third is supported by sentence 18. Info from 17 should be inlcuded too which notes that the Shinawatras are also loathed in Thailand as much as they are loved.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}}}, "human_2": {"answer": "Based on information about previous elections and citizen reactions to Thaksin, there is a mixed response to Paetongtarn's campaign. She and her family are just as loved as they are loathed. Some hope her leadership could lead to the economic growth ushered in by her father and aunt, while others believe it will lead to continued political instability and the potential for more coups. Though not explicitly stated, it is likely the conservative party opposes her appointment to office and would want to prevent the family from regaining power again. ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is relevant and helpful, and likely accurate but contains some speculation, so it is not derived solely from info in the article. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response summarizes major parts of the article. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " People are showing strong support for Paetongtarn in opinion polls, with twice the support of Prime Minister Prayuth Chan-ocha. She is campaigning hard in the vote-rich rural strongholds of the Shinawatra family\u2019s Pheu Thai political juggernaut, hoping to reignite the kind of fervour that swept father Thaksin and aunt Yingluck to power in unprecedented landslides.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "the article also explains where the Shinawatras are loathed and accused of cronyism.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "People are giving her their full support as she is even leading in the opinion polls", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article says she is leading in the opinion polls, so this shows that the people are strongly behind her ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Overall the response seems relevant, and all the info is from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "People are reacting positively to the potential of Thaksin's daughter, Paetongtarn, winning the election. She is campaigning hard in the vote-rich rural strongholds of the Shinawatra family's Pheu Thai political juggernaut, hoping to reignite the kind of fervour that swept her father and aunt to power in unprecedented landslides. She is promising Pheu Thai will complete unfinished business from three stints in office since 2001, all of which were cut short by", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is cut off, but otherwise valid", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "the article also explains where the Shinawatras are loathed and accused of cronyism.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is generally good and helpful, but not 100% relevant since the last sentence is cut off and therefore incomplete.", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is mostly good and likely accurate overall, but includes some inference or speculation rather than only information directly from the document", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Since she is far ahead in the opinions polls, it means people are supporting her political ambition and want a change for their community according to the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "10": {"question_id": "3", "question": "Why did Thailand's conservative party orchestrate military coups?", "system": {"davinci": {"answer": "The answer is simple. Thailand's conservative party is not a political party. It is a group of people who are not happy with the current government. They are not happy with the current government because they are not getting enough money from the government. They are not getting enough money from the government because the current government is not giving them enough power. The current government is not giving them enough power because the current government is not giving them enough money.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 18 provides answer to the question. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is speculative and/or unhelpful, though the article doesn't provide a direct answer.", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't actually say why they orchestrated the coups, just that they did. However, it can be inferred that the conservative party didn't like what was done for the citizens. They don't care about them and so they ousted the Prime Ministers.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is incoherent, but the article does not address the reasons behind the coups", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not supported by what is written in the document. The document never specifically addresses the reason for the coups, but sentence 18 notes that opponents have accused them of cronyism and of buying off the poor with wasteful populist policies.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide specific reasons for Thailand's conservative party orchestrating military coups. However, it appears that the military coups were driven by resistance against the Shinawatra family's Pheu Thai political party, which has been accused of cronyism and buying the support of the poor with wasteful populist policies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 corresponds with the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable and supported by the facts in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 17 and 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not clear why the conversative party orchestrated military coups.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in that the exact reason is not stated. Info is given however which speaks in generalities however (sentences 17 and 18) Those sentences note that the Shinawatras are loathed by some and have been accused of crimes and initiating wasteful policies.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 18 states the reason why the military coup was orchestrated. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct based on the article info ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "Though not explicitly stated, it can be inferred from the article that the family is loathed by the conservative party in Thailand, as well as by many citizens. They have been accused of cronyism and nepotism that drains the poor of what little resources they have. Thailand's conservative party likely orchestrated these coups to support the citizens who opposted the family's regime, as well as to regain control of policymaking. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is an excellent, and apparently valid, answer, but it is not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is relevant and likely correct but contains some speculation", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "'nepotism ' and the last sentence in the response are unsupported by the document, which makes the response incorrect and unsupported.", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The conservative party was opposed to the Shinawatra family's populist policies and accused them of cronyism and buying off the poor with wasteful policies. The Shinawatras deny the charges.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 17-20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and relevant and sourced from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The conservative party in Thailand orchestrated military coups in order to prevent the Shinawatra family from continuing to hold power and enacting their populist policies. The coups were seen as a way to maintain the status quo and prevent the Shinawatra family from gaining further influence.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 21 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not supported by any sentences within the document. Sentence 18 does provide a potential reason, but that is not given in the answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 summarizes the machine response. ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "7"}}}}}, "11": {"question_id": "3", "question": "Why must politics be stable to change lives?", "system": {"davinci": {"answer": "Politics is the process of making decisions.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The \"why\" question is barely hinted at in the article, but even a partial answer is better than this worthless response", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the economy was growing and things were better for the people, Shinawatra's father and aunt were removed from power by a coup. With a stable government, coups would be unlikely and the people would be allowed to flourish in a rich economy.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, there has to be cooperation between everyone in power in order for policies to be carried out and effective. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.The response missed the information stated in sentences 8,21,22,23,24,25,26,27.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer gives a definition of politics but does not address the question. As such, it is not relevant. The document never specifically states why politics must be stable to change lives.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Politics must be stable in order to change lives because stability creates an environment that allows for long-term policies with clear goals to be effectively implemented. This, in turn, can foster economic growth, social development, and overall improvement in people's livelihoods. When political stability is lacking, it can result in fits and starts or even reversals of policies, making it difficult to achieve sustainable change for the better.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from some parts of the article based on the incidences that had occurred in the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is a reasonable inference, and a valid answer to the question, but it was not based on the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although the information is not stated in the article, it is reasonable to infer the details in the answer.", "ebr_raw": "90", "ebr_num": 90, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first two answer sentences sound really good, but they are not drawn from the document. The document itself does not elaborate on the topic in the manner done by the answer. The final answer sentence is supported however, drawing from the material in 21-27.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from a summary of what the article portrays. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "human_1": {"answer": "Paetongtarn mentions says that the ouster of her predecessor led to certain things not being achieved. It can be inferred that if the politics were more stable, she believes that it would benefit the citizens of the country.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it failed to mention other factors mentioned in sentences 21,22,23,24,25 and 26.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The first sentence of the answer is supported by sentence 5. The second answer sentence is also supported by sentences 5 and 6.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in stating the info is not given. Paetongtarn makes this statement several times but never explains why politics must be stable. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The article talks about the coup and the overthrow of the past government, it can hence be inferred that politic has to be stable such that everyone in power has to work together to fight for a common goal.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The question mostly asks for outside information, but a partial answer could have been inferred", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Although the economy was growing and things were better for the people, Shinawatra's father and aunt were removed from power by a coup. With a stable government, coups would be unlikely.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": "Stable politics allows for policies to be implemented and sustained over time, which is necessary for people's lives to change in a sustainable manner. Without stability, policies may be implemented but are unlikely to be sustained, resulting in little to no change in people's lives.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 and 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question is not specified within the document. The answer's information sounds good, but it is not drawn from any of the document's sentences.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Stable politics are necessary for sustainable change in people's lives because it allows for long-term policies to be implemented and for the government to have the time and resources to focus on improving the lives of its citizens. Without stability, governments are more likely to be preoccupied with short-term solutions and unable to focus on the long-term needs of its citizens.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not provide the info needed to answer the question fully. It is partly addressed with sentences 21-24, but that alone does not give the whole picture.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer has a lot of reasonable speculation, but it is based on the article text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5,6,9 and 10 is in line with the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info in the answer is not drawn from any of the document's sentences. Sentences 21-24 partially addresses the question in indicating that instability lessens opportunities, but the document does not go beyond that to give a compete answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "12": {"question_id": "4", "question": "How did economic growth from previous leaders affect the country?", "system": {"davinci": {"answer": "The economic growth from previous leaders was not sustainable.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article says the economic growth under Thaksin and Yinkluck were good but they were ousted before it was known just how much.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the good deeds of the previous leaders, it is assumed that economic growth brought a lot of positive changes to the people. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.The document doesn't explicitly say how the economic growth from the previous leaders affected the country, but the sentiment of the people is stated in sentence 2.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is correct in what it is saying, but it is not relevant to answering the question. The document never expressly stated the direct effects of of economic growth experienced by the country.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide details about the precise impact of economic growth overseen by Thaksin and Yingluck. However, their popularity and electoral success suggest that their economic policies and administration improved the livelihoods of many people in Thailand, especially the working class.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The 1st answer sentence correctly identifies that the specific info needed to fully address the question is not given in the document. The second sentence's speculation is supported by what it written in 1, 2, and 6.", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 states that the previous leaders were popular, so this means they did great works during their time which had a positive impact on the economy and the people. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated how economic growth from previous leaders affected the country.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100.", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Economic growth from previous leaders brought about change, the previous leaders were able to fix everything in their first year", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in stating that the info is not explicitly stated. That being said, Sentences 5 and 6 imply that there were improvements in people's lives but they were not sustainable given government changes.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the necessary info is not given in the document. There are no sentences which speak specifically to examples that would show how the country was specifically affected by economic growth.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question would have required outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The past leaders managed to fix everything in their first year, the country was seeing changes", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't state the answer to the question", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The economic growth from previous leaders led to the Shinawatra family's Pheu Thai political juggernaut gaining unprecedented landslides in elections. It also led to the military-backed governments of Prime Minister Prayuth Chan-ocha since he seized power in 2014 offering incremental reforms.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The previous leaders did good work in the country before they were overthrown by a coup. Their past good work is the reason why the people are in great support of the next person coming into power.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer sounds good, but the document never says that is was economic growth that led to both the things listed as happening in the answer. The only mention of economic growth comes in sentence 6, but it is not explained what came of it.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant to the question. It failed to mention the effects of minimum wage and utilities subsidies, as per sentence 8.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The economic growth overseen by Thaksin and Yingluck had a positive effect on the country, as it improved the livelihoods of many people. However, despite this, they were still toppled by the army in 2006 and 2014, respectively. This suggests that the military had other motivations for ousting them, such as political power.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The 2nd and 3rd answer sentences are not relevant as they go beyond the questions scope. The first answer sentence is not supported by the document as it is never stated that lives were actually improved. Sentence 5 makes that claim that things were fixed, but sentence 17 and 18 indicate that they were not helpful, so the answer is not clear. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer does speculate a bit, but otherwise follows the article text", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed in sentence 5 and 6", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first sentence is not supported by any of the sentences in the document, as it is never expressly stated that livelihoods were improved. The final two sentences of the answer are not related to the question as they talk about how/why they were removed from power. The document does not contain the info needed to answer the question. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "13": {"question_id": "4", "question": "What kinds of things does Paetongtarn want to achieve? ", "system": {"davinci": {"answer": " Paetongtarn wants to achieve the following things.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Paetongtarn wants to achieve a stable politics as well as minimum wage increases , utilities subsidies , and long - promised high - speed rail systems and infrastructure to manage floods and droughts.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "the answer is incomplete and not helpful, misses all relevant info.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Paetongtarn wants policies which include minimum wage increases, utilities subsidies, high-speed rail systems and better infrastructure.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer fails to list anything, Sentence 8 contains the needed info. All the things she wants are listed there.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Paetongtarn Shinawatra hopes to achieve minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "What Paetongtarn want to achieve is stated in sentence 8.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is mostly good and is accurate, but misses a small amount of relevant information.", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the things that Paetongtarn wants to achieve.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The article describes Paetongtarn's agenda around offering minimum wage increases, utilities subsidies, and high-speed rail systems and infrastructure to manage floods and droughts.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in making use of sentence 8 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and a good summary of the relevant article information", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}}}, "human_2": {"answer": "Paetongtarn wants to provide minimum wage increases, utilities subsidies, high-speed rail systems and infrastructure to manage floods and droughts, and to descale military power in Thailand (which increased through the conservative coup). Paetongtarn also wants to restore her family's legacy and impact in Thailand. On a personal level, Paetongtarn's campaign is to restore her family's power in the government. However, she is also placing a strong emphasis on continuing the economic expansion her father and aunt started while in office. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and a good summary of the information presented in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially corrrect and missed the information about reducing opportunities for everyone,Thailand going backwards and needing progress, as per sentences 21,22,23,24,25,26,27.", "ebr_raw": "35", "ebr_num": 35, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "Paetongtarn is promising Pheu Thai will complete unfinished business from three stints in office since 2001, all of which were cut short by court rulings and military coups that it says were orchestrated by Thailand\u2019s conservative establishment. She is campaigning hard in the vote-rich rural strongholds of the Shinawatra family\u2019s Pheu Thai political juggernaut, hoping to reignite the kind of fervour that swept father Thaksin and aunt Yingluck to power", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "A lot of relevant information could have been added", "ebr_raw": "30", "ebr_num": 30, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 8 also provides additional information in the things that Paetongtarn wants to achieve", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The above sentences further adds to the machine response as achievements that Paetongtarn wants to achieve. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": " Paetongtarn wants to achieve minimum wage increases, utilities subsidies, and long-promised high-speed rail systems and infrastructure to manage floods and droughts. She also wants to complete unfinished business from three stints in office since 2001, which were cut short by court rulings and military coups that it says were orchestrated by Thailand\u2019s conservative establishment.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The info from sentence 8 used in the first answer sentence is valid and correctly answers the question. The second answer sentence is not relevant it does not state any of the actual things that she wants to achieve. Unfinished business is very vague and is not needed to be stated as the business looking to be finished was already given in the first answer sentence. ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms what Paetongtarn wants to achieve if elected into power", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}}}, "20": {"dataset": "inq-ext", "article_num": 13, "question_details": {"0": {"question_id": "0", "question": "How do citizens at the borders help ensure border safety?", "system": {"davinci": {"answer": "The answer is in the following article.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is nothing in the article that explains how citizens can help with the borders", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, when citizens live at the border, they will be able to help keep the borders safe and secured", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct. The information needed to answer the question is not in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response doesn't answer the question completely. It missed the information in sentences 3,4 and 5.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Citizens living in border areas can help ensure border safety by acting as additional eyes and ears for the authorities. They can report any suspicious activity, movements or illegal entries to the military or security forces. This can help security forces to respond quicker, track down any potential threats and maintain tighter security along the border.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is mostly (reasonable) assumptions, but at least it is based on the text", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention anything that's stated in the answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be implied from the article considering how concerned the government is  about security at the border.", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the urgency at which the government wants people to remain at the border", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not backed up by any of the document's sentences. The document fails to define how people being at the boarder will make is secure thus making the question unanswerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "human_1": {"answer": "It is not stated how citizes at the borders help ensure border safety.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not mention how the citizens will ensure border safety", "ebr_raw": "Score: 0. The response does not mention anything about how citizens will ensure border safety, which was the focus of the feedback.", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. The info needed to answer the question is not stated within the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response missed the answer as stated in sentence 3.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "It is not stated in the document but can be inferred that having people living in the border villages creates a sort of \"border protection\" for the Indian government. As there are people at the borders, it gives the government justification to monitor or have a presence so close to the border. Additionally, the people there are able to see firsthand what is going on and could report any potential threats back to their officials. Without people living there, the area would be much harder to protect. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is reasonable speculation, based on the allusion to border safety in sentence 5", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from parts of the article considering the scenario that is taking place in the article.", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response seems to be speculation and isn't contained in the document", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 3 in the document, Darma Valley is a very sensitive area in terms of national security as it is situated barely 35 km from China border.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " Citizens living at the borders can help ensure border safety by providing information about any suspicious activity or potential threats to the authorities. They can also act as a deterrent to potential threats by their presence.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is reasonable speculation, based on the allusion to border safety in sentence 5", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There's nothing in the article that says what regular citizens can do about helping with the borders.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "There isn't any relevant info missed, but the response contains speculation and/or outside information", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "Citizens living at the borders can help ensure border safety by keeping a watchful eye on any suspicious activity and reporting it to the authorities. They can also help patrol the borders and provide information about any potential threats.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The government says they do not want destructions of the project, hence people will need to live at border villages so that they can help monitoring any suspicious activities to protect the project by ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is mostly reasonable speculation, but it fits with the article text overall", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The AI made some embellishments. There is no mention of why the government wants people to live in the border town other than from statement #5. We can assume that it's because from statement #3 that the area is close to the Chinese border and is a matter of national security.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is nothing in the article that explains how citizens can help with the borders.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "It si expected that citizens of the border will join the project and also keep a watch eye out for any disruption of the project", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "1": {"question_id": "0", "question": "Why are they protesting the hydroelectric project?", "system": {"davinci": {"answer": " The answer is simple. They are protesting because they are not getting any monetary benefits from the project.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer completely ignores the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 2 gives a well detailed response to the question. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The residents are worried about the blasting and tunneling that may cause irreparable damage to the town and property. If completed, seven villages in the valley with have to relocate. It would also adversely impact the environment and also the culture and tradition of the valley which are connected with nature.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.The response missed the information stated in sentences 2,3,4,5,7.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer provides a reason for protest that is not identified in the document. Rather other reasons are given in 2, 4, and 7 where it is noted the people do not want to have their town or property damaged, they don't want to have to move, and they don't want the environment, culture, and traditions damaged.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "They are protesting the hydroelectric project because they live in a seismically sensitive zone of the Himalayas, and they believe the project work, which includes blasting and tunneling, may cause irreparable damage to their town and property, much like what happened at Joshimath. They also have concerns about the project's impact on the environment, culture, and tradition of the valley.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The residents are protesting the hydroelectric project but it will require blasting, which will lead to landslide and subsidence, negatively impacting the environment of the nearby villages. According to a memo, seven villages will have to relocate if the project goes through.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The people in town are afraid the project will adversely impact the environment and also the culture and tradition of the valley which are connected with nature.", "ebr_raw": "70", "ebr_num": 70, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The 1st answer sentence should also include info about what the blasting and tunneling would do, per sentence 2 (may cause irreparable damage to the town and property). The effects are what they are worried about. Info from sentence 7 also needs to be included which lists concerns about culture and tradition. There is also a type in sentence 1 where the word \"but\" should be changed to \"because\" (line 1).", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed other reasons why they are protesting the hydroelectric project, as per sentences 3,5,6,7.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "human_2": {"answer": "The hydroelectric project poses a great risk to those living in Dharchula. The project requires a large blast to create tunnels, but this blast has the potential to cause large landslides and debris. Additionally, the project would have catastrophic effects on the environment, and many environmental organizations are also boycotting it. People living in the town would end up having to evacuate for safety reasons and relocate elsewhere. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2, 4 and 7 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The 1st two answer sentences and 1st part of sentence 3 are supported by info in sentences 2 and 7, but not enough info from those those sentences is included as  the answer fails to mention to mention the potential damage from the landslides as well as the adverse impact to the culture of the area. The 3rd sentence's second portion about environmental organizations is not correct as those are not mentioned in the doc. The final sentences is supported by sentence 4.", "ebr_raw": "40", "ebr_num": 40, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "\"many environmental organizations are also boycotting it. \" in the response is unsupported. The response missed the information in sentences 3,5 and 7 which caused the protest.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " They are apprehensive about the hydro power project work, which includes blasting and tunnelling that may cause \u201cirreparable damage to the town and property\u201d, much like what happened at Joshimath. They are also concerned about the impact on the environment, culture, and tradition of the valley, as well as the potential displacement of seven villages in the valley.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the border safety issue as per sentence 5.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The residents of Dharchula are protesting the proposed Bokang-Baling Hydroelectric Project in the Darma Valley, which will be executed by THDC India Limited, because they are apprehensive about the hydro power project work, which includes blasting and tunnelling that may cause \u201cirreparable damage to the town and property\u201d. They are also concerned that the project will adversely impact the environment, culture, and tradition of the valley, and will lead to the relocation of residents from", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer was cut off, but the relevant information was already included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first 3 lines of the answer are not relevant as they are restating already known information. The next 3 lines are supported by sentence 2 and serve to help answer the question. The 2 lines which follow are supported by sentence 7. The final line is supported by sentence 4.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence of the response is incomplete. As per sentence 4, it will lead to the relocation of residents from seven villages of Darma Valley.Also, the response doesn't mention that the project will require blasting which will lead to landslide and subsidence, as per sentence 7.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 clarifies the reason for the protest", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "2": {"question_id": "1", "question": "How will the tunneling and digging affect the town?", "system": {"davinci": {"answer": " The tunneling and digging will affect the town in the following ways:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The blast will lead to landslides and subsidence. And this will adversely impact the environment but also the culture and tradition of the valley which are connected with nature.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 2 answers the question. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer fails to list any of the ways the town will be effected. Such info comes in 2 and 7, with the first of those indicating that irreparable damage may be caused to the town and property. Sentence 7 says the environment, culture, and tradition of the valley will be adversely affected.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response doesn't answer the question completely. It missed the information in sentences 3,4,5, and 7.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The tunnelling and digging may affect the town by causing blasting which could lead to landslides and subsidence.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer uses sentence 7 as its basis. This info is correct, however info from sentence 2 needs to be included as well as that indicates further affects (irreparable damage).", "ebr_raw": "30", "ebr_num": 30, "workerid": "6"}}}, "human_1": {"answer": "The tunneling and digging most likely require blasting, causing landslide and subsidence, negatively impacting the environment of the nearby villages.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 7 provides additional information on the effect of the project", "ebr_raw": "40", "ebr_num": 40, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The first part of the answer which draws from sentence 7 is correct in listing the outcomes that it did. Information from sentence 2 should added however which states that such would cause irreparable damage to the town and property. The second portion of the answer which notes the environmental impact noted in sentence 7 is also correct, but the information that states the culture would also be affected is missing. ", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it failed to mention the adverse impact to the environment and also the culture and tradition of the valley which are \u201c connected with nature \u201d, as per sentence 7.", "ebr_raw": "65", "ebr_num": 65, "workerid": "1"}}}, "human_2": {"answer": "The tunneling and digging will force the townspeople to relocate. The town would no longer be safe to live in because of the debris and landslides caused by the man-made tunneling and from digging. The town will also be subject to environmental harm; the human intervention will disrupt the natural ecosystem. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is haphazardly written, but it contains the relevant information", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 7 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and a good summary of the relevant article information", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\"The town would no longer be safe to live in\" and \"the human intervention will disrupt the natural ecosystem.\" are just inferred by the response and are not supported by the document. The effects of tunneling and digging on the town are stated in sentence 7.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The blasting required for the tunneling and digging work will lead to landslide and subsidence, which could cause irreparable damage to the town and property.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is relevant, but misses some useful info from the document", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed other damages as per sentences 4,5 and 7.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The tunneling and digging work will cause blasting which will lead to landslide and subsidence, adversely impacting the environment, culture and tradition of the valley which are connected with nature.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 corresponds to the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Found in statement #7.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 corresponds with the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "3": {"question_id": "1", "question": "How would the project cause irreparable damage to the town and property?", "system": {"davinci": {"answer": "The project will cause irreparable damage to the town and property because the project will cause landslide and subsidence.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is an uncharacteristically valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Also, based on the article, people will have to relocate, properties, culture and tradition will be damaged due to the project", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information stated in sentences 3,4,5,7.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by document sentence 7.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "gpt4": {"answer": "The project could cause irreparable damage to the town and property due to the blasting and tunnelling work involved. This may potentially lead to landslides and subsidence in the seismically sensitive area, negatively impacting the environment, culture, and traditions of the valley.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer to a mostly redundant question", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The project will require blasting, which will lead to landslide and subsidence, negatively impacting the environment of the nearby villages.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly draws from sentence 7 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the other damages such as tunnelling and digging work, as mentioned in sentences 2, 6 and 7.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "human_2": {"answer": "The project would destroy parts of the environment because of the man-made tunnels and digging that would occur. The project would also disrupt the ecosystem and the plants and animals living there. For the people who reside there, the landslides and debris could destroy their homes and the infrastructure. The townspeople would have to relocate due to the risks and the effects of the construction. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is some outside information (or speculation) added, but otherwise, this is a sufficient answer", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2, 4 and 7 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The only info correct to include in the answer is that drawn from sentences 4 and 7, which note the landslides and subsidence and people having to move. The other info sounds correct, but none is coming from the document. The document mentions things like environmental impact, but does not state that in terms of hurting animals/plants", "ebr_raw": "Score: 50. The response includes some correct information from the article (sentences 4 and 7 about landslides, subsidence, and relocation of residents), but also includes additional information that is not mentioned in the article (about hurting animals/plants). Overall, the response is partially correct but does not strictly adhere to the feedback's instructions to only include information from certain sentences in the article.", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Both \"man-made tunnels\" and \"disrupt the ecosystem and the plants and animals living there\" in the response are unsupported by the document. The response failed to mention other damages as per sentences 4, 5 and 7.", "ebr_raw": "30", "ebr_num": 30, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The blasting and tunnelling work required for the project could cause landslides and subsidence, which could lead to destruction of the town and property.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed other damages as stated in sentences 4, 5 and 7.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 7 provides additional information on how the project will cause damage", "ebr_raw": "40", "ebr_num": 40, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 7 further adds that the project will destroy the culture and tradition", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The project would cause irreparable damage to the town and property by blasting and tunnelling. Blasting and tunnelling can cause landslides and subsidence, which can lead to destruction of buildings and other infrastructure. This is similar to what happened in Joshimath, where blasting and tunnelling for a hydroelectric project caused landslides and subsidence, leading to destruction of buildings and other infrastructure.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer includes some reasonable speculation, but it fits with the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The 1st answer sentence uses doc. sentence 2. The first part of the 2nd uses sentence 7, but the 2nd part is not mentioned within the document. The final answer sentence mentions Joshimath several times, but it is not relevant to answering the question as it says those things happened does not expand further in explaining how and is therefore just repetative.", "ebr_raw": "Score: 68", "ebr_num": 68, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "\"destruction of buildings and other infrastructure\" and events happened in Joshimath in the response are unsupported. The response failed to mention that the project will not just adversely impact the environment but also the culture and tradition of the valley which are \u201c connected with nature \u201d, as mentioned in sentence 7.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 2,6, 7 and 9 confirms the machine response", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}}}}}, "4": {"question_id": "2", "question": "How will the landslides caused from the project affect safety?", "system": {"davinci": {"answer": " The landslide will not affect the safety of the project. The landslide will affect the safety of the people living in the valley.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Answer ignores the question and the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "While not mentioned in the article, it is reasonable to infer that landslides can destroy homes and properties and kill anyone in the way, as landslides are known to do.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The landslides will cause people to relocate, and this people relocating will mainly be from the border villages. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not explain how the safety will be affected, which is what the question is looking to find out about. The document does not specifically address this, making the question unanswerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and sounds nonsense. It missed the information in sentences 3, 4 and 5.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The landslides caused by the project could affect safety by damaging property, infrastructure, and the environment in the town of Dharchula and the Darma Valley. Furthermore, the landslides could disrupt the culture and tradition of the valley which are connected with nature.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "In addition to the machine response, the project will cause residents of seven villages to relocate, and the borders will be exposed to insecurity.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "People will have to relocate as a result of the project, and the border will become very porous.", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not specifically address safety. It talks about property damage (sentence 2) but safety as its own topic is not discussed. As such the question is not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "human_1": {"answer": "It is not stated how landslides would affect safety, but landslides are generally dangerous events that can cause harm to bystanders and damage to property.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "It will affect safety such that people will need to relocate from their towns", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, and could have been inferred from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the sentence is correct in identifying that the topic of safety is not specifically addressed. Sentence 2 notes \"damage\" but does not directly speak to safety. The rest of the information in the answer is drawn from an outside source.", "ebr_raw": "Score: 35", "ebr_num": 35, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response appears to be correct, but the answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "The townspeople will be forced to evacuate for safety reasons should the project continue. The townspeople, and those working on the project, will be subject to landslides from the tunneling and digging. The project may also affect national/border safety, which poses a threat to the entire country, not only those living in the border towns. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 4 and 7", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is good and an accurate summary of document info", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence in the response is unsupported. The response missed the information about relocation of people which adversely affects border safetly, as per sentences 4 and 5.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "While not mentioned in the article, it is reasonable to infer that landslides can destroy homes and kill anyone in the way, as landslides are known to do.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": "The landslides caused from the project could cause irreparable damage to the town and property, and could potentially put the safety of the town and its residents at risk.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 7 also adds that apart from damage to properties and environment, the project will also destroy their culture and tradition.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is generally good and sourced from the document info", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response missed the border safety casued by the landslides as per sentences 4 and 5.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The landslides caused by the project could potentially cause irreparable damage to the town and property, leading to a decrease in safety for the residents of the town. The landslides could also lead to disruption of the culture and tradition of the valley, which are connected with nature, further decreasing safety for the residents.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms that the machine response is correct", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. All of the information is in statement #7 where the question was derived from.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "the machine response is confirmed in sentence 7.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "5": {"question_id": "2", "question": "What kind of damage would the project cause?", "system": {"davinci": {"answer": " The project will cause irreparable damage to the town and property.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is correct, but insufficient", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The project will also impact the environment, and also destroy the culture and tradition", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The residents are worried about the blasting and tunneling that may cause irreparable damage to the town and property. If completed, seven villages in the valley with have to relocate. It would also adversely impact the environment and also the culture and tradition of the valley which are connected with nature.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information stated in sentences 3,4,5,7.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is supported by document sentence 2. Additional information is given in 7, which notes additional kinds of damage in terms of damage to environment, culture, and tradition of the valley.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "gpt4": {"answer": "The hydro power project, which includes blasting and tunneling, could cause irreparable damage to the town and property, such as landslides and subsidence. It could also adversely impact the environment, culture, and tradition of the valley connected with nature.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Everything in the machine response is included in sentence 7 of the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is again answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The project would cause landslide and subsidence, negatively impacting the environment of the nearby villages.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 7", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 7 is used to provide the info in the answer. It should also be noted that the culture and tradition of the valley would be damaged too as those are types of damage listed in sentence 7.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed the other damages as per sentences 3,4,5,7.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "1"}}}, "human_2": {"answer": "The project would cause both environmental and societal destruction. The town and its people would essentially be dissovled, as all townspeople would need to relocate, and the buildings would be destroyed from landslides and debris. The project would also cause significant environmental damage. The construction would disrupt the ecosystem, including things like plants and animals that are found in that area. It is considered an \"ecologically sensitive\" area. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is essentially a repeat of the previous question, but the judgment is the same", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is completely seen in sentence 2, 4 and 7", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The 1st answer sentence is supported by sentence 7. The second sentence uses 2, 4 and 7. Sentence 3 and uses 7 as well, but fails to include the info in that sentence which speaks to damage to culture and tradition. Sentence 4 is not drawn from the document, and sentence 5 goes beyond the question's scope.", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "\"The construction would disrupt the ecosystem, including things like plants and animals that are found in that area. It is considered an \"ecologically sensitive\" area.\" in the response is unsupported. The response failed to mention other damages such as border safety, culture and tradition of the valley, as per sentences 5 and 7.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The project would cause irreparable damage to the town and property, such as landslides and subsidence, due to the blasting and tunnelling work that would be required.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a redundant question, but it is answered again sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed the other damages as per sentences 4, 5 and 7.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The project will also harm the environment and the culture and tradition of the valley which is connected with nature.", "ebr_raw": "40", "ebr_num": 40, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 7 also informs that the project will also cause damage to the culture and tradition", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "In addition, the project will damage the culture and tradition", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The project would cause irreparable damage to the town and property, such as blasting and tunnelling, which could lead to landslides and subsidence.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The project will also harm the environment and the culture and tradition of the valley which is connected with nature.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer is not relevant as it just repeats what is in sentence 2. The second part includes info from sentence 7, which does note damage in the form of landslides, but it does not note how those will affect the buildings. Sentence 2 notes it will be irreparable damage, but there is no expansion beyond that.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and failed to mention that the project will not just adversely impact the environment but also the culture and tradition of the valley which are \u201c connected with nature \u201d, as per sentence 7.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "In addition to the machine response, the damage would also impact the environment, as well as the culture and tradition.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}}}, "6": {"question_id": "3", "question": "How was the memorandum received?", "system": {"davinci": {"answer": "The memorandum was received by the SDM of Dharchula, Divesh Sasni.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This overly literal response in actually all of the information available in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as it indicates who received the memo. The  question wants to know how it was received, as in how it was perceived. Such information is not available within the document making the question not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The text does not explicitly state how the memorandum was received by the Sub-Divisional Magistrate of Dharchula, Divesh Sasni.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on sentence 2, the memorandum was likely received by hand", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 2 suggests that the memorandum was received by hand. ", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this question would have required outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}, "human_1": {"answer": "The article states that the memo was \"handed over\", but it is unclear exactly how this was delivered by the protestors.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer takes the question too literally, but the article does not provide a sufficient answer to how the government responded to the memo, either.", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 2", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The question is asking about the reaction of those who received the message, not the manner in which is was delivered. The document however does not contain any info that speaks to the reaction of the receiver of the memo making the question unanswerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The passage doesn't say how the memorandum was received, and the response failed to mention that.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 2 states that the memorandum was handed over, meaning it was received by hand", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that this info is not provided.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct and supported.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The Sub-Divisional Magistrate (SDM) of Dharchula, Divesh Sasni, received the memorandum from the protesters. It is not stated in the article how the memorandum was received.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The memorandum was received by hand from the residents of Dharchula town during their protest", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The memorandum was received by the Sub-Divisional Magistrate (SDM) of Dharchula, Divesh Sasni.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It was not stated how it was received within the document. The answer is just noting who the recipient was which is not what the question was inquiring about.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The ending part of sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "7": {"question_id": "3", "question": "How will the project affect the culture and tradition of the valley?", "system": {"davinci": {"answer": "The project will affect the culture and tradition of the valley in the following ways:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Landslides and land that gives way to craters always adversely affect the surrounding nature and the town is connected deeply with nature.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The culture and tradition are connected with nature, so the landslide will have a negative effect.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer fails to list the ways in which things will be affected. Such info is relayed in 4, which notes that people would have to move from the valley. Sentence 7 also notes that culture and tradition are connected with nature, and the environment will be impacted adversely, thus affecting tradition and culture.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response doesn't answer the question completely. It missed the information that the landslide caused by the project will affect the nature, which in turns affects  the culture and tradition of the valley, as per sentence 7.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The project will affect the culture and tradition of the valley by causing potential damage to the environment, which the culture and traditions are \"connected with nature.\" This connection may be disrupted if the hydroelectric project leads to landslides, subsidence, and other environmental harm. Furthermore, due to the project, residents of seven villages in Darma Valley may have to relocate, which would also impact local culture and traditions by displacing communities.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2, 4 and 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The first two answer sentences are supported by doc. sentence 8. The third answer sentence is supported by sentences 4 and 7 which indicate that the people would have to move from the valley, and that they valley is where their culture and tradition are.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}}}, "human_1": {"answer": "It is not stated how the culture and tradition of the valley are affected, but these are stated to be connected with nature, which will be negatively impacted by the project.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 7", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Relevant information in the article was missed", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is incorrect. Sentence 7 specifically notes that the project will adversely impact both the environment. culture, and tradition of the valley.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this question would have required outside information (or speculation)", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 7 in the document, blasting of the project will lead to landslide and subsidence, which affects the culture and tradition of the valley which are \u201c connected with nature\".", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The tradition and culture is connected to nature, so the digging and tunneling will affect the nature", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the question isn't directly addressed in any additional detail", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Landslides and land that gives way to craters always adversely affect the surrounding nature and the town is connected deeply with nature.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": "The project will adversely impact the culture and tradition of the valley as it will require blasting which will lead to landslide and subsidence. This could cause irreparable damage to the town and property, and could lead to the relocation of residents of seven villages of Darma Valley.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Due to relocation because of the project, culture and tradition will be destroyed. Also, culture and tradition is connected with nature, and once nature is destroyed, the culture and tradition will be destroyed as well. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the connectedness with nature as per sentence 7.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The project will adversely impact the culture and tradition of the valley as it will involve blasting and tunnelling, which could cause irreparable damage to the town and property. This could lead to landslides and subsidence, which could disrupt the culture and tradition of the valley which are connected with nature.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 summarizes the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. It could cause damage. All of the information is contained in statement #7.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 7 backs up the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "8": {"question_id": "4", "question": "How long did the protests last? ", "system": {"davinci": {"answer": " The protests lasted for a few hours.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article does not give any indication of the length of the protests (even though it can be inferred that they took place only on a single day)", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not state how long the protests lasted.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't say how long the protests lasted.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The exact duration of the protests is not  given by the document. Sentence 1 limits the activities descried in the document to taking place on Monday, but there is not further information beyond that given.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide information on how long the protests lasted.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asked for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information about how long the protest lasted.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not provide information about how long the protest lasted.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article only says that a protest took place on Monday, so answering this question in any more detail would require outside information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The protest was stated to have happened on Monday. While it is not explicitly stated, it is implied that the protest likely lasted a single day, but political activism is ongoing.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, given the text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not mention how long the protest lasted for", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Most of what is written in the answer supported by sentence 1. The very last portion however (but political activism is ongoing) is supported by sentence 8, which gives such an example of how activism will be ongoing.", "ebr_raw": "85", "ebr_num": 85, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence of the response is correct. But political activism is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "1"}}}, "human_2": {"answer": "The protests seem to be ongoing. Based on the information presented in the article, we can infer that the purpose of the protests is to stop the project. Due to this, it is likely the protests will continue until the project is halted and the people's demands are fulfilled. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, given the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "From the article, the protest is still ongoing", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Sentence 1 notes that the protest in question took place on Monday. The answer is correct however in stating that, overall, the protest movement may be ongoing as sentence 8 notes that the residents will not work in/support the project. The inferences are likely correct, but rather than state when they might end, which is speculation not based on anything in the article, it would be better to reference the info in sentence 8.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "\"until the project is halted\" in the response is unsupported as the project has just been proposed and no resident from Dharchula and Darma Valley will work in the power project, as per sentences 1 and 8. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The length of the protests is not specified in the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question would have required outside information", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response should have also mentioned the protest was staged on Monday, as per sentence 1.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not specify the length of the protest", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state the duration of the protest.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The duration of the protests is not specified in the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article does not answer this question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that the duration was not specified.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response failed to mention that the residents staged the protest on Monday,as per sentence 1, though the duration is not mentioned in the passage. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not talk about the duration of the protest", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}}}, "9": {"question_id": "4", "question": "Why will no resident work on the power project?", "system": {"davinci": {"answer": "The answer is simple. The residents of Dharchula and Darma Valley are not fools. They know that the project will destroy their town and property. They know that the project will destroy their culture and tradition. They know that the project will destroy their environment. They know that the project will destroy their lives. They know that the project will destroy their future. They know that the project will destroy their children's future. They know that the project will destroy their children's lives. They know that", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer is a jumbled mess, but it at least fits with the text", "ebr_raw": "65", "ebr_num": 65, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The residents know their town will be damaged and they don't want to suffer the same fate as Joshimath. They don't want the project there so they will not help if the government pushes it.", "ebr_raw": "40", "ebr_num": 40, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed from the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "7 and 2 support the answer's statements that they know the project will destroy their town and property, culture, tradition, and environment. The rest of the information is not supported by the document's sentences. Info from sentence 9 should be included as well, which notes that they do not want to suffer the same fate as the people of Joshimath.", "ebr_raw": "55", "ebr_num": 55, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The latter part of the response is unsupported and repetitive. The response missed the information in sentences 9 and 10.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}, "gpt4": {"answer": "No resident from Dharchula and Darma Valley will work on the power project because they are opposed to the project, fearing it will cause irreparable damage to their town and property due to the town being in a seismically sensitive zone. Additionally, they are concerned that the project will adversely impact their environment, culture, and traditions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is comprehensive, making using of sentences 7, 9, and 10 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_1": {"answer": "The residents will not work on the power project as a form of protest.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They will not work on the project because they do not want to suffer the same fate as people f Josphimath did", "ebr_raw": "55", "ebr_num": 55, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "This is an accurate answer, but it is much too brief, glossing over important information", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "While not explicitly stated within the document the reason for the mentioned moratorium, sentences 8-11 support the answer. With 8 mentioning they do not want to support it in any way, 9 saying they don't want to suffer as others did, 10 noting they don't want the project, and 11 noting a protest.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "It is not stated in the document but can be inferred that no resident will work on the project as a form of protest against the project. Protesters have been gathering to try and stop the project from happening. The townspeople refuse to accept \"destruction for the sake of development\". The residents want to stop the project more than anyone, which is why they have banded together to protest and refuse to help. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2, 10 and 11 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is mostly correct but contains some speculation or inference rather than only relying on information directly in the article", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "Residents are apprehensive about the hydro power project work, which includes blasting and tunnelling that may cause \u201cirreparable damage to the town and property\u201d, much like what happened at Joshimath. They are also concerned about the impact on the environment, culture, and tradition of the valley.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the fact that the residents are apprehensive and frightened, as per sentence 7. ", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears to be correct and accurate and is taken from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "Residents of Dharchula and Darma Valley have decided not to work on the power project or support it in any way due to their apprehension that the project will cause irreparable damage to the town and property, much like what happened at Joshimath. They are also concerned that the project will adversely impact the environment, culture, and tradition of the valley which are connected with nature.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7-10 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information is in statement #2, #7 and #9.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in sentence 2, 7-10.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "10": {"question_id": "5", "question": "How does the project affect national security?", "system": {"davinci": {"answer": "The project is located in the Darma Valley, which is a very sensitive area in terms of national security as it is situated barely 35 km from China border.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "A lot of vital information was left out of this answer (though at least a good faith effort was actually made this time)", "ebr_raw": "Score: 35", "ebr_num": 35, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "People's relocation due to the project will affect the security at the border. ", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information in sentences 4 and 5.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer merely repeats sentence 3 without providing any additional information, which is not useful to answer the question. Sentences 4 and 5 provide the info needed in that they state that the Central government says they want people to live there to ensure safety of the boarder, but the proposed power project will see those people relocated.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The hydro power project may affect national security because it might lead to the relocation of residents of seven villages in Darma Valley, which is situated barely 35 km from the China border. The Indian Central government wants people to live at border villages to ensure the safety of borders, so such a project might have the opposite effect of what they want.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The project would require seven villages to relocate. The government has stated that people living at border villages can help ensure the safety of the borders, though exactly how they do this is not clear.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 and 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly references info from sentences 4 and 5 and identifies that the exact way in which all that works is not identified.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response missed the geographical sensitivity of the area, as per sentence 3.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}, "human_2": {"answer": "It is not stated but can be inferred that the project would create a vulnerabliity at the India-China border. The construction has the potential to destroy existing natural barriers that serve as a border, as well as keep the people in these border areas safe. Additionally, there may be increased monitoring from China at the border due to the construction, which has the potential to affect natural security. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer is based on the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 says that people will relocate and no one will keep watch over the border, and national security will therefore be affected.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Info from sentences 4-5 speak of the India China boarder. It is indicated that the issue lies with the people leaving the area. That would be an issue as without them, there would be no one to ensure the boarder was safe. The info included in the answer as it currently stands is not backed up by what is written in the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response seems good but is not stated in the document anywhere. This makes the response incorrect and unsupported. The response failed to mention the effect of the project on national security, as per sentences 3, 4 and 5.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The project could lead to relocation of residents of seven villages of Darma Valley, which could weaken national security as the Central government wants people to live at border villages to ensure the safety of borders. The project could also lead to blasting and digging work, which could cause landslides and subsidence, further weakening national security.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "'further weakening national security.' in the response is unsupported.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3-5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3-5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The project could potentially cause irreparable damage to the town and property, which could compromise national security as it is situated close to the China border. This could lead to landslides and subsidence, which could put the safety of the border at risk.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the limited information in the text", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not state that the safety could be at risk from the landslides, rather sentences 3-5 note that it could be affected by people leaving the area which would go against the goverment wish of having people living in the area to ensure safety.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incorrect and the correct reasons are stated in sentences 4 and 5.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "11": {"question_id": "5", "question": "What was the fate of the residents from Joshimath?", "system": {"davinci": {"answer": "The fate of the residents of Joshimath is not known.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "An answer can be inferred from the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The blasting and tunnelling caused irreparable damage to the town and property in Joshimath.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The fate of the residents of Joshimath is the same as what will happen to the people of Dharchula town. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct. Sentence 2 of the document states that there was irreparable damage to the town and property there.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document, but the fate of the residents from Joshimath is stated in sentence 2 in the document.", "ebr_raw": "10", "ebr_num": 10, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific information on the fate of the residents from Joshimath. However, it mentions that residents of Dharchula town, who are protesting against the Bokang-Baling Hydroelectric Project in the Darma Valley, are worried about blasting and tunneling operations that may cause \"irreparable damage to the town and property,\" which is what happened at Joshimath.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "It can be reasonably inferred from the article that damage occurred at Joshimath", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the article. ", "ebr_raw": "75", "ebr_num": 75, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The adverse effect of the project likely happen at at Joshimath since the article made reference to it. ", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly states that the specific info is not given and also provides the info that is available from sentence 2.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}}}, "human_1": {"answer": "It is not clear what happened to Joshimath, although it can be inferred that a similar government project may have negatively impacted their community.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article mentions that the people of Dharchuka made reference to the people of Joshimath, meaning a similar project occurred in Joshimath as well", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The article makes it clear that tunnelling and blasting caused irreparable damage in Joshmiath", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that the specifics were not stated but sentence 2 does indeed imply a similar project caused damage.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response didn't mention the blasting and tunnelling that caused irreparable damage to the town and property at Joshimath, as per sentence 2. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "It can be inferred from the article that the residents of Joshimath were subjected to \"irreparable damage to the town and property\"", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The people of Joshimath most likely suffered destruction of their homes, tradition and culture due to the blasting and tunneling", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete. However, it can be inferred that the people of Joshimath had to evacuate their homes.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 2 in the document,  blasting and tunnelling caused \u201c irreparable damage to the town and property \u201d  at Joshimath.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct, it isn't specifically stated in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " The residents of Joshimath suffered irreparable damage to their town and property due to the hydro power project work, which included blasting and tunnelling.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The residents from Joshimath suffered damage to their property and town according to sentence 2.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is clear and accurate, and uses document info to answer the question", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The residents of Joshimath suffered irreparable damage to their town and property due to the hydro power project work, which included blasting and tunnelling.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 talks about the damage that will be caused by the project, and this was the same damage that the people of Joshimath faced", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient, if concise, answer", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information is found in #2.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The same project that is to be done in Dharchula town was done in Joshimath, and they experienced damage to their environment and property ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}}}, "21": {"dataset": "inq-ext", "article_num": 14, "question_details": {"0": {"question_id": "0", "question": "How did the NDRF help in Turkey?", "system": {"davinci": {"answer": "The NDRF team was deployed in Turkey to help in the rescue and relief operations after the 6.7 magnitude earthquake that struck the country on February 6. The team was deployed in the city of Ercis, which was the epicentre of the earthquake. The team was deployed in the city of Ercis, which was the epicentre of the earthquake. The team was deployed in the city of Ercis, which was the epicentre of the earthquake. The team", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The NDRF was deployed to go on a rescue mission after the earthquake that happened in Turkey, the article did not mention the city of Erci.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Most of this answer was taken from outside sources, but it at least fits with the article text", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The NDRF sent search and rescue personel to look for victims. They found at least 85 bodies. They also sent  dogs who assisted in finding two girl who were still alive.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The document doesn't mention the magnitude of the quake or city name, so the response is unsupported. Also, the response is repetitive at the end.", "ebr_raw": "20", "ebr_num": 20, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first answer sentence is not relevant as that info is already contained in doc. sentence 1, which the person asking the question already knows. The 2nd answer sentence and those which follow are not supported by any of the document's sentences. Info needed to answer the question comes in 18 which notes they rescued 2 children and recovered at least 85 bodies.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The NDRF (National Disaster Response Force) team from India helped in search and rescue operations in Turkey after the February 6 earthquake. They rescued two children who were trapped for over 84 hours and assisted local authorities in finding at least 85 bodies. Their rescue dogs also played a significant role in finding two girls, aged 6 and 8, who were alive.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 and 20 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response answers the question well and gives all relevant info needed from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "NDRF sent 151 personnel to Turkey to help with the recovery effort in the aftermath of the Febuary 6 earthquake. The article specifically describes how the force rescued two trapped children and helped local authorities find at least 85 bodies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The first answer sentence is supported by document sentences 1 and 3. The second answer sentence is supported by sentence 18. ", "ebr_raw": "Score: \nAnswer sentence 1: 100 (supported by sentences 1 and 3)\nAnswer sentence 2: 100 (supported by sentence 18)", "ebr_num": null, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is captured in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and a good summary of the relevant info in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "The NDRF is a response force that was dispatched to Turkey after the deadly earthquakes. The team helped rescuers in Turkey find bodies buried in the rubble, as well as try to find any possible survivors. The NDRF's goal is to put \"humanity first\" and aid in humanitarian crises, such as the earthquakes. The NDRF was also able to bring equipment that was used to clean up the destruction while looking for people. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The article didn't say they cleaned up the rubble while using their equipment.", "ebr_raw": "40", "ebr_num": 40, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is good and relevant but misses some useful information in the document", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The 1st sentence of the answer contains info that is not relevant. It is not needed to identify who the NDRF is as that was done in sentence one, which the asker already has. The info from sentence 1 of the doc that is in answer sentence 1 is important however  as it speaks to their assistance. The second sentence is supported by sentence 18 of the doc. The 3rd sentence is beyond the scope of the question. The 4th answer sentence is drawn from info outside the document.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " The NDRF rescued two children, trapped for over 84 hours, after the earthquake and helped local authorities find at least 85 bodies. They also assisted personnel in finding two girls \u2013 6 and 8 years old \u2013 alive.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer given is taken directly from the text, but it seems to imply that four children in total were rescued, while the text is probably just referring to the same two girls in two different sentences ", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18-20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 and 20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The NDRF sent three teams to Turkey under India\u2019s Operation Dost. The force rescued two children, trapped for over 84 hours, after the earthquake and helped local authorities find at least 85 bodies. The NDRF's rescue dogs also assisted personnel in finding two girls \u2013 6 and 8 years old \u2013 alive.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4, 17 and 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is good and complete, relevant and uses info from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "1": {"question_id": "0", "question": "Why does the NDRF go abroad?", "system": {"davinci": {"answer": " The NDRF is a specialised force of the Ministry of Home Affairs (MHA) of the Government of India. It is a multi-disciplinary force with expertise in disaster management, rescue, relief and rehabilitation. The NDRF is a part of the National Disaster Management Authority (NDMA) and is under the administrative control of the MHA. The NDRF is a force of about 10,000 personnel and is equipped with resources and equipment to respond to a wide range of natural", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer may be factually accurate, but it was not taken from the article text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the mission of NDRF described in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer given is not from the article. The NDRF offers assistance in disasters including cyclones, floods, earthquakes. They also work to provide medicines and vaccines to other countries. They send personnel and whatever equipment is needed for the particular disaster.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "the response may be correct, but uses outside information, and misses relevant information in the article.", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but  is not stated in the document anywhere. This makes the response incorrect and unsupported. It missed the reason stated in sentence 1.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The National Disaster Response Force (NDRF) goes abroad to assist with search and rescue operations in countries affected by natural disasters like earthquakes. By doing so, they provide much-needed help and support to local authorities and showcase India's commitment to helping other nations in times of crisis.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\"National Disaster Response Force \" and India's commitment in the response are not mentioned in the document so they are unsupported .", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "It can be deduced from the article that the NDRF goes abroad on rescue mission after the occurrence of natural disasters. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "It is not explicitly stated, but based on Modi's remarks about providing selfless service to other nations, it can be inferred that they are sent abroad for humanitarian reasons and to give a positive impression of India.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response may be mostly correct but contains speculation and may miss some relevant info in the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a very reasonable supposition based upon information given in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response didn't mention the reason was for the search and rescue operations in the aftermath of the February 6 earthquake.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on the article, the NDRF is sent abroad on a rescue mission after the occurrence of natural disasters.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_2": {"answer": "Though not explicitly stated, it can be inferred based on the article that the NDRF also goes abroad to be able to serve humanity on a wider scale. The team's mission is to put \"humanity first\", and Prime Minister Modi has stated that it is the country's mission to create the best response force in the world. They have already assisted in relief missions both in their home country and abroad in places like Turkey, Ukraine, Afghanistan, and Nepal. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response corresponds to the given scenario in the article", "ebr_raw": "85", "ebr_num": 85, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "'it can be inferred based on the article that the NDRF also goes abroad to be able to serve humanity on a wider scale.' in the response is just a inferral and not supported by the document.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is good and relevant overall, but contains some speculation or inference that is not directly in the article info.", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " The NDRF goes abroad to provide assistance in search and rescue operations in the aftermath of natural disasters such as earthquakes.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is irrelevant and not specific enough. The correct answer is stated in sentence 1.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article says the NDRF goes abroad so as to offer search and rescue mission after a natural disaster occurrence. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer uses sentences 1 and 31 to craft its response. Info from sentence 22-23 should be included as well as it indicates such efforts are part of India's commitment to to help other countries in need, which get to the core reason of why they are going abroad.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "The NDRF goes abroad to provide assistance in search and rescue operations in the aftermath of natural disasters such as earthquakes, floods, and cyclones.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document says  that the NDRF is a rescue mission, so they go abroad to rescue people from disasters", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is good, clear concise and based on the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document says the NDRG goes abroad so as to provide search and rescue from natural disasters. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "2": {"question_id": "1", "question": "How many people did the NDRF help through the operations?", "system": {"davinci": {"answer": "The NDRF rescued two children, trapped for over 84 hours, after the earthquake and helped local authorities find at least 85 bodies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by 18.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "gpt4": {"answer": "The NDRF helped rescue two children trapped for over 84 hours after the earthquake and assisted local authorities in finding at least 85 bodies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and based on information provided in the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The article describes how the force rescued two trapped children and helped local authorities find at least 85 bodies. It is unclear if they directly helped others.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly makes use of sentence 18 to answer the question and correctly notes that it does not have access to compete information. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 and 20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears to be correct and accurate and is taken from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "The team was able to rescue two children, who were still alive and buried under the earthquake rubble. The team was also able to find 85 bodies that were buried. The NDRF's rescue dogs were also able to help people, finding two young girls who were still alive. The organization also helped to clear the rubble, which helped countless people start the process of rebuilding their lives and looking for their loved ones after the earthquakes. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response overall is very good and relevant but the last line contains some inference or speculation", "ebr_raw": "95", "ebr_num": 95, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Sentence 1 is supported by sentence 18. Sentence 2 is also supported, but it is not clear how many people this helped, as the family members of the deceased were not discussed. The 3rd sentence is repetitive and therefore not relevant as it refers to the people already discussed in sentence 1. The sentence is not supported by any of the documents sentences.. ", "ebr_raw": "57", "ebr_num": 57, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 and 20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_few_shot": {"answer": " The NDRF personnel helped local authorities find at least 85 bodies and rescued two children, trapped for over 84 hours, after the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but could have included a little bit of additional detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 and 20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The NDRF helped to rescue two children, trapped for over 84 hours, after the earthquake and helped local authorities find at least 85 bodies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is pretty good but may miss one line that helps answer the question, though it is not clear if the two girls rescued were the same ones previously referenced", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 18-20 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "3": {"question_id": "1", "question": "Why is it so difficult to remove debris and rescue people from earthquakes?", "system": {"davinci": {"answer": "The answer is given in the following link.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article does not address the \"why\" question, though this does not excuse these worthless answers", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no specific reason given in the article, but it could be due to the extent of the disaster when buildings must have collapsed. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article states that it is difficult but does not go into detail why it is so. It can be inferred that the rubble from an earthquake would be heavy and each piece would have to be removed to search beneath them. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not helpful or relevant, the article does not specify an answer directly.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported, though it is an incomplete sentence.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Removing debris and rescuing people from earthquakes is difficult due to several reasons. The sheer volume and weight of debris can make it challenging to access trapped individuals. Collapsed structures can be unstable and dangerous, posing risks to both trapped victims and rescuers. Additionally, aftershocks can worsen the situation by causing further damage and making it unsafe for rescue efforts. Rescuers need specialized equipment, training, and coordination to safely and effectively conduct search and rescue operations under these challenging conditions.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This appears to be a very comprehensive and accurate answer, but it was not taken from the text.", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response looks right but this is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The extent of the damage caused by the earthquake in the article can be used to match up the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention anything detailed in the answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This appears to be an excellent, and factually accurate, answer, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}, "human_1": {"answer": "It is not stated why these are difficult.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct, it is not directly stated", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no explicit answer given in the article, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not categorically state why there is difficulty in removing debris and rescuing people from earthquakes.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asked for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Response is correct and misses on explanation in the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although information is not stated but it can be deduced from the article that it is difficult because of the extent of the earthquake ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}}}, "text-davinci-003_few_shot": {"answer": " Removing debris and rescuing people from earthquakes is difficult because of the unstable and dangerous environment created by the earthquake. The debris can be heavy and difficult to move, and the environment can be hazardous due to the risk of further collapses or aftershocks. Additionally, the rescue personnel must be careful to not put themselves in danger while trying to help others.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is mostly reasonable speculation, but it fits with the article text overall", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention anything detailed in the answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although the article did not provide detailed information as regards the question but I believe the intensity of the earthquake makes it difficult for debris to be removed. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question is not included in the document. It is never actually stated why it is difficult. The answer sounds good, but it is not drawn from sentences within the document.", "ebr_raw": "Score: 0.", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "Removing debris and rescuing people from earthquakes is difficult because of the unstable and hazardous environment created by the earthquake. The debris can be heavy and difficult to move, and the area can be dangerous due to the risk of further collapses or landslides. Additionally, the area may be difficult to access due to blocked roads or other obstacles. The rescue personnel must also be careful to avoid further injury or death while attempting to rescue people.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "I believe the intensity of a disaster will make it difficult to remove debris or rescue people, as buildings must have fallen on people", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention any of the facts in the answer.", "ebr_raw": "0. The response did not include any information from the article to answer the question.", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "All of the information in the otherwise excellent answer appears to be valid, but it is not taken from the article", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer makes sense and is likely correct but uses outside information to answer the question", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Though not detailed in the document, I believe the machine response is correct because successfully carrying out a rescue mission is difficult as a result of the gravity of the damage that is caused by the disaster", "ebr_raw": "95", "ebr_num": 95, "workerid": "7"}}}}}, "4": {"question_id": "2", "question": "How can the force become the best response force in the world?", "system": {"davinci": {"answer": " The force can become the best response force in the world by following the following steps.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the force can become the best force in the world by documenting the learnings they had in Turkey", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This question was not addressed in the article, though the answer made no attempt to even answer it", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They can become the best by continuing to help other, learning from other countries, and documenting everything they have learned. Experience and learning from others.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response doesn't answer the question. It missed the information in sentences 3,11,14,15,24,26.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not list any of the ways the force could became the best in the world and it is therefore not relevant. Relevant info is contained in the document however. Sentence 11 notes that they should document their learnings, and 14 notes that they should work on preparing for future challenges.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide specific steps for the NDRF to become the best response force in the world. However, some general suggestions could include improving training, investing in advanced equipment and technology, strengthening collaboration with other international rescue forces, and continually learning from experiences in different disaster situations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as could have been taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 11-16", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response may be correct but contains speculation and/or outside information not directly supported by the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from Modi's comments in the article. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated how the force can become the best response force in the world, but it can be inferred that continuing to participate in rescue efforts in other countries can bolster their reputation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that the information is not given. That being said, sentences 32-36 back up the inference referenced in the answer. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the limited information in the text", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article says the force has to continue its good deeds of rescuing people from disasters. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable and likely correct, and does not miss any information", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and correct", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that the info needed is not provided. The steps that would need to be taken to achieve the stated goal are not presented. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "They should document everything they have learnt and use the experience to continue to help others in future situations.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}}}, "text-davinci-003_few_shot": {"answer": " Modi suggested that the NDRF should document the learnings that its personnel had in Turkey, such as the work by other countries or the equipment they used during the operation, and use this information to improve their own operations. He also praised the work of the NDRF\u2019s rescue dogs and suggested that the force should continue to provide selfless service to other nations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed other aspects that the force can become the best response force in the world, as per sentences 14,15 and 28.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5-24 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11-28 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The NDRF can become the best response force in the world by continuing to provide selfless service to other nations, documenting the learnings from their operations, and utilizing the latest technology and equipment to ensure the most efficient and effective response to disasters. Additionally, the NDRF should continue to train and develop their personnel to ensure they are well-prepared for any situation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The whole document talks about the force documenting how they were able to help as this will serve as a proof of their good works. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response doesn't miss anything from the document, but is mostly speculation and/or outside information not taken directly from the document", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11-16 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "5": {"question_id": "2", "question": "How did the people respond to the NDRF's rescue mission?", "system": {"davinci": {"answer": "The people of Turkey were very happy with the NDRF's rescue mission.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response of the Turkish people is not addressed in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into detail about the feelings of the people but it is safe to assume they would be very grateful for the assistance.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "the answer is reasonable based on the article, may miss one line of relevant information", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.The response missed the information of how the people in Turkey and India reacted, as stated in sentences 21 and 25.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details on how the public responded to the NDRF's rescue mission; however, it does mention that Prime Minister Narendra Modi expressed his pride in the NDRF personnel and their efforts. He said, \"The whole nation is proud of you,\" to the personnel, indicating a positive response from the people in the country.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9-21 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "All available and relevant information from the text was included in this answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "It is not stated how people specifically responded to the rescue mission.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct, as it is not directly stated.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asked for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "According to Modi, the whole nation is proud of the NDRF", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is the people felt proud about the NDRF's rescue mission, as per sentences 21 and 26.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "At least a partial answer could have been taken from the article", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The people were proud of the NDRF's mission", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 20,21 and 22 in the document, the people felt proud, shared their stories and trusted NDRF personnel on the ground.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The people responded with admiration and gratitude. The Prime Minister praised the work of the unit and the whole nation was proud of them. The Prime Minister also shared his experience of working as a volunteer during the 2001 Gujarat earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence in the response is unsupported.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8-27 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence of the answer is not supported by any of the document's sentences. The second sentence is correct and is drawn from sentences 9-10. The final answer sentence is not relevant to the question given what is being asked as it is talking about what else happened after the event versus a response from the people.", "ebr_raw": "Score: 67", "ebr_num": 67, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "The people responded to the NDRF's rescue mission with pride and admiration. Prime Minister Narendra Modi praised the work of the unit and expressed his appreciation for the courage and professionalism of the personnel. He also shared his experience of working as a volunteer during the 2001 Gujarat earthquake and said that the whole nation was proud of the NDRF personnel.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-16 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response answers the question well and gives all relevant info needed from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5-10 corresponds to the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "6": {"question_id": "3", "question": "What kind of learnings did the personnel have in Turkey?", "system": {"davinci": {"answer": "The reader asked a very good question. I was also wondering the same thing. So I decided to find out.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The question is barely addressed in the article, and completely ignored by the answer writer", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They learned about the challenges that can be faced when on a rescue mission. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "To learn from other countries about their work or the equipment they used to help with the disaster. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Answer is useless. Document does not provide a specific answer.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response doesn't answer the question directly. It missed the information stated in sentences 12 and 14.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details about the learnings the NDRF personnel had in Turkey, but it mentions that they had experiences working with other countries, learned about the equipment they used, and saw the challenges they should be prepared for.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12-14 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "All available and relevant information from the text was included in this answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "It is not stated what learnings the personnel had in Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article does not state specific learnings", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information in the article that could have been used to improve upon this answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response failed to mention they learned the work by other countries or the equipment they used, as per sentence 12.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They learnt about the challenges that is involved in the rescue job and how best to work on it. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "human_2": {"answer": "The personnel learned about how they could be a better relief team, as well as being able to see firsthand the impact of what they were doing. The Prime Minister acknowledged that each team member would have their own experiences and personal learnings to share. The team was also able to collaborate with teams from other countries and on the ground in Turkey. Through this, they were exposed to different methods of doing things and we able to observe how/what different equipment other countries use for rescue and relief missions. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the challenges the team has learned, as per sentence 14.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is complete and helpful, and likely accurate, but contains some speculation or inference which is not directly taken from the article info.", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11-19 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_few_shot": {"answer": "The personnel learnt about the challenges they should be prepared for and how to work on them, as well as the equipment used by other countries during rescue operations. They also learnt about the work of rescue dogs and how to use them in rescue operations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer is reasonable, given the article text, though it is difficult to judge if these specific things were learned during the mission", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence in the response is unsupported.", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 14 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer draws on sentences 12 and 14 to create the first answer sentence. With that said, the answer misses the portion of sentence 12 which indicates they learned about the work of other countries. The final answer sentence is not supported by the document as sentence 20 indicates that it was the NDRF's dogs that were involved. They would have already known about them and how to use them.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}}}, "text-davinci-003_zero_shot": {"answer": "The personnel of the NDRF had learnt about the challenges they should be prepared for and how to work on them, as well as the equipment used by other countries during rescue operations in Turkey. They also had stories to share about the work of the NDRF's rescue dogs, Romeo, Julie, Bob, Roxy, Rambo and Honey, who assisted personnel in finding two girls alive.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13-20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer doesn't miss anything but the last sentence about the dogs isn't really relevant.", "ebr_raw": "95", "ebr_num": 95, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in sentence 11-20", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "7": {"question_id": "3", "question": "Why had the team not gone abroad since 2015?", "system": {"davinci": {"answer": " The answer is in the following article.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The reason why the team had not gone abroad since 2015 is not stated in the article, but it could have been because there has not been any occurrence of earthquake since 2015", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The Why question was not addressed by the text, and completely ignored by the answer's author", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not go into detail as to why they have not been abroad since 2015.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document doesn't mention the reason why the team has not gone abroad since 2015.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct. The document does not contain the info needed to answer the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide a specific reason as to why the NDRF had not sent a team abroad for rescue work since the 2015 Nepal earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no part of the article that talks about the reason why the NDRF had not gone abroad since 2015 ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asked for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The article did not provide information on why the NDRF had not gone abroad since 2015.", "ebr_raw": "20", "ebr_num": 20, "workerid": "7"}}}, "human_1": {"answer": "It was not clear why no NDRF team had gone abroad since 2015.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the requested information is not available in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0. The question requires outside information which the answer acknowledges. The response does not contain any relevant information from the article to answer the question.", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not give a reason why the NDRF had not gone abroad since 2015", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as there is no specific answer given in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state the reason why the team had not gone abroad since ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Response is accurate and correct", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification. The fact is simply stated in sentence 4 and never followed up with. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " It is not stated in the article why the team had not gone abroad since 2015.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article mentions floods, cyclones, and earthquakes but virtually nothing else in the answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this question would have required outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not clearly state the reason why the team have not gone abroad since 2015", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state why the team had not gone abroad since 2015 but I believe it could be because there ahs not been any reason for them too go abroad.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The NDRF had not sent a team abroad for rescue work since the 2015 Nepal earthquake.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not state the reason why the team had not gone abroad since 2015", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer does not miss anything, but also doesn't really answer the question fully", "ebr_raw": "80.", "ebr_num": 80, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article does not give any more information that could answer the \"why\" question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The NDRF had not sent the team abroad since 2015 according to sentence 4", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "8": {"question_id": "4", "question": "How does the NDRF work?", "system": {"davinci": {"answer": "The National Disaster Response Force (NDRF) is a specialised force constituted by the Government of India for the purpose of specialist response to a disaster. It functions under the aegis of the Ministry of Home Affairs (MHA) of the Government of India. The NDRF is a part of the National Disaster Management Authority (NDMA) and is one of the disaster response elements of the National Disaster Response Force.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced based on the scenarios in the article. ", "ebr_raw": "75", "ebr_num": 75, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is probably an accurate answer, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported. The response missed the information stated in sentences 3,28,31.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not address how exactly the NDRF works. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The National Disaster Response Force (NDRF) is an Indian specialized force that works on disaster response and relief operations, specifically for search and rescue missions. They are equipped and trained to provide quick and efficient relief during disasters, such as earthquakes, floods, and cyclones. The NDRF works closely with local authorities, and occasionally, they are sent abroad to help with rescue work in other countries, as was the case in the 2015 Nepal earthquake and the recent earthquake in Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed from the article, considering the activities the NDRF partook in from the article. ", "ebr_raw": "85", "ebr_num": 85, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially accurate and relevant and supported by the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on the article, the NDRF works as a rescue team, they go on rescue missions after the occurrence of natural disasters. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated how the NDRF works. The force sent abroad participated in rescue efforts, including using dogs to find two trapped girls.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is correct in stating that the info is not given in the document. The second answer sentence is not relevant however as it is describing what the group does and what it accomplished rather than how it functions as an organization which is what the question was looking to find out about.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The above sentences infers how the NDRF works", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is basically accurate and correct", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "Though not explicitly stated, it can be inferred through the article that the NDRF is an Indian volunteer natural disaster relief team. They go abroad when there are natural disasters to help the people on the ground, as well as the rescue teams in those countries. The team currently has 151 members -- all volunteers. They have assisted in missions for events like the 2015 Nepal earthquake. The team aims to put \"humanity first\" and also gives aid to countries like Ukraine and Afghanistan, supplying medicines and vaccines. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article does not explicitly say that the NDRF is an all-volunteer force, but otherwise, this is a valid and comprehensive answer", "ebr_raw": "95", "ebr_num": 95, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is overall good and relevant and pretty much uses information contained in the article to form the answer", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed from the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer correctly ids that the info is not stated. There are errors in its inference however in that it was not implied that it was volunteer organization. Rather, it was stated that Modi did similar work as a volunteer, but the status of the NDRF's members was not talked about. Sentences 31 and 32 support that they go aborad in natural disasters, but it is also implied that they assist domestically.  The final sentence mixes up India as a whole with the NDRF.", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The NDRF is a response force that works to rescue people in the aftermath of natural disasters such as earthquakes, floods, and cyclones. They are equipped with specialized tools and techniques to help in search and rescue operations. They also provide medical assistance and relief to affected areas.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The article mentions floods, cyclones, and earthquakes but virtually nothing else in the answer.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is a well-written answer, and probably 100% accurate, but it is not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the rescue dogs as per sentence 20.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document states that the NDRF is a rescue mission that goes out to help people after a natural disaster has occurred. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "the article talks about the NDRF as a search and rescue mission that helps after a natural disaster. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The National Disaster Response Force (NDRF) is a specialized force constituted \"for the purpose of specialist response to a threatening disaster situation or disaster\" under the Disaster Management Act, 2005. The NDRF is trained to respond to various types of disasters such as floods, cyclones, earthquakes, landslides, industrial accidents, and terrorist activities. The NDRF personnel are trained in search and rescue operations, medical aid, firefighting, and other disaster response activities. They are equipped with specialized", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document summarizes the work of the NDRF in rescuing mission during disasters", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The document doesn't answer the question fully, but there is some relevant information missed, and the response uses outside information not contained in the document", "ebr_raw": "Score: 70. The response does incorporate some relevant information from the article, but it is incomplete and uses outside information not contained in the document.", "ebr_num": 70, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The article mentions floods, cyclones, and earthquakes but virtually nothing else in the answer.", "ebr_raw": "Score: 5. The response briefly mentions an earthquake but misses a lot of relevant details from the article, including the role of the NDRF in search and rescue operations in Turkey, the Prime Minister's interaction with the NDRF team, the NDRF's mission to be the best response force in the world, the NDRF's use of rescue dogs, and Modi's comments on India's selfless service to other nations. The response falls far short of providing a complete and accurate summary of the article.", "ebr_num": 5, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is loosely based on the article, but it is almost entirely outside information", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The NDRF serves as a rescue mission during the time of disaster according to the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}}}, "22": {"dataset": "inq-ext", "article_num": 15, "question_details": {"0": {"question_id": "0", "question": "How are the police trying to locate the truck driver?", "system": {"davinci": {"answer": " The police are trying to locate the truck driver by checking the CCTV footage of the area.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No reference was made to CCTV in the article, so this answer was probably fabricated", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is wrong, and misses relevant info.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The driver's identity has been ascertained. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The police know who the driver is and it's just a matter of time before he's arrested.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct. There is no mention of cctv in the document. The document does not provide much information on this topic, only noting in 25 that his identity was ascertained, and that there was an FIR registered against the driver (sentence 29). ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The police have ascertained the identity of the truck driver and are working on arresting him.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information is found in #25 and #26. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 25 and 26 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and supported by the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_1": {"answer": "It is not specifically stated how the police are trying to locate the truck driver, but they assert that he will be arrested soon.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information, but relevant passages from the article are still successfully retrieved", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. It is not stated how they will locate the suspect.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct in that the details of the search are not given. The info included from sentence 25 is not relevant however as that info is merely stating that they intend to find him whereas the question is asking how they are going to do that.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct, but misses some seemingly relevant information", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The explicit answer to this question was not given in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The police know who the driver is and it's just a matter of time before he's arrested.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. They only mentioned they know his name, but not how he will be located and/or arrested. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The driver's identity has been ascertained.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response misses some information that is relevant to the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": " Police have ascertained the identity of the truck driver and are working to arrest him soon.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some relevant information was missed", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Correct. Information in #25 and #26.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 25 and 26 backs up the machine response.  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The police are trying to locate the truck driver by using the registration number of the truck and by ascertaining his identity. They are also investigating the incident to find any clues that could lead to the driver's whereabouts.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 29 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not given within the document. Looking at the answer, the second answer sentence was not drawn from the document as it makes no mention of such actions. The first answer  sentence references the truck's number, but it was never said in the document that this number was aiding in the search. Following, it is stated in sentence 25 that the man's identity was ascertained, but it is never explained how that is aiding in the search.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is generally good and correct but the last sentence technically isn't from the document, and seems more like speculation or inference", "ebr_raw": "95", "ebr_num": 95, "workerid": "2"}}}}}, "1": {"question_id": "0", "question": "Why was Sharma the sole breadwinner in his family?", "system": {"davinci": {"answer": "The answer is simple. He was the only earning member in his family.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_all", "feedback": "That overly literal response ignores key information", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sharma's brother is still unemployed, his parents live with him, and his wife is pregnant.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is technically correct, and the document gives no further explanation.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the story in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as it is restating already known info, just in a different way. The questioner wants to know why no one else in the family earns money. The article does not say why they do not. For example, sentence 2 notes his brother is still unemployed, but does not say why that is the case.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide a specific reason for Sharma being the sole breadwinner in his family. It only mentions that his younger brother is unemployed and that their parents lived with the deceased in Kherki Daula.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The fact that Sharma's wife was pregnant is also relevant to this question", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "From the article's description, Sharma is the breadwinner because his younger brother is unemployed.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sharma is the bread winner of the family mainly because his younger is unemployed. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer should also have mentioned that Sharma's wife is pregnant", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}}}, "human_1": {"answer": "It is not stated why Sharma was the sole breadwinner in his family.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The question could at least have been partially answered by referring to the fact that Sharma's wife is pregnant, which would explain why only her husband was working", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "While the AI is slightly accurate, it is inferred from the text that they were the sole breadwinner because their brother was unemployed and their parents lived with the deceased. We can assume no other family member has a job.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It can be inferred from the article that Sharma is the sole breadwinner of his family because his younger brother is still unemployed. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sharma's brother is still unemployed, his parents live with him, and his wife is pregnant.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. Sentences 13, 14, and 16 mention their being an unemployed brother, parents, and pregnant wife, but there is no mention of why they are unemployed/not working. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "human_2": {"answer": "There are several reasons why Sharma had become the sole breadwinner in his family. His younger brother was unemployed and struggling to find work. Sharma's wife was pregnant and likely was unable to work. He also had two young kids under the age of 3, who would need to be taken care of while Sharma was working. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "While correct, the AI forgot to mention that his parents also lived with him.", "ebr_raw": "70", "ebr_num": 70, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is mostly correct but missed the fact about the surviving parents, as per sentence 26.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " Sharma's younger brother is still unemployed and their parents lived with the deceased in Kherki Daula.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The fact that Sharma's wife was pregnant is also relevant to this question", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. He was the only person in his family according to the article. His brother was unemployed #14 and his parents lived with him, but we are not told if they were working other than the fact that #1 and #13 tell us as such.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and accurate and contains info from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sharma is the only employed person in his family according to sentence 1.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": " Sharma was the sole breadwinner in his family because his younger brother was still unemployed and his parents lived with him in Kherki Daula.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The status of the deceased's wife is also relevant to this question", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in sentence 2 and 3", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not answer the question. As it stands, the first part just reinforces that Sharma was the sole breadwinner and the second part does not speak of employment but rather where his parents live. The document does not indicate why he alone was the only one to be making money in the family (does not explain why his brother is unemployed/his parents don't work)", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "2": {"question_id": "1", "question": "How would the truck driver be charged if caught?", "system": {"davinci": {"answer": " The answer is given in the following section.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and misses all useful details in the story", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The correct answer is given in sentence 29. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "An FIR against was registered against the driver under Section 279 (rash driving or riding on public way), 304A (causing death by negligence), and 338 (grievous hurt endangering life or personal safety of others)", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as it fails to list the charges given in 29: rash driving or riding on a public way, causing death by negligence, and grievous hurt endangering life or personal safety of others. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "If the truck driver is caught, they would be charged under Section 279 (rash driving or riding on public way), 304A (causing death by negligence), and 338 (grievous hurt endangering life or personal safety of others) of the Indian Penal Code (IPC) at Kherki Daula police station, according to the police.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. All information found in #29.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The last sentence in the article matches the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and all facts are supported by the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_1": {"answer": "The article does not describe what charges will be pressed against the truck driver.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "A relevant answer could have been taken from the final passage in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "An FIR was registered police said. Section 279 (rash driving or riding on public way), 304A (causing death by negligence) and 338 (grievous hurt endangering life or personal safety of others) of the Indian Penal Code (IPC).", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct. The info needed is given in sentence 28. There all of the charges are listed.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "An FIR against was registered against the driver under Section 279 (rash driving or riding on public way), 304A (causing death by negligence), and 338 (grievous hurt endangering life or personal safety of others)", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not relevant and misses all important information", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "human_2": {"answer": "The driver would be charged for causing the accident and killing one person, as well as fatally injuring two others. Specifically, the driver would be charged under Section 279 of Indian law, which declares he was driving in a \"rash\" manner. He would also be charged with Section 304A, \"causing death by negligence\", and 338, \"grievous hurt endangering life or personal safety of others\". The police are searching for the driver and have already registered the charges.  \n", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence is more of an inference from facts in #8, but the AI get is wrong when it states \"fatally injuring two others.\" Those were not fatalities.That sentence is not really needed. The bulk of the information about the charges is found in #28. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 28 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and based on information provided in the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The truck driver will be charged under Section 279 (rash driving or riding on public way), 304A (causing death by negligence) and 338 (grievous hurt endangering life or personal safety of others) of the Indian Penal Code (IPC).", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information is in #29.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 29", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "If the truck driver is caught, he will be charged with Section 279 (rash driving or riding on public way), 304A (causing death by negligence) and 338 (grievous hurt endangering life or personal safety of others) of the Indian Penal Code (IPC).", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 29 ascertains the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly draws on sentence 29 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "3": {"question_id": "1", "question": "Why is his younger brother still unemployed?", "system": {"davinci": {"answer": "The younger brother is unemployed because he is still unemployed.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "No further information was available in the article, though the answer writer clearly didn't even check", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why the younger brother is unemployed.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer isn't helpful, while the document does not address the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no reason given as to why his younger brother is still unemployed. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is repeating what is asked in the question. The document does not indicate why he is still unemployed, thus making this question not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide an explanation for why his younger brother is still unemployed.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no evidence to show why his younger brother is still unemployed. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no reason for why his younger brother is unemployed. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}, "human_1": {"answer": "It is not stated why Sharma's younger brother is unemployed.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told why the brother is still unemployed. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state why the younger brother is still unemployed", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. The document fails to disclose the reason for his unemployment.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not stated.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The reason why the younger brother is still unemployed is not stated in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " The essay does not provide any information as to why his younger brother is still unemployed.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question would have required outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told why the brother is unemployed. The AI correctly identified this.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not give the information as to why the younger brother is still unemployed.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and as good as possible given the info provided in the documet", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "The reason why his younger brother is still unemployed is not specified in the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not state why the younger brother is still unemployed.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question would require outside information (or speculation)", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not truly state why the younger brother is still unemployed.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the info is not provided.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}}}, "4": {"question_id": "2", "question": "How do these barriers help to protect the construction workers?", "system": {"davinci": {"answer": "The answer is simple. The barriers are meant to protect the workers from the oncoming traffic.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although the answer writer clearly isn't reading the articles, this answer is valid", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is somewhat relevant but not as complete as it could be", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The barriers should help guide a driver away from the area, therefore, protecting the people who are working on the other side of the barriers. Unfortunately, this driver plowed through the barriers.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not explain how this is accomplished. That info is contained in 4, which notes that the barriers were meant to divert traffic from the workers. 7 and 8 indicate that they were meant to make traffic slip into the other lane, away from the workers..", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The barriers, which are barricaded with iron frames and reflectors, help to protect the construction workers by providing a physical barrier between them and the traffic. These reflectors make the barriers more visible to drivers, ensuring that traffic traveling toward the construction site can see the barriers and are able to shift into other lanes, thus avoiding the workers and allowing them to conduct the repair works safely.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Though not specifically stated on how they would work, the AI correctly inferred from what would be considered general knowledge. The only mention in the article other than how the barriers were constructed was how they were meant to divert traffic #6, which the AI also included in it's generalization.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 and 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and all facts are supported by the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_1": {"answer": "The article states that the iron frame barricades were intended to divert traffic around the workers.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. The answer was already contained within the statement.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentences 4 and 6.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and correct", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "The barriers are set up to keep traffic flowing and away from the road repair workers. The barriers are reinforced with strong iron frames to create a strong physical barrier. The barriers also have reflectors on them; these are well-suited for night work and help drivers stay on the road when it is dark. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Some minor inferences, but still spot on.", "ebr_raw": "95", "ebr_num": 95, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is relevant and a good summary of information contained in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The barriers are used to divert the incoming traffic away from the construction site, allowing the workers to safely carry out their repair works. The reflectors help to make the barriers more visible to drivers, reducing the risk of accidents.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the information in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence is not stated in the document anywhere. This makes the response unsupported.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The AI took some liberties with general knowledge. It is not discussed how the barriers would protect the workers, just that it would protect them, but this does not satisfy the question that was asked, therefore I marked it as misses all. The AI should have just said its not disclosed in the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4, 6 and 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The barriers help to protect the construction workers by diverting the incoming traffic away from the construction site. The barriers, which are made of iron frames and reflectors, alert drivers to the presence of the workers and ensure that they are able to safely carry out their repair work without the risk of being hit by oncoming traffic.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not directly taken from the text, but it fits with the information therein", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence of the answer draws from sentences 7 and 8 in terms of diverting traffic and is correct to include in the answer. The sentence which follows reports info that is not specifically stated within the document.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and relevant and answers the question with info from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "5": {"question_id": "2", "question": "What is the incident that occurred in Kherki Daula?", "system": {"davinci": {"answer": "The incident is the death of a worker.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "That answer is technically correct, but overly simplified", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer is only partially correct. A worker was crushed to death and two others, including a private security guard, were grievously injured when a speeding trailer truck ran over them after hitting traffic diversion barriers.", "ebr_raw": "Score: 25", "ebr_num": 25, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is correct but misses useful details in the article", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is correct in mentioning the worker's death, but there is a lot of info missing from 2. The workers who were injured need to be mentioned as well as how they were injured to fully describe the incident.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "6"}}}, "gpt4": {"answer": "The incident that occurred in Kherki Daula involved a speeding trailer truck running over and killing a worker, while also causing serious injuries to two others. The truck hit traffic diversion barriers on National Highway (NH-48) near Sector 82 and crashed into the workers who were carrying out repair work on the road.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is found in sentence 4", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The incident involved a speeding trailer truck running over workers after hitting traffic diversion barriers.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid, if concise, answer", "ebr_raw": "95", "ebr_num": 95, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer uses sentence 4 to give a broad overview of the incident, thereby answering the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_2": {"answer": "A worker and two other people were crushed to death in Kherki Daula in the early hours of the morning. The worker and two other citizens were killed by a speeding trailer truck. The truck had hit the traffic diversion barriers on the national highway. After the driver hit the people, they got out of the trailer truck and fled the scene. The police suspect the driver had fallen asleep and was unable to change lanes in time to avoid the accident. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The driver ignored barriers and killed 1 person and injured 2 others.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is incorrect because only one worker was dead and the other two were injured, as per sentence 4.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4, 20, 23 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is incorrect. A worker was crushed to death and two others , including a private security guard , were grievously injured when a speeding trailer truck ran over them after hitting traffic diversion barriers.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " A worker was crushed to death and two others, including a private security guard, were grievously injured when a speeding trailer truck ran over them after hitting traffic diversion barriers on National Highway (NH-48) near Sector 82 in the wee hours of Sunday.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct, the statement that question was derived from has all of the answers.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and relevant and answers the question with info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": " The incident that occurred in Kherki Daula was a speeding trailer truck running over three people, killing one and injuring two others. The deceased was Bramhadev Sharma (26), while Rahul Das (18) and guard Subhash Kumar (28) were critically injured.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Sentence 4 clearly detailed the incident that occurred in Kherki Daula.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentences 4 and 9 are used to craft the answer which appropriately answers the question. ", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}}}}}, "6": {"question_id": "3", "question": "How are labour contractors responding to the situation?", "system": {"davinci": {"answer": " The labour contractors are not responding to the situation. They are not even aware of the situation. They are not even aware of the fact that the workers are dying. They are not even aware of the fact that the workers are getting injured. They are not even aware of the fact that the workers are getting killed. They are not even aware of the fact that the workers are getting crushed. They are not even aware of the fact that the workers are getting run over. They are not even aware of", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is unhelpful and speculation/false. There isn't any specific info to answer the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The labors contractors are keeping track of the victim in the hospital.  ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They have been to the hospital and are working with the police.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer draws from an outside source as the document does not mention the response among this group of people.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "In response to the situation, the labor contractor, Raj Kumar, said that Das and Kumar were rushed to a private hospital in Sector 87 and are undergoing treatment. Furthermore, the contractor has not yet told Sharma's wife about his death, fearing that her health might deteriorate or she might lose the unborn child. They have also put the repair work on hold.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "While this may not have been the intent of the question asker, all of the relevant and available information is used to give a somewhat literal answer to the question", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 17-19 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. The AI correctly stated that they put the work on hold. Though I would have taken the question to be how have they responded to the work, the AI inferred how has the contractor responded in general and included that in the complete answer. I don't think it's wrong for doing so and adds a bit more for the reader.", "ebr_raw": "90", "ebr_num": 90, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and all facts are sourced from the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_1": {"answer": "It is not stated how the labour contractors are responding more broadly, but the repair work has been put on hold, which likely has an impact on the contractors.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. It is not stated specifically. Only specifics given is found in #12, which is the repair work was put on hold.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer is correct in that it correctly identifies that the info is not provided in the document. The second part of the sentence however is not related to how the contractors are responding, rather it is speculating about how they may be affected, which is quite different.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is a bit vague, but the response seems relevant and correct based on the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this question would have required outside information (or speculation)", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The only thing we are told is that the work has been put on hold.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The labor contractors are keeping track and getting updates of the injured victims", "ebr_raw": "Score: 10. The response only partially touches on the topic of injured victims and lacks any specific information from the article.", "ebr_num": 10, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as there is no specific answer given in the article ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_few_shot": {"answer": "The labour contractor is providing support to the injured workers and the family of the deceased. He is making sure that the injured workers are receiving medical treatment and that the family of the deceased is being taken care of.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some relevant information was missing", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "'that the family of the deceased is being taken care of.' in the response is unsupported. The response missed the responses as per sentences 13 and 17.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The AI embellishes. There is no mention of how the contractor is responding to the injured workers so not sure why it was stated a as matter of fact by the AI. The only response we are given is the contractor put the work on hold at the site, as found in #13.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The contractor is making sure the surviving workers are receiving medical treatment and getting updates on the family which is confirmed in sentence 10", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": " Labour contractors are responding to the situation by providing medical care to the injured workers. They have also filed an FIR against the driver of the truck and are supporting the family of the deceased by not disclosing the news of his death to his pregnant wife.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 15-19 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Sentence 1 of the answer is not correct as it is said in sentence 10 that they are getting treatment at a hospital, not from the contractor. It is not said who is paying for it. Sentence 29 indicates that an FIR was filed based on the contractor's statement, but it is not stated who did the filing. It could be said though that they gave a statement. The final sentence is accurate as that was stated in sentence 17", "ebr_raw": "60", "ebr_num": 60, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is generally correct, but it says the contractor is providing medical care, which isn't stated in the document", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}}}}}, "7": {"question_id": "3", "question": "How are those who were injured recovering?", "system": {"davinci": {"answer": " The injured are recovering.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no further information on how the two injured workers are recovering.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is relevant but misses relevant info", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "One of the injured needs to have his leg amputated. ", "ebr_raw": "40", "ebr_num": 40, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer just states that they are recovering without stating how. The document does not provide a lot of detail to this affect, only stating that they are receiving treatment (10) and that one of them needs to have a leg amputation (19).", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The two injured individuals, Rahul Das and Subhash Kumar, are currently undergoing treatment at a private hospital in Sector 87. Das reportedly sustained serious injuries and may need to have his left leg amputated.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10, 11 and 19 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There is no further information on how the two injured workers are recovering.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 and 19 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, although the article seems to suggest that Das's leg does need to be amputated, while the answer implies that there is still some doubt.", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}}}, "human_1": {"answer": "Brahmadev Sharma was killed in the accident. However, the other two workers, Rahul Das and Subhash Kumar, survived in critical condition and were rushed to a hospital. Das's left leg needed to be amputated to save his life.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Found in #8 and #18.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in different parts of the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first answer sentence is not relevant as it is about Bramhadev Sharma. He was a killed worker and not an injured one. The question only asked about the injured workers. The sentences which follow discuss the injured workers, but there is no info in the document stating the status of their recovery beyond the fact that Das needs a leg amputation. So the answer should state that beyond the leg amputation, such info is not available.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "human_2": {"answer": "Bramhadev Sharma, the worker who was hit, died due to the accident. The other two people on the scene, Rahul Das and Subhash Kumar, survived the hit, but are in critical condition. They were taken to a hospital immediately for emergency care. However, the injuries were severe and will likely have long-term impacts on their health and wellbeing.  ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The two survivors were rushed to the hospital due to critical injuries and one will loose his leg.", "ebr_raw": "50", "ebr_num": 50, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response doesn't mention the amputation of Das, as per sentence 18. The last sentence of the response is unsupported.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " According to police, Rahul Das (18), and guard Subhash Kumar (28), were critically injured and were rushed to a private hospital in Sector 87 and are undergoing treatment. Das's left leg needs to be amputated to save his life.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All available information is included in the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. #9 and #19 have the information.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response answers the question well and gives all relevant info needed from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10, 11 and 19 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "According to police, Rahul Das (18), and guard Subhash Kumar (28), were critically injured and rushed to a private hospital in Sector 87. They are currently undergoing treatment, but it has been reported that Das's left leg may need to be amputated to save his life.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 and 19 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 19 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence of the answer speaks to who was injured and where they were taken after the injury, both of which are beyond the scope of the question, making that sentence not relevant. The second setnence is drawn from sentences 10 and 19 in the document do serve to answer the question. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}}}, "8": {"question_id": "4", "question": "Why/How did the truck hit the traffic barriers? ", "system": {"davinci": {"answer": " The truck hit the traffic barriers because the truck driver was driving rashly.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is technically correct, but was probably not even based on a reading of the article, as key information is missed", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The barriers were in place but the driver didn't slow down and hit the workers. Officials say he may have fallen asleep but that is unknown that the time the article was written.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is reasonable based on the article, but lacks detail and misses important info", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 24 provides an accurate answer to the question. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is supported by the charges levied in 29. That being said, there is additional info which should be included in 24, which notes that there is the possibility that the driver fell asleep.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "6"}}}, "gpt4": {"answer": "The truck hit the traffic barriers because it was speeding and the driver did not slow down or change lanes in time to avoid the barriers. There is a possibility that the driver had fallen asleep and was not in a position to change lanes timely.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 20 and 21 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 20 and 24 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "It is not known why the truck hit the traffic barriers. However, according to a police inspector, the driver may have been asleep at the wheel. Moreover, the truck was loaded with stone chips, so it can be inferred that the truck may have been harder to stop than if it had been empty.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information from the article is included, along with some reasonable supposition", "ebr_raw": "Score: 90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Found in #19 through #23.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21-23 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first two answer sentences are correct in their identification and their use of sentence 23 regarding the driver potentially falling asleep. The final answer sentence does not align with the document however as sentence 20 notes the truck did not slow down before hitting the barriers, indicating the load did not have any bearing on the cause.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "human_2": {"answer": "The truck hit the traffic barriers because it failed to change lanes in time. The police suspect that the driver, who fled the scene after hitting three people, likely was asleep or falling asleep, given the time of the accident, and was unable to change lanes in time to avoid the accident. Additionally, the truck was speeding and hit the barriers hard. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 19-23 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " According to police, a private firm was carrying out repair work on the accident spot and two workers and the guard were engaged in diverting the incoming traffic. Investigators said the spot was barricaded with iron frames and reflectors to protect the workers and ensure that the traffic, travelling towards Delhi, slipped into the other lane for at least 200 metres for smoothly conducting the repair works. Police said the deceased, Bramhadev Sharma (26), died on the spot after coming under the wheels of the trailer", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "We only have speculation from the police that the driver fell asleep at the wheel.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The barriers were in place but the driver didn't slow down and hit the workers. Officials say he may have fallen asleep but that is unknown that the time the article was written.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4-8 gives detail of how the accident happened.  ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The truck hit the traffic barriers because the workers and the guard were diverting the incoming traffic and the spot was barricaded with iron frames and reflectors to protect the workers and ensure that the traffic, travelling towards Delhi, slipped into the other lane for at least 200 metres for smoothly conducting the repair works. However, the truck driver failed to notice the barricades and ran over them.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 20-22 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4-8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The first answer sentence describes the physical scene and why the barriers where there, but does not address why the truck hit them, making the sentence not relevant. The final answer sentence does address the question but is not supported by what is said in the document. Rather, sentence 24 provides the info that should be used in its place. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}}}, "23": {"dataset": "inq-ext", "article_num": 16, "question_details": {"0": {"question_id": "0", "question": "Why did the political battle move to the legislative assembly?", "system": {"davinci": {"answer": "The battle moved to the legislative assembly because the speaker of the legislative assembly is the head of the legislative assembly.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Speaker Narwekar said the ECI had recognized them as the Shiv Sena and the office now belonged to them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The political battle moved to the legislative assembly because the Shiv Sena party office took over", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and unsupported. It missed the information in sentence 9.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The answer is better stated using sentence 5, which notes that the election commission awarded the party name and symbol to the Shinde faction. This faction then took over the Shiv Sena party office and met with speaker Rahul Narwekar (sentence 2)", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The political battle moved to the Maharashtra legislative assembly because the Election Commission had allocated the Shiv Sena party name and the bow-and-arrow symbol to the faction led by Maharashtra chief minister Eknath Shinde on February 17. This led to Shiv Sena's chief whip Bharat Gogawale, accompanied by MLAs owing allegiance to Eknath Shinde, meeting speaker Rahul Narwekar and taking charge of the Shiv Sena party office.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The article describes a clash between two factions in the Shiv Sena, the Shinde and Uddhav faction. The relevant event is that a group of the Shiv Sena were recognized by the ECI over the Uddhav faction. The Uddhav faction then rushed to the BMC headquarters and political assembly to forestall further loss of their power. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information is retrieved from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first answer sentence is not correct in that the two parties were the Shiv Sena and the Uddhav. Sentence 1 holds the correct info.  The middle answer sentence is partially correct in referencing info from sentence 4. Info from sentence 3 however is also needed as that describes the way the battle was moved (a taking over of offices). The final answer sentence involves the BMC and not the legislative assembly, thus making it not relevant. ", "ebr_raw": "Score: 66", "ebr_num": 66, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Only the first sentence of the response is correct and supported and the other parts are unsupported by the passage. The response didn't mention the correct reasons in sentences 9 and 12.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 3 and 6 states that the legislative assembly took charge without been elected ", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Practically all relevant information was missed", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Speaker Narwekar said the ECI had recognized them as the Shiv Sena and the office now belonged to them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 9 in the document, it was because former corporators from Uddhav Thackeray\u2019s faction feared that Shinde\u2019s men would also takeover the party\u2019s office at the Brihanmumbai Municipal Corporation, so they rushed to the civic headquarters.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as the info needed to answer the question is in sentences 3 and 4 where it is stated that there was a meeting with the speaker and an office take over.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Shiv Sena's chief whip Bharat Gogawale accompanied by MLAs owing allegiance to Eknath Shinde met speaker Rahul Narwekar on Monday morning and took charge of the Shiv Sena party office. Until Friday, it had belonged to the Uddhav Thackeray faction.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Some relevant information was missed", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response seems relevant and accurate overall, and the info is sourced from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is marked as incorrect and missing all information, as it missed the reasons as per sentences 13,14,15 and 16.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The political battle moved to the legislative assembly because Shiv Sena's chief whip Bharat Gogawale accompanied by MLAs owing allegiance to Eknath Shinde met speaker Rahul Narwekar on Monday morning and took charge of the Shiv Sena party office. Until Friday, it had belonged to the Uddhav Thackeray faction. The Election Commission had previously allotted the party name and the bow-and-arrow symbol to the faction led by Maharashtra chief minister Eknath Sh", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The answer is valid, but cut off", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-4 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer seems relevant and comes from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer was cut off, and could have included more information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}}}}}, "1": {"question_id": "1", "question": "Why did he take charge of the office?", "system": {"davinci": {"answer": " The office belongs to the party, not to the person.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Narwekar told the media that ECI had recognized them as the Shiv Sena and the office now belonged to them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The above sentences provides information as to why he took over.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and unsupported. It missed the information in sentence 4.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question comes from 4 and 5, which say that the election commission gave the party name and symbol to the Shinde faction. The office was then claimed by the faction as the recognized Shiv Sena group.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "Bharat Gogawale took charge of the Shiv Sena party office because the Election Commission had recognized their faction, led by Maharashtra Chief Minister Eknath Shinde, as the official Shiv Sena party and allotted them the party name and the bow-and-arrow symbol.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in sentence 2-5", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "It is not clear precisely why they took charge of the office; however, given the political power struggle described in the article, they likely wanted more influence over the affairs of the Shiv Sena.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Narwekar told the media that ECI had recognized them as the Shiv Sena and the office now belonged to them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, given the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct. The reason is given in sentence 4", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response didn't mention the reason stated in sentence 4.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "Shiv Sena took charge of the party office because the party was officially recognized by the Election Commission (ECI). The party then claimed the office was rightfully theirs to take, due to the formal recognition. The Uddhav faction immediately went to their office to protect it after Shiv Sena took over the Shiv Sena party office. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The last sentence of the response is not relevant to the question.", "ebr_raw": "80", "ebr_num": 80, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first two sentences are supported by sentence 4 and they effectively answer the question. The final answer sentence is not relevant as it speaks of the further actions of Uddhav following the office take over.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The Election Commission on February 17 allotted the party name and the bow-and-arrow symbol to the faction led by Maharashtra chief minister Eknath Shinde. Since ECI has recognised them as the Shiv Sena, this office now belongs to them.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the apparent focus of the question", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears relevant and is as useful and possible given the article info", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "He took charge of the office because the Election Commission had recognized his faction as the official Shiv Sena party and thus the office belonged to them.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response matches sentence 4 and 5", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer seems relevant and accurate and uses info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "2": {"question_id": "2", "question": "How did the faction respond?", "system": {"davinci": {"answer": " The Uddhav Thackeray faction has not responded to the sentence.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Uddhav Thackeray challenged ECI's decision in the Supreme Court.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Uddhav Thackeray has challenged ECI \u2019s decision in the Supreme Court", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant, nonsense and unsupported. It missed the information in sentences 7,9,17,18,19,20.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is given in 7 and 9 which say that they challenged the commission decision in the Supreme Court and also rushed to the party office at the Brihanmumbai Municipal Corporation, fearing it would be taken over too.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The Uddhav Thackeray faction responded by challenging the Election Commission of India (ECI) decision in the Supreme Court, terming it as \"murder of democracy.\"", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is documented in sentence 7", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The Thackeray faction rushed to the civic headquarters, spearheaded by former mayor Vishakha Raut.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Uddhav Thackeray challenged ECI's decision in the Supreme Court.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The above sentence adds up to how the faction responded. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 9.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "The Uddhav faction responded by immediately going to the Maharashtra legislative assembly. The Uddhav faction also went to their headquarters to prevent the Shiv Sena faction from potentially taking over their offices, too. The faction reesponded by immediately criticizing the takeover and declaring the opposing faction undemocratic. The hostilities between the two factions increased significantly after the takeover. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7, 15 and 17 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the legal challenge as per sentence 7.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentences is not supported by any of the document's sentences. The 2nd and 3rd sentences are supported by 7 and 9, however there is missing information from sentence 7 as well. The challenge of the decision in the Supreme Court needs to be noted. The final answer sentence is not relevant as it goes beyond the questions scope.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": " Uddhav Thackeray has challenged ECI\u2019s decision in the Supreme Court and termed it as \u201cmurder of democracy\u201d. Fearing that Shinde\u2019s men would also takeover the party\u2019s office at the Brihanmumbai Municipal Corporation, former corporators from Uddhav Thackeray\u2019s faction rushed to the civic headquarters, spearheaded by former mayor Vishakha Raut.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7-9 corresponds to the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response answers the question pretty well and gives all relevant info needed from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "The Uddhav Thackeray faction has challenged the Election Commission's decision in the Supreme Court and termed it as \"murder of democracy\". They have also been camping outside the Shiv Sena party office at the Brihanmumbai Municipal Corporation since the morning, with former corporators from Uddhav Thackeray's faction leading the charge.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7-11 corresponds to the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is vague, but this is a reasonable answer", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}}}}}, "3": {"question_id": "3", "question": "Why was the constitution considered undemocratic?", "system": {"davinci": {"answer": "The constitution was considered undemocratic because it was not drafted by the party members. It was drafted by Bal Thackeray himself.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It's unclear what this answer is even referring to", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The constitution was not considered undemocratic, it was Shiv Sena. It is because the Shiv Sena party appointed people as office bearers without any election. Elections are called for in the constitution.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The party was not voted in, instead they were appointed. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incorrect and unsupported. It missed the information that the office bearers were appointed without any election, as per sentence 6. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not supported by the document. Sentence 6 indicates that the constitution was judged to undemocratic as it appointed people from a coterie as office bearers without any election at all.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The constitution of the Shiv Sena party was considered undemocratic because it had been \"mutilated to undemocratically appoint people from a coterie as office bearers without any election at all,\" according to the Election Commission of India (ECI).", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is seen in sentence 6.", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in sentence 6.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "It is not clear why the constitution was considered undemocratic, although the stated reason is a lack of election for appointed officials.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "There was no election for people to vote in their political choice, instead the ruling party was appointed into power which is against the democratic law. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer is not correct as the answer is given properly in the second portion of the answer (supported by sentence 6).", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "The Shiv Sena constitution is considered undemocratic because of the way leaders are appointed. The Uddhav faction argues that the process is undemocratic and ordinary citizens do not get a say in the which leaders are appointed. The process is closed off to members of the party and doesn't allow democratic elections. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response corresponds to sentence 6", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence is supported by sentence 6. The second sentence repeats what was already said and introduces info about ordinary citizens that is not in the document. The final sentence is a summary of sentence 6 which helps further explain the situation and answer the question. ", "ebr_raw": "Score: 67", "ebr_num": 67, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The constitution of the Shiv Sena party was undemocratic because it allowed people from a coterie to be appointed as office bearers without any election.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a brief but valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 6", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is concise, correct, and sourced from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The constitution of the Shiv Sena party was considered undemocratic because it allowed people from a coterie to be appointed as office bearers without any election.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 24 summarizes the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears to be accurate and uses info from the document to answer the question ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "4": {"question_id": "4", "question": "How did people respond to these undemocratic elections?", "system": {"davinci": {"answer": " The Shiv Sena was founded in 1966 by Bal Thackeray, who was then a cartoonist with the English language daily, Free Press Journal. The party was set up to counter the growing influence of the Indian National Congress in Mumbai. The Shiv Sena's main objective was to protect the rights of the native Marathi people of Maharashtra. The party's main agenda was to ensure that the state government jobs and educational institutions were reserved for the native Marathi people. The party also wanted to protect", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not in the article. There is nothing about the feelings of the people.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the people are protesting at the civic headquarters.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and unsupported. It missed the information in sentence 7.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not address this topic specifically. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide details on how people responded to the undemocratic elections in the Shiv Sena party.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question required outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The people challenged the commission and took their grievances to the civic headquarters", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The people are challenging the decision and visiting the headquarters every day", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}, "human_1": {"answer": "It can be inferred that the undemocratic elections were part of the reason for the splinter between the Shinde and Thackeray factions, leading to the present series of events.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable supposition, given the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by what is written in sentences 5 and 6. Technically, this answers the question, as those in the opposite political faction would be people, but I think the question is more looking for regular people's opinions, but those are not given in the document.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 7-18 talks about how people respond to these undemocratic elections", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the other reaction as per sentence 7.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The above sentences states that the people have been going to the offices to try to fight for their right", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer to this specific question was not given in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 7 in the document,  Uddhav Thackeray has challenged ECI \u2019s decision in the Supreme Court and termed it as \u201c murder of democracy \u201d.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the info is not given.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "Uddhav Thackeray has challenged ECI\u2019s decision in the Supreme Court and termed it as \u201cmurder of democracy\u201d. Former corporators from Uddhav Thackeray\u2019s faction rushed to the civic headquarters, spearheaded by former mayor Vishakha Raut. The Mumbai police, anticipating a face-off, ensured heavy security. About 90 former corporators had attended a meeting on the weekend pledging loyalty to Uddhav Thackeray.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7-20 details the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response seems good, relevant, complete and is taken from info in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "The Uddhav Thackeray faction has challenged the Election Commission's decision in the Supreme Court and termed it as \"murder of democracy\". The former corporators from Uddhav Thackeray's faction have also rushed to the Brihanmumbai Municipal Corporation headquarters to prevent the Shinde camp from taking over the party's office. The Mumbai police have also ensured heavy security to prevent any face-off between the two factions.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is backed up in sentence 7-11", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response seems relevant and accurate overall, and the info is sourced from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "5": {"question_id": "5", "question": "Why was the decision coined as 'the murder of democracy'?", "system": {"davinci": {"answer": " The word 'murder' is used in the sense of 'killing' or 'slaughtering'.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 6 explains why it was called the murder of democracy. ", "ebr_raw": "0.", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and unsupported. It missed the information in sentence 6.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer provides synonym for murder, but those synonyms are not drawn from the document. The document does not say why the word murder specifically was used, but sentence 6 notes that that people were being appointed to officers without any election taking place at all.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The decision was coined as \"the murder of democracy\" because the Election Commission allotted the party name and the bow-and-arrow symbol to the faction led by Maharashtra Chief Minister Eknath Shinde, recognizing them as the legitimate Shiv Sena party, while Uddhav Thackeray's faction lost its recognition. Thackeray has challenged this decision in the Supreme Court, believing it to be undemocratic and unjust.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "In addition to the machine response, the decision is coined as murder of democracy because there was no election conducted. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The people were appointed instead of been elected into power.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "It is not stated why this decision is \"the murder of democracy\". Thackeray is displeased with the decision because it favors his political adversary, Shinde. It is not clear whether the Election Commission needs to abide by particular democratic standards.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Relevant information from the text is used to try to answer a subjective question", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The final answer sentence is not relevant as it goes beyond the scope of the question. The first answer sentence is not correct as it contradicts what is in the second answer sentence. The second answer sentence is supported by sentences 5 and 7. 5 gives Shinde as being the rival in that he was the leader of the opposition and 7 indicates displease with the ruling via it being challenged. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 6 states why the decision is called a murder of democracy", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.The correct reason is the appointment of people from a coterie as office bearers without any election as stated in sentence 6.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "The decision was coined as 'the murder of democracy' because of the tensions between the two factions. The Uddhav faction declared that the ECI's constitution was \"undemocratic\" and that it had been \"mutilated\" to deliberately appoint leaders, rather than let the people have a voice in the government. The Uddhav faction strongly opposes the ECI's takeover. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer sounds correct, but it was never explained in the document why this terminology was used.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The decision was to allot the party name and the bow-and-arrow symbol to the faction led by Maharashtra chief minister Eknath Shinde. Uddhav Thackeray believes that this decision was undemocratic and was made without any election.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more detail could have been included.", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "According to sentence 6, the decision was coined  'the murder of democracy' because people are been appointed from a coterie as office bearers without any election at all", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is generally useful and relevant, and sourced from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence in the response is irrelevant, and the response doesn't mention that people were appointed from a coterie as office bearers, as per sentence 6.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The decision by the Election Commission of India (ECI) to allot the party name and the bow-and-arrow symbol to the faction led by Maharashtra chief minister Eknath Shinde was seen as undemocratic by Uddhav Thackeray. He argued that the constitution of the Shiv Sena party was undemocratically amended to appoint people from a coterie as office bearers without any election. He felt that this decision was a violation of democratic principles and hence", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The appointment into office was not legally done, and was not in accordance to the democracy they uphold according to the document", "ebr_raw": "40", "ebr_num": 40, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response doesn't miss anything important from the article, but does appear to contain some information that is not relevant to the question", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer was cut off, but it is unclear if it was going to refer to any more sections of text, as it already appears to be mostly sufficient", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}, "6": {"question_id": "6", "question": "How do the former corporators hope to resolve the situation?", "system": {"davinci": {"answer": "The former corporators are not going to resolve the situation. They are going to sit outside the party office for nobody visits BMC headquarters.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is garbled, but at least it fits with the text", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The Thackeray faction will have to wait on the decision by the Supreme Court on whether the ECI acted legally or not.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 7 states how the former corporators hope to resolve the situation.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The first part of the response is unsupported. It missed the information in sentences 18,19,20,21,22.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Information that speaks to resolution comes from Sentences 20 and 29, which note that there was a meeting of former corporators and also that there were apparent intentions to create a new party.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The former corporators from Uddhav Thackeray's faction hope to resolve the situation by physically occupying the party's office at the Brihanmumbai Municipal Corporation and preventing Shinde's men from taking over. They visit the headquarters every alternate day and sit outside the office, assisting people with their problems and meeting additional and deputy municipal commissioners to resolve issues. They also pledged loyalty to Uddhav Thackeray during a meeting attended by about 90 former corporators.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes some reasonable assumptions, but matches the overall theme of the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9, 16 and 17 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9 and 16 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "It is not clearly stated, but the corporators appear to be staging a protest and garnering support for their cause through their barricade of the BMC building.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The Thackeray faction will have to wait on the decision by the Supreme Court on whether the ECI acted legally or not.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "As much relevant information as can be taken from the article is included in this answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first part of the answer is correct in identifying that the info is not specifically stated. The info which follows in the answer is not relevant however as it is speaking more to how they plan to continue the conflict rather than resolve it.", "ebr_raw": "Score: 25", "ebr_num": 25, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed the othe actions that the former corporators do to resolve the situation, as per sentences 11,12,15,16,17,18,19.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The former cooperation are taking their fights to the offices and hoping that their voices are heard", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "At least a partial answer could have been inferred from the article text", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. But the answer is stated in sentences 17,18 and 19.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. It was stated in 15-18 that they go every other day to the office and sit on the sofas, guiding people around, but there was no mention of how the present situation would come to an end or how it would be solved.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "text-davinci-003_few_shot": {"answer": "The former corporators hope to prevent the Shinde camp from taking over the party's office at the Brihanmumbai Municipal Corporation by camping outside the office and showing their loyalty to Uddhav Thackeray. They also hope to resolve people's issues by taking them to the respective officers and resolving their issues.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9-19 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears to be correct and relevant and is taken from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The former corporators hope to resolve the situation by occupying the Shiv Sena party office and preventing the Shinde faction from taking it over. They are also visiting the BMC headquarters every alternate day to meet additional and deputy municipal commissioners and resolve people's issues.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 16-20 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Overall the response seems relevant, and all the info is from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}}}}}, "25": {"dataset": "inq-ext", "article_num": 17, "question_details": {"0": {"question_id": "0", "question": "How is the public responding to the new cryptocurrency regulations?", "system": {"davinci": {"answer": "The public is responding to the new cryptocurrency regulations in Hong Kong in the following ways:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "No attempt was made to actually answer the question using the text, though the public's response was not addressed in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, the people are responding positively to the regulations as it is believed that that a new wave of capital , talent and companies will set up office in Hong Kong to capitalise on these opportunities.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not mention how the public is responding to the new regulations.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer fails to list any of the ways in which the public were responding. The document also does not list any ways the general public is responding. 23 and 24 note optimism from company leaders, but they do not represent the general public consumer the question is asking about.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is irrelevant, and the document does not provide a clear answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The article does not provide information on the public's response to the new cryptocurrency regulations in Hong Kong.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct as there is no reference to how the public is responding to the new policies in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not make reference to how the public is responding to the cryptocurrency regulations ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}}}, "human_1": {"answer": "The response of the public in Hong Kong to the new cryptocurrency regulations is not described.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "the machine response is correct as the article did not talk about the people's reaction", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Outside information would be needed to answer this question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct, as it is not directly stated in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct as the public response is not stated anywhere in the passage. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "Based on information presented in the article, the public seems to be responding with enthusiasm. People are beginning to get the new SFC license to be able to particpate. The new regulations also allow people of various economic and financial backgrounds to get involved in the cryptocurrency trading world, which was something that was difficult before the regulatory expansion. The government seems to believe the opportunity is also lucrative to people abroad and has the potential to attract foreign investors, companies, and talent to Hong Kong. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fairly valid assumption based upon the article text", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response may be correct but is almost entirely speculation or outside information, as the article doesn't directly address a specific public response", "ebr_raw": "40", "ebr_num": 40, "workerid": "2"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention anything about how the public is responding to the new regulations.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response looks good but \"Based on information presented in the article, the public seems to be responding with enthusiasm.\" in the response is unsupported by the document.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The public is responding positively to the new cryptocurrency regulations. Companies and investors are looking to set up offices in Hong Kong to take advantage of the new regulations. Coinbase CEO Brian Armstrong and Gemini co-founder Cameron Winklevoss have both expressed their support for the new regulations on social media.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There isn't much in the article that talks about how the public feels about it. And Brian Armstrong talked about the US losing it's place as a financial hub.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 21-24 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7, 21-23 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The article doesn't really directly address the question, and the response contains some speculation, inference, and/or outside information", "ebr_raw": "30", "ebr_num": 30, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "The public response to the new cryptocurrency regulations in Hong Kong has been largely positive. Many cryptocurrency firms based in the United States have expressed their support for the regulations, with Coinbase CEO Brian Armstrong and Gemini co-founder Cameron Winklevoss both expressing their belief that the regulations will lead to a new wave of capital, talent, and companies setting up offices in Hong Kong. Additionally, the SFC's consultation paper has been met with enthusiasm from virtual-asset services providers such as Tykhe Capital Group", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document confirms that majority are in support of the changes made, as they believe it will help to curb loses from investments", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is not missing anything from the document but appears to use some information from outside of it", "ebr_raw": "80", "ebr_num": 80, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer appears to be accurate, but it is not taken from the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There isn't much in the article that talks about how the public feels about it. And Brian Armstrong talked about the US losing it's place as a financial hub.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "I believe the question is looking for information on how regular people are responding which is not addressed in the document. The answer provides info about companies and industry insiders which do not fit within the scope of the question. Had the question been how are companies responding, then the the answer would be appropriate.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}}}, "1": {"question_id": "0", "question": "What does it mean that retail investors in Hong Kong will be able to buy big-cap watchdog?", "system": {"davinci": {"answer": "The Hong Kong Securities and Futures Commission (SFC) said on Monday that retail investors will be allowed to buy big-cap watchdog.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The Hong Kong Securities and Futures Commission (SFC) will allow retail investors to trade in large-cap cryptocurrencies and provide protections for investors.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It means that they will be able to buy large - cap virtual assets. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response just reinstated the question without answering it. It missed the information in sentence 2.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer repeats info without telling what it means, therefore not answering the question. The info needed to answer the question comes from sentences 2 and 3 of the document, which notes that retail investors can be offered tokens which are included in at least two acceptable indicies issued by at least two independent index providers.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "This statement is likely an error in the text. The correct interpretation should be that Hong Kong will allow retail investors to buy big-cap cryptocurrencies through licensed platforms, as per the new regulatory regime coming into effect on June 1. The term \"watchdog\" refers to Hong Kong's Securities and Futures Commission (SFC), which is the regulatory body overseeing these new rules.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly sums up the situation, drawing on sentences 1 and 2 of the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "the machine response provides information in the article. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}}}, "human_1": {"answer": "These investors will be allowed to purchase a certain set of large-cap cryptocurrency tokens approved by the SFC.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentences 2 and 3.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct and contained in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "Retail investors in Hong Kong will now be able to participate in the cryptocurrency market. The government has put regulations into place, however, to protect investors by only showing them available investments within their financial means, determining what people can trade, and in what cryptocurrency people can invest. This will also make unlicensed cryptocurrency trading a federal offense in Hong Kong, which authorities hope will encourage people to trade domestically and to keep investors from being victims of overseas trading scams. The development is a part of Hong Kong's goal to emerge as a \"major virtual-asset hub\". ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence is supported by sentences 1 and 18. The second answer sentence goes into further explain the watchdog/large cap aspect of the question by noting the restrictions mentioned throughout the document. The final two answer sentences go beyond the scope of the question by addressing other types of cryptocurrency (that which is not big-cap watchdog) and the implications of Hong Kong allowing the purchase of large cap watchdog cryptocurrency.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Only the first sentence of the response is relevant to the question. The rest is redundant and extra to the question.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Major parts of the article confirms the machine response", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " Retail investors in Hong Kong will be able to buy cryptocurrency tokens that are included in at least two acceptable indices issued by at least two independent index providers. The Hong Kong Securities and Futures Commission (SFC) has proposed a cryptocurrency regulatory regime that will come into effect on June 1 this year, which will allow exchanges to offer these tokens to retail investors.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "'Securities and Futures Commission' in the response is not stated in the document anywhere. This makes the response unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-3 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is correct except that the full name 'The Hong Kong Securities and Futures Commission ' is  is not stated in the document anywhere. This makes the response unsupported.", "ebr_raw": "Score: 90", "ebr_num": 90, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "It means that retail investors in Hong Kong will be able to buy large-cap virtual assets through licensed cryptocurrency platforms. The Securities and Futures Commission (SFC) of Hong Kong has proposed a new regulatory regime that will come into effect on June 1 this year, allowing retail investors to buy eligible large-cap virtual assets. These platforms will need to follow certain criteria set by the SFC, such as conducting knowledge assessments on investors before serving them, and setting exposure limits for customers depending on their financial", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first answer sentence makes use of document sentence 2 and is the only answer sentence that it relevant. It answers the question asked. The second answer sentence answers how this will be possible and sentence 3 talks about restrictions which are beyond the scope of the question. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears to be correct and accurate and is taken from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "2": {"question_id": "1", "question": "How will the cryptocurrency regulatory regime regulate the sales?", "system": {"davinci": {"answer": "The cryptocurrency regulatory regime will regulate the sales of the cryptocurrency tokens.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The above sentences answers the question. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response doesn't answer the question correctly. It missed the information in sentences 10,11,12,13.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not provide information which speaks to how the regime will regulate sales. The info needed comes from 15 and 16, which notes licensing requirements for crypto currency activity and 10-13 which note the requirements to be compliant with that license.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The cryptocurrency regulatory regime will regulate the sales by allowing platforms to only offer \"eligible large-cap virtual assets\" to retail investors. These eligible assets must be included in at least two acceptable indices issued by at least two independent index providers. The platforms will also need to conduct knowledge assessments on investors, set exposure limits based on their financial situations, and establish token admission and review committees to ensure compliance with the SFC's admission criteria.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer correctly makes use of 2,3,12, and 13 to list off the requirements around regulation. Info from 11 is missing however. Therein, info is provided about additional considerations platforms need to take involving the people behind the tokens being sold to investors.", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2, 3 and 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The SFC will allow purchases of tokens included in at least two acceptable indices issued by at least two independent index providers. In addition, they will also set up token admission and review committees. The mechanisms by which they regulate the sales specifically are not mentioned, but it can be inferred that prohibitions on investment are enforced through legal channels.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and comprehensive answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The 1st answer sentence is supported by doc. sentence 3. The rest of the answer is not quite correct. Sentence 10 notes that the regime will have exchanges determine which tokens are tradeable and how much the investors can invest. Sentence 11 notes the requirements for listing, sentence 12 notes what the platforms will need to consider in their customer determinations. Sentence 13 also notes that the companies will need to set up the referenced committees talked about in the answer.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is partially correct, partially incorrect, and misses some relevant info", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}}}, "human_2": {"answer": "The regulatory regime will consider a number of factors to regular cryptocurrency trade. First, investors will only be shown trade deals they can afford, and the regime will determine what people can trade and in what they can invest. The regime will also make unlicensed crypto trade a criminal offense in Hong Kong, in hopes that it prevents people from being vicitms to overseas or black market crypto trade fraud. The regime will also consider factors such as a virtual asset's management team, its maturity and liquidity, and the security infrastructure around the trade. At the final stages of any deal, a review committee will make the final decision of whether to allow, halt, suspend, or withdraw a trade offer. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer summarizes info from sentences 3, 10-13, 16 to form a its answer highlighting all the points around regulation in the document except for that which is listed in sentence 21, which requires platforms to also follow local laws and regulations in jurisdictions outside Hong Kong as well.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The phrase in the response \"black market crypto trade fraud\"  is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-13 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant facts from the article are included in this answer, along with some apparent outside information (or assumptions)", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}}}, "text-davinci-003_few_shot": {"answer": "The platforms will need to consider a range of general factors, such as a virtual asset\u2019s management team, its maturity and liquidity, and the security infrastructure of its blockchain protocol, before offering these tokens to investors. They will also need to conduct knowledge assessments on investors before serving them, and set exposure limits for customers depending on their financial situations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some additional detail could have helped.", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the other regulations as per sentences 10 and 13.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-13 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the step as per sentence 13.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The cryptocurrency regulatory regime will regulate the sales by requiring platforms to consider a range of general factors, such as a virtual asset\u2019s management team, its maturity and liquidity, and the security infrastructure of its blockchain protocol, before offering these tokens to investors. Platforms will also need to conduct knowledge assessments on investors before serving them, and set exposure limits for customers depending on their financial situations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "Some more detail could have been included, but was not strictly necessary", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-14 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Sentences 11 and 12 are correctly used in the answer. The answer is missing info from sentence which lists the necessity of token admission and review committees. Further, unlicensed trading was made a criminal offense in sentence 16. Also, the requirement to follow local laws and regulations was also noted in sentence 21.", "ebr_raw": "30", "ebr_num": 30, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "3": {"question_id": "1", "question": "What kinds of new opportunities has this created?", "system": {"davinci": {"answer": "The answer is:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It ahs created new opportunities for new talents and companies will also set up offices in Hong Kong", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The regulations will not be in effect until June of that year. All talk of opportunities is, so far, speculative.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not contain any info. info needed to answer the question comes in document sentence 2, which lists that retail investors could now buy cryptocurrency and platforms could be licensed to conduct that.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is useless, and misses all relevant info in the article.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The new opportunities created by the upcoming changes in Hong Kong's cryptocurrency regulations include allowing retail investors to trade big-cap cryptocurrencies on licensed platforms and offering selected large-cap virtual assets to these investors. This could potentially attract more capital, talent, and companies to set up offices in Hong Kong and capitalize on the increased access to crypto investment and the enhanced regulatory environment for virtual asset service providers.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "The opportunities are not explicitly stated, but the newly-regulated cryptocurrency investment environment will presumably be attractive to citizens and businesses that wish to deal in cryptocurrency.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article in general talks about how the new law will help to bring in more talents to the company, and more companies will set up office in Hong Kong", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Relevant information is taken from the article, though the question cannot be fully answered from this text alone", "ebr_raw": "75", "ebr_num": 75, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response misses some of the info in the article that may be relevant to the question", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "is not stated in the document anywhere. This makes the response incorrect and unsupported. The response didn't mention cryptocurrencies\u2019 \u201c next bull run is going to start in the East \u201d, as per sentence 23.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "The expansion of cryptocurrency has created new opportunities for investors nationwide. Investors will now have more options of what they can invest in domestically, which eliminates the need to look overseas for trade opportunities. This also puts Hong Kong in a position to become a \"financial hub\" for cryptocurrency, beating out competitors like the United States. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid, if informal, answer", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2,3,10 and 14 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and overall a good summary sourced from the information in the document", "ebr_raw": "95", "ebr_num": 95, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The Hong Kong government has committed to creating a cryptocurrency regulatory regime that will allow retail investors to buy big-cap virtual assets. This has created opportunities for exchanges to offer virtual assets to investors, as well as for companies to set up offices in Hong Kong to capitalize on these opportunities.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The above sentences emphasis that the regulation will help make cryptocurrency transactions safer, more businesses will be able to open up", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears to be correct and accurate and is taken from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "The new opportunities created by the Hong Kong government's commitment to becoming a leading Web3 hub include the ability for retail investors to buy large-cap virtual assets, the introduction of a cryptocurrency regulatory regime, and the ability for exchanges to determine what cryptocurrency tokens retail investors can trade and how much they can invest. Additionally, platforms will need to conduct knowledge assessments on investors before serving them, and set exposure limits for customers depending on their financial situations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-5, 9-10 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is generally correct and relevant and uses info from the article", "ebr_raw": "95", "ebr_num": 95, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The part about regulatory rules is not necessary when answering a question about opportunities.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first two lines of the answer are addressing what led to the opportunities being created and is therefore not relevant as it is beyond the scope of the question. The final answer sentence is also not relevant as it is discussing requirements rather than opportunities. The info in between from sentence 1, 2, and 10 serves to answer the question appropriately. ", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "6"}}}}}, "4": {"question_id": "2", "question": "How did the meltdowns affect cryptocurrency?", "system": {"davinci": {"answer": "The meltdowns in the cryptocurrency market in 2018 were caused by the following reasons:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 8 states how the meltdowns affect cryptocurrency.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The meltdowns caused everyone invested with cryptocurrencies to lose a lot of money. Their worth plummeted.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not provide any info from the document and is therefore not relevant. 8 and 9 contain relevant info however, with 8 noting substantial losses and 9 indicating regulation coming into play.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is incomplete and useless, while the article doesn't specifically address the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The meltdowns in the cryptocurrency market last year, such as the one at FTX, resulted in substantial losses for tens of millions of investors. These collapses highlighted exposure risks arising from the increasing interconnectedness between the virtual-asset market and the traditional financial system, underscoring the need for effective regulation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in sentence 8.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}}}, "human_1": {"answer": "No further information is given elsewhere in the article, but it is stated that the meltdowns led to substantial losses to tens of millions of investors.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "All of the relevant information from the article is included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response is somewhat correct, but misses some relevant info", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response missed a lot of information about the effects on cryptocurrency as per sentences 9,10,11,12,13,14,15,16. ", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}}}, "human_2": {"answer": "The article states that the meltdowns caused tens of millions of investors to lose their money. The meltdowns also revealed how deeply interetwined the virtual cryptocurrency world had become with the \"traditional\" financial system. The crash is part of what prompted Hong Kong to reevaluate its regulations on cryptocurrency. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is good and relevant and contains info taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it failed to mention the need for effective regulation as per sentence 9.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The meltdowns resulted in substantial losses to tens of millions of investors and exposed risks arising from the increasing interconnectedness between the virtual-asset market and the traditional financial system.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Many investors lost their money according to sentence 8", "ebr_raw": "20", "ebr_num": 20, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is good and relevant, given the information contained in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "The meltdowns caused substantial losses to millions of investors and exposed them to risks arising from the increasing interconnectedness between the virtual asset market and the traditional financial system.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is pretty good and mostly complete but leaves out one extra piece of information in the document that seems relevant", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer addresses how the meltdowns affected investors but not cryptocurrency itself. That info is contained in sentence 9 which should have served as the basis for the answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "5": {"question_id": "2", "question": "How will the cryptocurrency sales affect the government's commitment toward being the leading Web3 hub?", "system": {"davinci": {"answer": "The government's commitment toward being the leading Web3 hub is to attract more blockchain companies to set up their offices in Hong Kong.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "Score: 0. The response did not attempt to answer the question and did not include any relevant information from the article.", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The cryptocurrency sale will improve the economy when companies come to invest and set up office in Hong Kong ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information in sentence 7.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer addressing the goal of being a leading Web3 hub. The question wants to know how will the sales stemming from the new policy affect the government's commitment to being that hub. Sentence 5 indicates that they will emphasize it.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The cryptocurrency sales will allow retail investors to access big-cap licenses through a regulatory regime, demonstrating the Hong Kong government's commitment to creating a robust and regulated environment for virtual assets. This move is expected to attract more capital, talent, and companies to set up offices in Hong Kong, helping position the city as a leading Web3 hub.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-7 matches the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The first sentence makes use of 1, 2, 5, 14, 15, summarizing them to help answer the question. The second sentence is supported by information in 5 and 7. ", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response provided by the machine is found in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The cryptocurrency sales improve Hong Kong's outlook as a Web3 hub, with commentators saying that Hong Kong may surpass the US as a financial hub and potentially kick off a new cryptocurrency bull run.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer fails to note how the info it mentions will affect the government commitment to being the leading Web3 hub. The info needed to answer the question comes from sentence 5, which notes that the commitment is being underlined (i.e. strengthened) So it would be proper to say that such sales strengthen the commitment. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "There will also be a new wave of capital, talent and it will also make more companies to set up their offices in Hong Kong", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and uses information from the document", "ebr_raw": "95", "ebr_num": 95, "workerid": "2"}}}, "human_2": {"answer": "Hong Kong had taken a somewhat \"conservative\" approach to cryptocurrency trading since 2018. However, these new regulations open the doors for more people, especially those who did not meet investment guidelines in the past, to be able to invest in crypto. The decision reflects the government's decision to being the leading Web3 hub by creating an enticing opportunity for investors worldwide. The government hopes the decision will attract investors to Hong Kong to bring a new wave of capital, talent, and companies that are interested in participating in crypto trade/sale. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer uses sentences 17 and 7 to craft and answer that effectively answers the question. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the information in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article", "ebr_raw": "60", "ebr_num": 60, "workerid": "7"}}}, "text-davinci-003_few_shot": {"answer": "The Hong Kong government's commitment towards being the leading Web3 hub will be strengthened by the cryptocurrency sales as it will allow retail investors to buy big-cap virtual assets and will also introduce a new virtual-asset platform licensing regime. This will help to regulate the virtual-asset market and reduce exposure risks arising from the \u201cincreasing interconnectedness\u201d between the virtual-asset market and the traditional financial system.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed the other government's commitments toward being the leading Web3 hub, as per sentences 14, 15 and 16.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article mentions that the sales of cryptocurrency will help reduce losses especially with the new regulations and Hong Kong will be able to emerge as a virtual-asset hub", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it failed to mention the factors as per sentences 23 and 24.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The cryptocurrency sales will help the Hong Kong government to fulfill its commitment towards being the leading Web3 hub. The new virtual-asset platform licensing regime, which will come into effect on June 1 this year, will allow retail investors to buy eligible large-cap virtual assets, which will help to increase the liquidity of the virtual-asset market and attract more investors to the city. This will help to further develop Hong Kong's status as a Web3 hub.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5-8 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The question is answered within sentence 5, where it is noted that this process underlines the government's commitment. This is synonymous with what was written in answer sentence 1. Answer sentence 2 speaks to the effects and is therefore not relevant. Sentence 3 is simply repetitive and is therefore not relevant.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and about as good as possible given the information in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "6": {"question_id": "3", "question": "How does the SFC license allow people to manage a portfolio?", "system": {"davinci": {"answer": "The SFC license allows people to manage a portfolio of virtual assets.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is another lazy answer, but for once it (coincidentally) matches the information in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The regulation in place will provide some security for the investor and crack down on those trading without a license.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 10 provides accurate answer to the question. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response doesn't address the question properly. It missed the information in sentences 7,10,11,12,13.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer indicates that people can manage a portfolio of digital assets, but the question wants to know how that is done under the license. That question is not answerable, as the document focuses on retail investment, not portfolio management.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "An SFC license allows a company to manage a portfolio by granting it the necessary regulatory approval to operate and provide financial services related to virtual assets in Hong Kong, in compliance with the applicable laws and regulations. This enables the licensed firm to manage investments, trade and offer financial products related to cryptocurrencies and other virtual assets on behalf of clients, while ensuring consumer protection and adherence to anti-money laundering and counter-terrorist financing rules.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Every information in the machine response is supported by the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not provide the info needed to answer the question. The focus of the document is on retail trade, not portfolio management. The info in the answer is not drawn from the document as there are no sentences which speak specifically to this issue.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is provided in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated how the SFC license allows people to manage a portfolio, but since SFC is involved in regulating cryptocurrency, their approval seems to be required to manage these kinds of investments.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question asks for outside information, but the answer makes a valid attempt at pointing toward the relevant passages nonetheless", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The portion of the answer which notes that the info is not available is correct. The second portion of the answer, which speculates on required approval, is not relevant as this information does not add any new knowledge or information. The presence of a license requirement indicates that approval is needed. That is obvious to an observer and need not be stated.  ", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The above sentences states how the regulation will help people manage portfolio, there will be close monitoring of every activity", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Though I am not quite sure of what specifically the question is asking, the response seems to miss some potentially relevant information", "ebr_raw": "60", "ebr_num": 60, "workerid": "2"}}}, "human_2": {"answer": "It is not explicitly stated but can be inferred that the SFC license allows people to particpate in cryptocurrency trade under the regime's new regulations. The SFC plans to ban all unlicensed cryptocurrency trade, so investors must get a license to be able to particpate. By having this license, people of all economic backgrounds and at all investment levels will be able to participate. The license has opened up cryptocurrency trade to a broader scope of participants. ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence is correct in identifying that the info is not given. Its inference is very generic and can be supported by sentence 2 of the document. The info which follows in the answer however speaks to who is allowed to trade and does not deal with what the question is looking for, which is knowing how the portfolio run by Pioneer is maanged.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The sentence 'By having this license, people of all economic backgrounds and at all investment levels will be able to participate. ' in the response is unsupported. The response doesn't mention how the SFC license allows people to manage a portfolio, which is stated in sentence 7 and 21.", "ebr_raw": "30", "ebr_num": 30, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11-16 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": "The SFC license allows people to manage a portfolio of 100% virtual assets. Platforms will need to consider a range of general factors, such as a virtual asset\u2019s management team, its maturity and liquidity, and the security infrastructure of its blockchain protocol, before offering these tokens to investors. Platforms will also need to conduct knowledge assessments on investors before serving them, and set exposure limits for customers depending on their financial situations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed other steps on how the SFC license allows people to manage a portfolio, as per sentences 7 and 13.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10-13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response also missed the setting up of offices by companies to manage the assets, as per sentence 7.", "ebr_raw": "Score: 80. The response provided some relevant information from the article to answer the question, but missed the setting up of offices by companies to manage the assets (sentence 7).", "ebr_num": 80, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The SFC license allows people to manage a portfolio of virtual assets by setting up token admission and review committees, which are responsible for following through with the SFC\u2019s admission criteria and for making final decisions on whether to admit, halt, suspend or withdraw offerings. Platforms will also need to conduct knowledge assessments on investors before serving them, and set exposure limits for customers depending on their financial situations.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11-13 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "Sentence 10 is needed as it gives an overview and sentences 11-13 and 21 give the needed details.then sentences ", "ebr_raw": "10", "ebr_num": 10, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Based on the document, the answer seems correct and relevant", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}}}, "7": {"question_id": "3", "question": "Why have the virtual asset market and the traditional financial system become so interconnected?", "system": {"davinci": {"answer": "The answer is simple. The virtual asset market is a new financial market. The traditional financial system is the old financial market. The virtual asset market is a new financial market. The traditional financial system is the old financial market. The virtual asset market is a new financial market. The traditional financial system is the old financial market. The virtual asset market is a new financial market. The traditional financial system is the old financial market. The virtual asset market is a new financial market. The traditional financial system is", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This answer is a worthless mess, but the article does not really address the \"why\" question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the situation described in the article, the virtual asset market and the traditional financial system has become so interconnected because people are now engaging in cryptocurrency trades and using this form as a means of payment. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain how the virtual asset market and the traditional financial system became so interconnected.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not provide information that addresses the question. The info needed to answer the question is not contained with the document, making the question not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not helpful or relevant. The article does not directly answer the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}}}, "gpt4": {"answer": "The virtual asset market and the traditional financial system have become increasingly interconnected due to the growing adoption of cryptocurrencies and other digital assets by various market participants, including institutions, banks, and individual investors. As cryptocurrencies have become more mainstream, they are being integrated into traditional financial products and services, such as trading platforms, investment funds, and payment systems. Additionally, regulations and policies are evolving to accommodate such assets, further deepening the relationship between the two sectors. This interconnectedness can also be attributed to the increasing investment and interest in blockchain technology by traditional financial institutions. As a result, the boundaries between virtual assets and traditional finance are becoming less distinct, leading to higher exposure risks for investors and the market in general.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This seems to be a very accurate and comprehensive answer, but it is only loosely based on the text, and is almost entirely outside knowledge and suppositions", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from a summary of the article. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response provides information that is present in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "It contains a lot of outside background knowledge, but the answer is at least consistent with the article text.", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}}}, "human_1": {"answer": "It is not stated exactly how the virtual asset market and traditional financial system have become interconnected.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article says it ahs become interconnected because many people invest in the virtual asset market, and they also use it as a means of payment and exchange", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information, which the respondent acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate as the article doesn't really discuss the answer to this in any further detail", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "It can be inferred from the article that the virtual asset market and the traditional financial system have become so intertwined because people are using what we consider \"traditional\" currency in virtual spaces. The creation of cryptocurrency has created a new form of capital. People who, for years, chose to invest in stocks and bonds are now turning to \"digital\" forms of currency. ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is entirely speculation, and is not taken from any specific passage in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer may be correct but is largely speculation rather than info taken directly from the article.", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response looks good, but is not stated in the document anywhere. This makes the response unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The SFC does not provide an answer to this question, but it is likely due to the increasing popularity of cryptocurrencies and the need for regulation of the virtual asset market.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question called for outside information, which the answer clearly acknowledges, along with a reasonable assumption", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document states that people are diving more into virtual assets and making payments easier", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "It had become so interconnected because many investors were investing in virtual assets and using it as an easier means of traditional payment system according to the document", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is likely true but contains speculation, as the document doesn't directly address the question", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}}}, "text-davinci-003_zero_shot": {"answer": "The virtual asset market and the traditional financial system have become increasingly interconnected due to the rise of digital assets and the increasing acceptance of cryptocurrencies as a legitimate form of payment. As more people become aware of the potential of digital assets, more investors are looking to invest in them, which has led to a greater level of integration between the two markets. Additionally, as more businesses and financial institutions begin to accept cryptocurrencies as a form of payment, the two markets become more intertwined.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 summarizes the machine response, and also people are now more familiar with technology advances. hence the more digital way of transacting business.    ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and probably correct but includes some speculation and/or outside information not explicitly stated in the document", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer includes some outside information, but it fits with the overall theme of the article", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated within the document. None of the info in the answer is supported by any of the document's sentences.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}}}, "8": {"question_id": "4", "question": "How could a new wave of talent, capital, and companies affect Hong Kong's economy?", "system": {"davinci": {"answer": "Hong Kong's economy is a service-based economy. The service sector is the largest sector in Hong Kong's economy, accounting for more than 70% of Hong Kong's GDP. The service sector is also the largest employer in Hong Kong, accounting for more than 80% of the workforce.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "missing_all", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the new regulation, the economy will grow due to the investments.  ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported. It missed the information in sentences 14,15,16,23,24.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is drawn from outside the document. The document does not address the overall economy of Hong Kong.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "A new wave of talent, capital, and companies setting up offices in Hong Kong to capitalize on opportunities in the virtual-asset sector could have several positive effects on Hong Kong's economy. These may include increased investments, job creation, enhanced innovation and technology development, and growth in the overall financial sector. This could potentially increase tax revenues, boost GDP, strengthen Hong Kong's international reputation as a leading financial hub, and contribute to the overall economic growth and prosperity of the city.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is all reasonable supposition, but it is not based on the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The overall information provided in the article proves that the machine response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "missing_all", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is based on info not drawn from the document, as the document does not expand on how all of this would affect the economy.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Due to the current economy melt down in Hong Kong, new companies establishment will help to reduce the prices on importation according to the aticle. ", "ebr_raw": "0. The response is not related to the content of the article and does not answer any question related to it.", "ebr_num": 0, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated, but new talent, capital, and companies will generally benefit a country's economy.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable supposition, but the question was asking for information not given in the article", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct in saying the needed info is not stated in the document. The speculation included in the answer is not supported by the document however. There is nothing to indicate the effect on the country's economy within the document, meaning that this would be coming from outside information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "I believe the new wave will generally bring in more businesses which will generate more revenue for the country", "ebr_raw": "20", "ebr_num": 20, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Though it doesn't perfectly answer the question, there may be some relevant info that the response is missing", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that that info is not given. The statement is made in sentence 7, but there is no follow up which would provide further information.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response failed to mention the effect on the economy as per sentence 23.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "There will be an improvement in the economy when people's trading and investment activities can be protected and regulated.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "At least a partial answer could have been taken from the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}}}, "text-davinci-003_few_shot": {"answer": "The influx of talent, capital, and companies could help to boost Hong Kong's economy by creating new jobs, increasing investment, and providing new services and products. It could also help to attract more foreign investment and help to make Hong Kong a global financial hub.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is reasonably answered with the available information", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Only the last sentence in the response is supported, the other sentences are unsupported. It also missed the possibility as per sentence 24. ", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "With the exception of the wording from the part 7, the article doesn't mention anything that is used in the answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article states that people will have more interest in investing once they are confident that their investment will be safe and secure", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence in the response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_zero_shot": {"answer": "The influx of talent, capital, and companies into Hong Kong could have a positive effect on the economy. It could create new jobs, increase investment, and stimulate economic growth. It could also lead to the development of new technologies and services, as well as the expansion of existing businesses. This could lead to increased innovation and productivity, which could further boost the economy. Additionally, the presence of these companies could attract more foreign investment, which could further stimulate economic growth.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "With the exception of the wording from the part 7, the article doesn't mention anything that is used in the answer.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "While loosely based on the article, this answer is almost entirely speculation (albeit reasonable speculation)", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although the document did not provide full details but sentence 7 points out that talent and new companies will set up office one the new regulation is implemented, and with this there will be an improvement in the economy of Hong Kong.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document as the information contained in the answer is not present in the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "While the response may be correct, it contains speculation and information not included in the article", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}}}}}}}, "26": {"dataset": "inq-ext", "article_num": 18, "question_details": {"0": {"question_id": "0", "question": "How long had the survivor been trapped?", "system": {"davinci": {"answer": "The survivor was trapped for about 10 hours.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not state how long the survivor had been trapped for.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This may be an accurate answer, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The earthquake happened two weeks ago so it is likely the survivor had been trapped since then.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response answer is not stated in the document anywere. This makes the response incorrect and unsupported.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The amount of time the person was trapped was not specified in the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The text does not provide information on how long the survivor had been trapped in the rubble.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this specific question would require outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not provide the information of how long the survivor was in the rubble. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The earthquake happened two weeks ago so it is likely the survivor had been trapped since then.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information about how log the survivor had been trapped. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the document does not indicate how long the person was trapped.", "ebr_raw": "80", "ebr_num": 80, "workerid": "6"}}}, "human_1": {"answer": "It is not clear exactly how long the survivor was trapped. The earthquake took place two weeks ago, so the survivor was presumably trapped for less time than that.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not stated.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable supposition, given the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not specifically state how long the survivor was trapoed.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Duration is not stated.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question required outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The duration of the trap is not given in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The earthquake happened two weeks ago so it is likely the survivor had been trapped since then.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " The essay does not specify how long the survivor had been trapped.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The text does not provide a specific answer", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told how long the survivor was trapped for.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is actually no information about the duration the survivor was trapped", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state how long the survivor was trapped", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The length of time the survivor had been trapped is not specified in the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct based on the article info", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does say that the survivor was found on February 11th, and that the earthquake was two weeks prior to the date of writing", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. The AI correctly identified that the length of time the survivor was trapped was not mentioned in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "1": {"question_id": "0", "question": "How were the survivors able to stay alive?", "system": {"davinci": {"answer": "The survivors were able to stay alive because they were trapped in a pocket of air.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "This is probably an accurate answer, but it was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not state how the survivors were able to stay alive. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Although the answer may be correct, it should be clarified that this information is not in the article.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is speculation or outside information. The question is not directly answered in the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentence 5.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide specific details on how the survivors were able to stay alive while trapped in the rubble.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no evidence about how the survivors managed to stay alive under the rubble", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly identifies that the needed information is not present in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state how the survivors stayed alive under the rubble.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated how the survivors were able to stay alive.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the information requested in via the question is not available within the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told how they survived. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not really state how the survivors were able to stay alive", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not stated. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not provide the information on how survivors were able to stay alive", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct and supported.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " It is not stated in the article how the survivors were able to stay alive. It is likely that they were able to find air pockets in the rubble or were able to access food and water.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, given the text", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The AI was correct in identifying that we are not told how the survivors survived, however, in the same breath the AI then interjects an opinion not based on stated facts.", "ebr_raw": "50", "ebr_num": 50, "workerid": "4"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The second sentence in the response is unsupported.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not provide information about how survivors were able to stay alive.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The survivors were able to stay alive by being protected from the debris and having access to air and other resources. The Hong Kong team used drones to assess the environment and used thermal imaging cameras to help detect survivors, which allowed them to locate and rescue the survivors from the rubble.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The team also had two springer spaniel rescue dogs and eight tons of equipment.", "ebr_raw": "20", "ebr_num": 20, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 15 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question is not provided by any of the sentences within the document. As far as the given answer goes, the first answer sentence is not drawn from the document, and the second answer sentence is explaining how the survivors were rescued, not how they survived.", "ebr_raw": "Score: 0.", "ebr_num": 0, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is some reasonable speculation, but this is a valid answer, given the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the response incorrect and irrelevant.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "2": {"question_id": "1", "question": "How many firefighters from other countries went to Turkey to help?", "system": {"davinci": {"answer": "According to the information from the Turkish government, the following countries sent rescue teams to Turkey:", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "59 firefighters from Hong Kong were sent on the rescue mission", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to actually answer the question using the text, and only a partial answer could have been retrieved", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not say how many firefighters from other countries were there.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not stated in the document anywhere. This makes the incomplete response incorrect and unsupported.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not detail the numbers of firefighters that came from other countries. The question is therefore not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide an exact number of firefighters from other countries that went to Turkey to help.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this question would have required outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There were 59 firefighters from Hong Kong that went to Turkey to help", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "There were 59 firefighters from Hong-Kong.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct. The document does not give the info needed to answer the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "human_1": {"answer": "Only firefighters from Hong Kong and China are mentioned in the article. It is unclear what other countries' firefighters went to Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are only told of these two groups in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question cannot be answered from the text, but relevant information is retrieved", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Only two countries are mentioned in the article, Hong and China were the only countries that was involved in the rescue mission.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response  is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information is not stated.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The only country that was involved in the rescue mission was Hong Kong, there were fire fighters from Hong Kong", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " The Hong Kong team was part of a larger international effort, with 59-strong Hong Kong team sent on a mercy mission to the ravaged country.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All of the available information is given in this answer, though the grammar could be improved", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "We are not old how many other teams were sent, but we are told that a team from Hong Kong as well as mainland China were sent. Even though Hong Kong is now part of China for the last decade, I think its important to acknowledge the separation as China operates Hong Kong a such, though hat is changing..", "ebr_raw": "40", "ebr_num": 40, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document did not mention other firefighters from other countries. Only the team from Hong Kong were on the rescue mission", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article states Hong Kong as the only country where firefighters were sent from, and there were 59 firefighters. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The Hong Kong team sent to Turkey was 59-strong, and they joined their mainland Chinese counterparts in the country. It is unclear how many firefighters from other countries went to Turkey to help.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is accurate and as good as possible based on the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. The AI identified correctly that only the Hong Kong team numbers were mentioned, which is in #3.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "3": {"question_id": "1", "question": "How many people died because of the earthquakes?", "system": {"davinci": {"answer": "The death toll from the 7.8-magnitude earthquake that struck the eastern Turkish province of Van on Sunday has risen to 5,903, the country's disaster and emergency management agency said on Monday.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer was not taken from the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "More than 46,000 people ahs been killed in the disaster. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "None of the answer is in the article. The correct answer should be: 46,000 people died in the earthquake.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be wrong and conflicts with information given in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Except for the 7.8 magnitude, all other parts in the response are unsupported . It missed the information in sentence 12.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The earthquakes in Turkey killed more than 46,000 people.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is an accurate answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is fully supported by sentence 12 and effectively answers the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The earthquake killed over 46,000 people according to the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 11.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information in #11.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_2": {"answer": "It is estimated that more than 46,000 people died because of the earthquakes. The article does not mention any official counts on how many people were injured or how many people disappeared because of the disaster. Entire cities were destroyed because of the natural disaster. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information from the article is included, although the rest of the answer was unnecessary", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information stated in #11.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Only the first sentence of the response is relevant. The rest is not relevant.", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The disaster has killed more than 46,000 people.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are told this in #12.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 correspond to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The earthquake has killed more than 46,000 people.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly uses sentence 12 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "4": {"question_id": "2", "question": "How did the team help the people in Turkey?", "system": {"davinci": {"answer": "The team helped the people in Turkey by rescuing them from the rubble.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is correct based on the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This simplistic answer actually fits with the article text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information in sentences 4,5,6,8,11,15.", "ebr_raw": "Score: 30", "ebr_num": 30, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is supported by sentence 8, which notes that people were helped from the rubble. Sentence 8 also mentions that the team recovered bodies, which also would have been a help to the people of the area.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "gpt4": {"answer": "The 59-strong Hong Kong rescue team helped the people in Turkey by rescuing survivors trapped in the rubble after an earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "It's a valid answer, but it could have been more detailed", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "It is not relevant to state the number of people on the team, as this goes beyond the scope of the question. The answer partially uses sentence 8 of the document well, but missing the fact that the team also removed bodies. That was a help to the people in Turkey and should be mentioned.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}}}, "human_1": {"answer": "The team from Hong Kong helped pull four survivors and six bodies out of the rubble in Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "While the AI correctly answered with the survivor and body counts, there was more to be discussed. They employed search dogs, drones to 3D model the site, and used thermal imaging. It can be assumed that the information gathered was given to other teams.", "ebr_raw": "60", "ebr_num": 60, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 7 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed other ways that they helped the people in Turkey, as per sentence 14.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "human_2": {"answer": "The 59-person and two-dog team from Hong Kong was deployed to Turkey to aid in relief efforts after the deadly earthquakes. The team searched for bodies under the rubble, so they could be returned to their families for mourning. The team also searched for people who were still alive to rescue them, and they provided medical services on the ground to those who were injured. The team also worked to help clear the rubble and destruction so people could begin the process of rebuilding. ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "No where does it mention that the team searched for bodies so families could mourn. Nor does it mention the team provided medical services to the injured. They also did not clear the rubble to help rebuild. They were sent into to find surviors, and deceased. They did this with 59 workers, 2 dogs, drones and thermal cameras.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "4"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The response failed to mention the two springer spaniel rescue dogs and other technologies used, as per sentence 10 and 14. The phrase \"so they could be returned to their families for mourning.\" and the last sentence in the response are unsupported by the passage.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is fully evident in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The team also had two springer spaniel rescue dogs and eight tons of equipment.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": "The Hong Kong team pulled four survivors out of the rubble \u2013 and also had the sadder task of recovering six bodies. They also deployed drones to assess the environment and used the footage to construct 3D models of the site, as well as used thermal imaging cameras to help detect survivors.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information is in #8 and #15.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The team also had two springer spaniel rescue dogs and eight tons of equipment.", "ebr_raw": "20", "ebr_num": 20, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1,8 and 15 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 8 and 15 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The Hong Kong team sent on a mercy mission to Turkey helped by rescuing four survivors from the rubble and recovering six bodies. They also deployed drones to assess the environment and used thermal imaging cameras to help detect survivors.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "How did the team help the people in Turkey?", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and correct based on the document information", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information is found in #3, #8, and #15", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct but failed to mention the two springer spaniel rescue dogs and eight tonnes of equipment, and the team used the drone footage to construct 3 D models of the site, as per sentences 11 and 15.  ", "ebr_raw": "75", "ebr_num": 75, "workerid": "1"}}}}}, "5": {"question_id": "2", "question": "How were efforts professional?", "system": {"davinci": {"answer": "The sentence is a summary of the article.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was even made to answer the question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 14 and 15 states how efforts were professional, it states that some types of technology were used.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They used and shared technology, strategic planning, operational techniques to help find victims. The team also had two springer spaniel rescue dogs and eight tons of equipment.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is not helpful. The question is vague and not specifically addressed in the document.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information in sentences 11,15,17.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The efforts of the city squad were considered professional because they were part of a 59-strong Hong Kong rescue team that went on a mercy mission to Turkey after a 7.8-magnitude earthquake. The team used technology like drones, thermal imaging cameras, and 3D modeling to locate and rescue four survivors from the rubble and recover six bodies. The Hong Kong team also collaborated with mainland Chinese counterparts and learned from their strategic planning, operational techniques, and resourcefulness, thus enhancing their performance, capacity, and efficiency.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a sufficient answer, based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is backed up by evidences in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first sentence serves as a summary of the mission conducted but does not speak to what exactly makes that a professional endeavor. The sentences which follow do answer the question in that they provide examples of activities and actions taken that are generally considered professional things to do, thereby showing how the efforts were professional. Additional examples are given in sentence 11.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 14 and 15 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated what made the efforts professional. However, they brought dogs and equipment, and used drones to produce models of the site, all of which indicate a sophisticated operation.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in that the document fails to state specifically what made the efforts professional. That being said, there are allusions made in sentences 10 and 14 which the answer picks up on.", "ebr_raw": "75", "ebr_num": 75, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information is included", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not stated. Would have to be inferred, which the AI stated some facts to be inferred.", "ebr_raw": "70", "ebr_num": 70, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "sentence 14 and 15 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_2": {"answer": "The team deployed formal rescue efforts, including assembling a team and using professional equipment. The team from Hong Kong also collaborated with other international teams on the scene, including one from China, and used it as an opportunity to learn about how to conduct rescue and relief missions in other ways. The team was also able to provide professional medical service, including health checks on the scene for those who were found buried under the rubble. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer fits with the overall theme of the article, though it includes a lot of outside information (or speculation)", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The AI took a lot of liberty in make some of these assumptions. The text does not state how they were professional. The mention of a collaboration is probably the only assumption that could be plausible to answer this.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 and 14 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct as it missed other information about the professionalism, as per sentences 10,14 and 16.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The Hong Kong team pulled four survivors out of the rubble \u2013 and also had the sadder task of recovering six bodies. They deployed drones to assess the environment and used the footage to construct 3D models of the site, as well as used thermal imaging cameras to help detect survivors. They also had a clinical psychologist brief team members on possible stress responses before the journey in a bid to help them cope with anxiety caused by a demanding mission in a dangerous and unfamiliar environment.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The team also had two springer spaniel rescue dogs and eight tons of equipment.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "We are not told how they acted in a professional manner. The AI took bits and pieces from the article to form an answer, or rather more of an assumption.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it missed the rescue dogs and eight tonnes of equipment, as per sentence 11.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 9-16 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The Hong Kong team pulled four survivors out of the rubble and also had the sadder task of recovering six bodies. They also deployed drones to assess the environment and used the footage to construct 3D models of the site, as well as used thermal imaging cameras to help detect survivors. The team also had a clinical psychologist to brief them on possible stress responses before the journey in a bid to help them cope with anxiety caused by a demanding mission in a dangerous and unfamiliar environment. These efforts demonstrate the professionalism of", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11-17 corresponds to the machine response. ", "ebr_raw": "80", "ebr_num": 80, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The examples provided by the answer do indeed show professionalism. Sentence 3 and 6 of the document do as well in demonstrating the care taken in the rescues. Sentence 11 lists the equipment which is also quite professional.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer was a bit cut off, but otherwise is sufficient to answer a vague question", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The last sentence of the response is incomplete and the phrase \"the team\" should be added at the end, as per sentence 15.", "ebr_raw": "80", "ebr_num": 80, "workerid": "1"}}}}}, "6": {"question_id": "3", "question": "How badly was the survivor injured?", "system": {"davinci": {"answer": "The survivor was not injured at all.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The survivor had few scratches on their head. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "That answer is almost technically correct, but it should have cited the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The survivor had stable life signs with only scratches on his head.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is wrong according to sentence 5.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document states that the person received scratches on their head.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The survivor's injuries were not severe, as they were reported to be breathing with stable vital life signs and only had scratches on their head.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentences 4 and 5.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_1": {"answer": "The survivor was not severely injured: it was reported that the person was breathing with stable vital life signs and only scratches on their head.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Found in #4.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be confirmed in sentence 4", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "The person who was buried under the rubble survived with minimal injuries. After being checked by the team, they found the survivor had stable vitals and no life-threatening injuries. The survivor did have some scrapes and cuts on their head, but the injuries were minimal, and the team believes the survivor will recover with no issues. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The text does not mention any information more than the survivor was breathing, had stable vitals and some scratches. The AI embellished some of the facts. Not once does it state that anyone expected the survivor to recover with no issues.", "ebr_raw": "40", "ebr_num": 40, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The phrase \"but the injuries were minimal, and the team believes the survivor will recover with no issues.\" in the response is unsupported so the response is partially incorrect.", "ebr_raw": "70", "ebr_num": 70, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": "The survivor had only scratches on their head and was breathing with stable vital life signs.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Found in #5.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The survivor was not badly injured, as they had only few scratches according to sentence 5", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The survivor had only scratches on their head and was reported to be breathing with stable vital life signs, so their injuries were not severe.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is clear, concise and accurate, and based on the document info.", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Found in #5.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "7": {"question_id": "3", "question": "How did the team feel about the mission?", "system": {"davinci": {"answer": "The team members were very happy to be able to help the victims of the earthquake. They were also very proud to be able to help the people of Turkey.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The response appears to be speculation. The question isn't addressed much, but some relevant info may be present.", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but is unsupported. It missed the information in sentences 14,15,16.", "ebr_raw": "40.", "ebr_num": 40, "workerid": "1"}}}, "gpt4": {"answer": "Members of the Hong Kong team felt that they gained precious experience and insight from the mission, and their performance, capacity, and efficiency were tested. They also learned about the use of technology, strategic planning, and operational techniques from their mainland counterparts.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 14 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 14. The info contained therein effectively answers the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 14 and 15 corresponds with the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The team's feelings are not stated, although they were professional, shared their experiences, and learned a lot from their mainland Chinese counterparts.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The feelings of the team are stated in sentence 13, where a team commander quote is given. Sentences 14 and 15 back up what was written in the answer about learning a lot from their Mainland Chinese counter parts. The parts of the answer saying they were professional and shared their experiences is not relevant as that the professional comment was made by Lee, not the team and sharing in this context is an action, not a way of feeling.", "ebr_raw": "25", "ebr_num": 25, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Relevant information is taken from the article in a valid attempt to answer a vague question", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Feelings of the team's experiences were not stated. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 13 summarizes the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_2": {"answer": "It is not explicitly stated, but can be inferred through the article that the team saw the experience as a learning opportunity. They were able to collaborate with other rescue teams, as well as try out new tactics and equipment. However, the mission also took a mental toll on the team. They were able to meet with psychologists to discuss possible PTSD and anxiety responses from having to witness such destruction and sadness in Turkey. The job was \"demanding and unfamilar\", but the team members were also able to learn a lot. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable supposition, given the article text", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The AI correctly identified that this was not stated. I agree with its assumptions though.Every fact was taken from the text.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The last few sentences of the article confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\"possible PTSD and anxiety responses \" in the response is unsupported as per sentence 16, which just says \"possible stress responses\". ", "ebr_raw": "90", "ebr_num": 90, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The team felt that the mission was treacherous, but they also gained a lot of experience and insight from the operation. They were also given a hero's welcome when they returned to Hong Kong.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "We are not told how the team felt. Only the Lee mentioned it was treacherous, though that was an assumption on his part as he was not there. Gaining of experience is not a feeling.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct as it doesn't say the team felt the mission was demanding as per sentence 17.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although the rescue mission was stressful but the team were happy they were able to gain a lot from the experience and they were well celebrated for a job well-done.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The team felt that the mission was demanding and stressful, but they were able to gain valuable experience and insight from it. They also felt grateful for the support they received from the mainland Chinese counterparts.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 10 and 14 affirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "For the 1st answer sentence, it was never said that the team actually ended up feeling it was stressful and demanding, but sentence 17 indicates that they were prepared for it to be. Sentence 2 of the answer includes some info from sentence 14, but some is missing in regards to the mention of learning a lot. The final answer sentence talks about gratitude for their counterparts, but that is beyond the scope of the question.", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The word \"stressful\" is unsupported. As per sentence 17, the mission is dangerous.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}}}, "8": {"question_id": "4", "question": "How is the survivor recovering?", "system": {"davinci": {"answer": "The survivor is recovering well.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Even though this respondent is probably fabricating answers, this one is a reasonable assumption to make, based on the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention how the survivor is recovering.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentence 5.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not speak to how the rescued person is recovering. The question is therefore not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The survivor was found to be breathing with stable vital life signs and only had scratches on their head.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is enough information to make a reasonable assumption about the answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article doesn't mention how the survivor is recovering.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer describes the condition the survivor was discovered to be in rather than talking about their recovery. The document does not follow up on what happened to the person after their rescue, so the question is not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "human_1": {"answer": "It is mentioned that the survivor only has minor injuries. It can be inferred that they are probably recovering from those injuries.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "While correct. The AI should have included that the information was not stated in it's answer to the question. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information from the text is given, along with very reasonable assumptions to answer the question", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 4 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears correct but is not stated in the document anywhere. This makes the response unsupported. The correct answer is stated in sentence 4.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "A reasonable assumption could have been made based on the article's information", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not stated.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "Sentence 4 states how the survivor is recovering", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": "The survivor was reported to be breathing with stable vital life signs and only scratches on their head.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "We are not told anything further about the survivor or their recover. They AI did not correctly identify this.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 corresponds to the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "At this time, it is not known how the survivor is recovering. The acting senior firefighter reported that the survivor was breathing with stable vital life signs and only had scratches on their head. However, it is not known if the survivor has been released from the hospital or if they are still receiving medical care.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and seems correct given the info in the document ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information from the text is included in this answer.", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. The AI identified this correctly. We are not told anything more than the survivor was found with scratches and stable vitals found in #5.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The report by the acting senior firefighter in the response is unsupported, as per sentence 5 which doens't say who reports it.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}}}, "9": {"question_id": "4", "question": "Why was the situation in Turkey so treacherous?", "system": {"davinci": {"answer": "The answer is simple.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No answer is available in the text, but this worthless answer made no attempt to even check", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The situation in Turkey was treacherous based on the extent of wreckage and many people that were killed ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain why the area is treacherous but an answered can be inferred from the information available. There was a strong earthquake that flattened hundreds of buildings and killed 46,000 people. With buildings in piles, everything is unstable and the rubble could shift at any time. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is useless and incomplete, misses all relevant info in the document", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incomplete and missed the information about the magnitude and death toll of the earthquake, as per sentences 11 and 12.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The situation in Turkey was treacherous due to the 7.8-magnitude earthquake that hit the country, causing widespread devastation, thousands of deaths, and leaving many trapped under collapsed buildings and rubble.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the overall theme of the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 and other parts of the article corresponds to the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported via information within sentences 3, 11, 12, and 8. It effectively summarizes the situation with this data.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 11 and 12 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not explicitly stated in the article, but the situation is probably perilous due to the conditions after the earthquake.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct in that there was no direct statement explaining why it was treacherous.  There are pieces of info that indicate treacherous conditions however. Sentence 1 mentions rubble and 2 mentions wreckage. Those pieces of information should be added to the information given in the answer about the earthquake, as that better demonstrates the the mentioned peril related to the sentence 1 mentioned earthquake..", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "A reasonable supposition is made based on limited information in the article", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told how the conditions were treacherous, but it could be inferred based on the article that situation was dangerous due to the earthquake's aftermath.", "ebr_raw": "80", "ebr_num": 80, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "According to the article, the earthquake was very intense", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_2": {"answer": "The situation in Turkey was so treacherous because of the large scale of destruction. Thousands of people died, and many spent days trapped under the rubble while rescue efforts ran all over the city trying to help. The team members from Hong Kong were able to rescue a few people who were alive and trapped, but they also faced the harsh reality of uncovering bodies in the debris. The situation impacted the rescue team's mental health. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "It is not specifically stated in the text. The AI made some decent assumptions though.", "ebr_raw": "60", "ebr_num": 60, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\"Thousands of people died\" in the response is unsupported. As per sentence 11, it should be tens of thousands of people died.  Also the response failed to mention it was a 7.8 - magnitude earthquake, as per sentence 10, so the situation was treacherous.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " Turkey was hit by a 7.8-magnitude earthquake two weeks ago, which has killed more than 46,000 people. The Hong Kong team had to pull survivors out of the rubble and also had the sadder task of recovering six bodies.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. The entire theme of the article is that it was a dangerous situation caused by an earthquake that caused a lot of destruction and death..", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The earthquake was very intense killing over 46,000 people, this is why the rescue mission was termed as treacherous according to the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The Turkey earthquake was a 7.8-magnitude disaster that killed more than 46,000 people. The Hong Kong team had to navigate through the rubble of destroyed buildings to search for survivors and recover bodies. The environment was dangerous and unfamiliar, making the rescue mission treacherous.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The situation was treacherous as it had killed over 46,000 people according to the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer uses sentences 11, 12, and 8 to properly answer the question. However, the final answer sentence which draws from sentence 17 is not relevant as it is basically just restating that the area was treacherous and using synonyms for treacherous to define it as such.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a fully sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "10": {"question_id": "5", "question": "How did the person survive after being buried under the rubble?", "system": {"davinci": {"answer": "The answer is simple. The person was not buried under the rubble. The person was found in the rubble.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not state how the person survived under the rubble. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer is incoherent, but the article does not really address the \"Why\" question", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not say how the survivor was able to stay alive. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Though the response is supported by the document, it's irrelevant to the question. It missed the information in sentence 5.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not provide the information needed to answer the question. The answer attempts to use a semantic argument, but that does not make sense and is therefore not relevant.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "gpt4": {"answer": "The article does not provide specific details on how the person survived after being buried under the rubble.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question asks for outside information", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information in the article about how the person survived under the rubble. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no specific information on how the person survived. ", "ebr_raw": "90", "ebr_num": 90, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the requested details are not provided.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_1": {"answer": "It is not explicitly stated how the person survived after being buried in the rubble.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. It is not stated in the article how the person survived. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This question requires outside information, which the answer acknowledges", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The article did not state how the person survived under the rubble.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The explicit answer to this question was not given in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not stated.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the document anywhere, so the response is correct.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no information about the person survived under the rubble.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " The Hong Kong team pulled the survivor out of the rubble and alerted paramedics to move in to assess the casualty. The paramedics reported that the person was breathing with stable vital life signs and only had scratches on their head.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information from the text is given, but the answer could be reworded to better address the question, and to acknowledge which information is unavailable", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "We are not told how the survivor survived when they were buried alive. The AI did not correctly identify this.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "4"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain how the person was able to survive. And the answer is completely wrong for the question.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-4 corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2-5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The person was rescued by the 59-strong Hong Kong team sent on a mercy mission to the ravaged country. Leung made his way out of the wreckage and alerted paramedics to move in to assess the casualty, the first rescued by the team. The team then put the casualty on a spinal board and carried them to safety.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is as good as possible given the information in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "We are not told how the survivor survived being trapped.", "ebr_raw": "0", "ebr_num": 0, "workerid": "4"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}}}, "27": {"dataset": "inq-ext", "article_num": 19, "question_details": {"0": {"question_id": "0", "question": "How long have the protests been going on?", "system": {"davinci": {"answer": " The protests started on the day of the budget announcement.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article did not state how long the protest had been going on.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer should have at least pointed out that the tax imposition was \"last week\" as of the time of publication", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document fails to state when the protests began, making the question not answerable.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "As per sentence 1, the protest was on Monday and the state budget was last week. So the response is incorrect.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not provide an exact duration for the protests.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "There is no explicit answer available in the document, but we can reasonably assume that the protests have been going on at least since a tax was passed one week prior to publication", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The exact duration of the protest is not stated in the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. It is not stated how long the protests have been going on. We only know the measure passed \"last week\" and can only assume protests started after that.", "ebr_raw": "50", "ebr_num": 50, "workerid": "4"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The people are protesting against the two-rupee cess imposed on petrol and diesel. This was entered into the state budget the previous week. As such, the protests have been going on for under two weeks.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in its identification. Sentence 1 notes Monday, but that was the date of one protest. it is indicated by the plural, protests, that there have been multiple taking place.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_1": {"answer": "It is not stated how long the protests have been going on; however, black flags were also removed on the day before the events described in the article.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "All relevant information is included, even if the question cannot be fully answered from the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The two-rupee was imposed last week, so it is likely that the protest had been going for about a week. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in noting that it was not stated how long the protests had been going on. Sentence 8 also supports the additional information given that it had been at least since the day before.", "ebr_raw": "90", "ebr_num": 90, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears to be correct based on the information contained in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no specific information on the length of the protest", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The people are protesting against the two-rupee cess imposed on petrol and diesel. This was entered into the state budget the previous week. As such, the protests have been going on for under two weeks.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The answer could have at least alluded to the fact that the impetus for the protests occurred in the last week", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is not stated in the documet anywhere, so the response is correct and supported.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The protests began last week when the two-rupee cess was imposed on petrol and diesel in the state budget.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms that the protest began last week.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not stated when the protests began. Sentence 1 indicates that the cess was imposed last week, but it was not stated the protests actually began then.", "ebr_raw": "0.", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The protest began last week immediately the increase in price of petrol and diesel was announced according to sentence 1.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The protests have been going on since last week when the two-rupee cess was imposed on petrol and diesel in the state budget.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The document only says the original event happened last week, not necessarily that the protests started then", "ebr_raw": "50", "ebr_num": 50, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is incorrect as the protests occurred on Monday, as per sentence 1.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}}}, "1": {"question_id": "0", "question": "What is the significance of the black flags?", "system": {"davinci": {"answer": " Black flags are used to protest against the government.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is correct, but incomplete", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Although the answer is true, it could be more detailed toward the article. The black flags are a sign of protest against the two - rupee cess imposed on petrol and diesel in the state budget last week. ", "ebr_raw": "60", "ebr_num": 60, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 1 of the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information that black flags mean death, as per sentence 9.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "gpt4": {"answer": "The black flags being shown to Kerala chief minister Pinarayi Vijayan by Congress workers are a form of protest against the two-rupee cess imposed on petrol and diesel in the state budget. The black flags are symbolic of opposition and disapproval.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first sentence of the answer is supported by sentence 1. The second sentence is a logical assumption, but the document does not define the specific symbolism of the flags. ", "ebr_raw": "54", "ebr_num": 54, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "It is not stated, but it is implied that the black flags are a form of protest.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are not told what the significance of the color black is, but it is being used as a symbol of the protests.", "ebr_raw": "90", "ebr_num": 90, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid inference, given the text", "ebr_raw": "90", "ebr_num": 90, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first portion of the answer is correct in stating that the info is not stated seeing as the symbolism behind the flags is never discussed within the document. There is an error in the second portion of the answer though in that sentence 1 of the document indicates that the black flags are a form of protest (they were shown as part of protests) rather than implying this as a fact. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not specifically stated. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The black flag is a symbol of the protest", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The explicit answer to this question was not given in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The black flags are a sign of protest against the two - rupee cess imposed on petrol and diesel in the state budget last week. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 11 in the document, the chief minister is dreading black. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The black flags are a symbol of protest against the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The protesters use the back flag to indicate their displeasure in the new price increase on petrol according to the article.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer draws from sentence 1 to craft an answer that addresses the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The black flags are a symbol of protest against the two-rupee cess imposed on petrol and diesel in the state budget last week by Kerala chief minister Pinarayi Vijayan.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document states that the black flags are a sign of protest", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is overall good and accurate, based on the article information", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document states that the protesters were hanging the black flags everywhere in the city which was a symbol of their protest.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "2": {"question_id": "1", "question": "What does it mean that they were taken under preventive custody?", "system": {"davinci": {"answer": "Preventive custody is a form of imprisonment in which a person is held by the state without trial. It is a form of detention.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer was not taken from the text, but it is a reasonable inference that can be drawn", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Based on the article, they were taken under preventive custody so as to prevent them from contesting.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "While the article does not explain detail what preventive custody is, it is reasonable to infer that it is for security reasons and to prevent an act of crime during civil protests.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. The document does not define the term preventative custody, making the question not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information that preventive custody means preventive arrests, as per sentences 2 and 15.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "To be \"taken under preventive custody\" generally means that people are arrested or detained by authorities to prevent them from potentially causing harm or participating in unlawful activities. In this case, Youth Congress state secretary Sandeep P, district president Sudeep James, and secretary V Rahul were taken under preventive custody to prevent them from protesting or causing any potential harm during Kerala Chief Minister Pinarayi Vijayan's visit.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be inferred from the scenario in the article.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer gives some outside information, but it also matches with the theme of the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer draws from an outside source to define what a preventative arrest is. The document never defines this term. Additionally, the document does not state the reason for the preventative arrests mentioned in the answer. Sentence 2 of the document seems to indicate it was related to preventing protests, but there is not enough info there to say for sure.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response can be deduced from the occurrences in the article. ", "ebr_raw": "70", "ebr_num": 70, "workerid": "7"}}}, "human_1": {"answer": "The preventative custody is to ensure that they do not cause harm to Vijayan.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response corresponds to the explanation in the article. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Inferred by #5.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a reasonable assumption, given the article text", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The term preventative custody is not defined within the document, thus making the question unanswerable. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not specifically stated.", "ebr_raw": "70", "ebr_num": 70, "workerid": "4"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They were taking under preventive custody so that they will not be able to protest", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question could not be answered with the article text alone", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "While the article does not explain detail what preventive custody is, it is reasonable to infer that it is for security reasons and to prevent an act of crime during civil protests.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 12 in the document, preventive custody means preventive arrest.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "Preventive custody means that they were arrested in order to prevent them from taking part in protests against the Kerala chief minister Pinarayi Vijayan.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The relevant information is included, but the article does not really address the question directly", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain what preventive custody is.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "According to sentence 2, the arrest was done as a preventive measure so that the people will not be able to protest.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The term is never specifically defined in the document as the answer makes it appear. Sentence 3 indicates that they were arrested, but beyond that, no information.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Preventive measures is a way of keeping them away from protesting according to the article. ", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "Preventive custody is a form of detention in which a person is held in order to prevent them from committing a crime or to protect them from harm. In this case, the Youth Congress state secretary, district president, and secretary were taken into preventive custody in order to prevent them from protesting against the Kerala chief minister Pinarayi Vijayan.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "So that they will be prevented from protesting as a means of precaution", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer includes some outside information, but it fits with the text", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "missing_all", "feedback": "The article does not explain what preventive custody is.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The answer though likely correct, uses information not present in the article.", "ebr_raw": "0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document states that they were taken under preventive custody possible to prevent them from protesting ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "3": {"question_id": "1", "question": "Why are people protesting against the two-rupee cess?", "system": {"davinci": {"answer": "The two-rupee cess is imposed on petrol and diesel.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a simplistic answer, but it is technically correct, and the article doesn't provide any additional insight", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The people are angered by the two - rupee cess imposed on petrol and diesel in the state budget last week putting a large tax burden on them.", "ebr_raw": "30", "ebr_num": 30, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document fails to state why they are protesting against the cess making the question not answerable.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}, "gpt4": {"answer": "People are protesting against the two-rupee cess imposed on petrol and diesel in Kerala's state budget as they are unhappy with the increased cost of fuel resulting from the tax.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "It can be deduced from the article that people are not happy with the increment, hence the reason for the protest. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information found in #1.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document does not say why they are protesting, just what they are protesting against. The answer is therefore not drawn from the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "human_1": {"answer": "It is not stated why people are protesting against the two-rupee cess.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "A reasonable answer could have been given simply based on the fact that this is a tax on necessary fuel", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The people are angered by the tax burden on petrol and diesel.", "ebr_raw": "75", "ebr_num": 75, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "There is no mention of the main reasons why they are protesting against the two-rupee in the article. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the answer is not provided in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The major reason for the protest against the two-rupee is not stated", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is accurate, as the article doesn't specifically state", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer was fairly self-evident, since this is a tax on necessary fuel", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Though not explicitly stated, it is likely the people cannot afford the additional tax burden.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 1 in the document, the two - rupee cess is imposed on petrol and diesel in the state budget.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "The two-rupee cess imposed on petrol and diesel in the state budget last week is seen as an additional burden on the people of Kerala.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The people don't like the financial strain the additional cost is going to have on them according to the document", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not drawn from the document. None of the sentences in the document back up the info in the answer. The info needed to answer the question is not present in the document.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Although the article did not state obvious reason for the protest but I believe the people are not comfortable with the price increase, hence why they are protesting", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "People are protesting against the two-rupee cess imposed on petrol and diesel in the state budget last week because it is an additional financial burden on them.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The document did not specify the major reason for the protest", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer fits with the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The response is probably correct, but it is making an assumption, as the article doesn't technically state why they are protesting the tax", "ebr_raw": "70", "ebr_num": 70, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "4": {"question_id": "2", "question": "How are Youth Congress workers behaving like suicide squads?", "system": {"davinci": {"answer": "The sentence is a metaphor. It means that Youth Congress workers are behaving like suicide squads.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 6 explains how they are behaving like suicide squads. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer just repeats the question in a circular fashion, missing the key information from the text", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Govindan describes suicide squads as workers menacingly running towards the convoy of the chief minister and deliberately creating problems.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer does not explain how they are behaving like this. Such information is located in 6, which states that they are menacingly running towards the convoy and deliberately creating problems.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.It missed the information in sentence 6.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Youth Congress workers are being accused of behaving like suicide squads because of the way they are menacingly running towards the convoy of the chief minister and deliberately creating problems.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 6 which effectively answers the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The Youth Congress workers are allegedly running towards the convoy of the chief minister and causing problems, according to the state secretary.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 5 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information in #5.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly uses info from sentence 5 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not specifically stated.", "ebr_raw": "75", "ebr_num": 75, "workerid": "4"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 5 states how the Youth Congress workers are behaving like suicide squads", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "At least a partial answer could have been taken from the article", "ebr_raw": "40", "ebr_num": 40, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 5 in the document, the youth congress workers are menacingly running towards the convoy of the chief minister and deliberately creating problems.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Govindan describes suicide squads as workers menacingly running towards the convoy of the chief minister and deliberately creating problems.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " The ruling Communist Party of India (Marxist) or CPI(M) justified Vijayan\u2019s increased security, claiming that Youth Congress workers are menacingly running towards the convoy of the chief minister and deliberately creating problems.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The first two lines of the answer are not relevant to the question, as the question is asking how they are behaving in that way, not the credentials of the people who are making the claim. The portion of the answer which follows correctly draws on sentence 6 of the document to provide an appropriate answer.", "ebr_raw": "Score: 60", "ebr_num": 60, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "Youth Congress workers are showing black flags to Kerala Chief Minister Pinarayi Vijayan despite preventive arrests of 10 Congress leaders in Kannur, Kasaragod, and Kozhikode districts. They are also running towards the convoy of the chief minister and deliberately creating problems, which is putting their own safety at risk.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document says they are running towards the convoy cars as if they want to kill themselves because of their anger", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and sourced from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "5": {"question_id": "2", "question": "Why are the workers showing black flags?", "system": {"davinci": {"answer": " The workers are showing black flags to the chief minister as a protest against the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is confirmed by sentence 1.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is actually a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and supported by document sentence 1.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response is partially correct and missed the information that black flag means death. as per sentences 9 and 13. ", "ebr_raw": "40", "ebr_num": 40, "workerid": "1"}}}, "gpt4": {"answer": "The workers are showing black flags to protest against the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article (without resorting to speculation)", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. We are told that the flags are being used in the protests. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly draws from sentence 1 to answer the question. The significance of the black flags in terms of greater symbolism is not defined within the document, so a more elaborate answer is not possible. The info in the answer does answer the asked question however.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_1": {"answer": "It is not stated, but it is implied that the black flags are a form of protest.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The black flags are a sign of their protest", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct in that the flags are being shown in protest. Sentence 1 specifically states this to be the case making it not implied.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer appears reasonable and accurate based on the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "The document does not specifically address why the workers are using black flags, but, based on the information provided, there seems to be political connotations. The black flags seem to symbolize opposition to Vijayan as chief minister. The flags are also being used as a symbol. The article states that the black flags were left at the house of a former legislator who had passed away a few days earlier. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The black flag is an indication of the protest", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is verbose and disorganized, but it contains valid information", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is reasonable and accurate based on the information in the article", "ebr_raw": "90", "ebr_num": 90, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "Most of the response except the last sentence is inferred and unsupported. The response failed to mention that Vijayan dreads black, as per sentence 11.", "ebr_raw": "50", "ebr_num": 50, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The workers are protesting against the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The workers are showing black flag as a representation of their protest against the additional rupee cess on petrol and diesel, this is evident in the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly draws on sentence 1 to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The black flag is an indication of their protest against the two rupee cess imposed on petrol and diesel, this is seen in sentence 1", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The workers are showing black flags to Kerala chief minister Pinarayi Vijayan as part of protests against the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The black flag is a sign of their disapproval of the two rupee cess that was imposed in the petrol and diesel budget", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer could perhaps use more detail, but gives the best answer given the info in the document", "ebr_raw": "65", "ebr_num": 65, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "6": {"question_id": "3", "question": "How are they deliberately creating problems?", "system": {"davinci": {"answer": "They are running towards the convoy of the chief minister.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer and it is based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is supported by 6, but info about what they are doing beyond that is missed. Such info is contained in 3, which notes that they are showing him black flags.", "ebr_raw": "70", "ebr_num": 70, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The response is partially correct and missed the information in sentence 7.", "ebr_raw": "60", "ebr_num": 60, "workerid": "1"}}}, "gpt4": {"answer": "The opposition Congress workers are deliberately creating problems by showing black flags to Kerala Chief Minister Pinarayi Vijayan and protesting against the two-rupee tax imposed on petrol and diesel. According to CPI(M) state secretary M.V. Govindan, they are behaving like suicide squads by menacingly running towards the chief minister's convoy.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as is given", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The second answer sentence is not relevant as that info is already known by the question asker from doc. sentence 6. The first answer sentence is supported by doc. sentence 1.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The nature of the problems they are causing is not directly stated.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They are deliberating causing problems by behaving like suicide squads, raising black flags everywhere and also causing Vijayan to take helicopter on a couple of occasion. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Not stated.", "ebr_raw": "50", "ebr_num": 50, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is asking for outside information, beyond what is given in the article", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in identifying that the problems referenced are not specified.", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "6"}}}, "human_2": {"answer": "The group has deliberately been creating problems by staging protests and blocking off the roads. The group has disrupted Vijayan's ease of access throughout the city, as well as put the police on high alert. The minister is also facing threats from the group, including the waving of black flags and physical threats, such as running toward the chief minister in a \"menacing\" way. ", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "It is not stated how they are creating problems, only that they are running towards the convoy.", "ebr_raw": "50", "ebr_num": 50, "workerid": "4"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-5 confirms the machine response. ", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The article does not explain what they meant by deliberately creating problems.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "\"blocking off the roads. The group has disrupted Vijayan's ease of access throughout the city, as well as put the police on high alert.\" in the response is not stated in the document anywhere. This makes the response incorrect and unsupported.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The Congress workers were showing black flags to Kerala chief minister Pinarayi Vijayan as part of protests against the two-rupee cess imposed on petrol and diesel in the state budget last week. The ruling Communist Party of India (Marxist) or CPI(M) justified Vijayan\u2019s increased security, saying the Youth Congress workers were \"menacingly running towards the convoy of the chief minister\" and deliberately creating problems.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 and 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "The final sentence of the answer is not relevant to answering the question as it is merely repeating information from sentences 4-6. The first sentence contains the answer which simply needs to be \"they were showing black flags\" (drawn from sentence 3) the rest of the answer goes beyond the scope of the question.", "ebr_raw": "Score: 40", "ebr_num": 40, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 and 6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The Youth Congress workers are deliberately creating problems by menacingly running towards the convoy of the chief minister.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The document confirms the machine response.", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid, if concise, answer", "ebr_raw": "85", "ebr_num": 85, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is clear and concise and correctly states the answer from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 6 confirms the machine ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}}}, "7": {"question_id": "3", "question": "Why were the 10 Congress leaders arrested?", "system": {"davinci": {"answer": " The answer is simple. They were arrested because they were protesting against the 2 rupee cess on petrol and diesel.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They were arrested as a preventive measure ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "The answer is misleading, as the article seems to suggest that these 10 were arrested pre-emptively, to stave off protests", "ebr_raw": "15", "ebr_num": 15, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They were not protesting yet. The Congress leaders were put under preventive arrest because they were planning to protest with the people.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer states that they were arrested for protesting, but document sentence 2 indicates that they were arrested before the protests were held. It was called a preventative arrest, and given the context, it can be said that it was meant to keep them from doing something related to the protests.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response appears to be correct but missed the reason that the arrests were preventive measure as per sentences 2 and 15.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The 10 Congress leaders were arrested under \"preventive custody\" measures ahead of Kerala Chief Minister Pinarayi Vijayan\u2019s visit, likely to avoid any disruption or protests during the visit.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 and 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "While it is likely a correct assumption based on the information presented, it is still an assumption. The AI should have at least indicated that it was not stated in the article why they were arrested for preventative measures.", "ebr_raw": "50", "ebr_num": 50, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The info needed to answer the question is not provided by the document. It is not stated what those who were arrested were expected to do. It is logical that it would have something to do with the protests, but the document does not provide this info, beyond saying that they were preventatively arrested.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}}}, "human_1": {"answer": "The leaders were arrested for either showing black flags or as preventative measures in advance of Vijayan's visit.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid and sufficient answer", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The 10 Congress leaders were held under preventive arrest. They did not have anything to do with the black flags.", "ebr_raw": "40", "ebr_num": 40, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 3 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "complete", "feedback": "Sentence 3 indicates that it was congress workers that were arrested for showing the black flags, not the leaders, so the first section of the answer is not supported by the document. The second portion of the answer is supported by sentences 2 and 3.", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is good and correct based on the information in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is correct as the article doesn't specifically state the answer  ", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "They were arrested as a preventive means against the protest", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The 10 Congress leaders' arrests were preventative. It was likely thought the protests would be canceled under the circumstances.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "At least a partial answer could have been taken from the article", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. But the reasons are stated in sentences 3 and 4 in the document.", "ebr_raw": "20", "ebr_num": 20, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The 10 Congress leaders were arrested in order to prevent them from protesting against the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, but it could have had more detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "It is not explicitly stated within the document why they were arrested beyond that it was preventative. It can be assumed that it was to prevent them from protesting as the the answer indicates, but this information is not specifically backed up by any of the document's sentences.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms the machine response ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The 10 Congress leaders were arrested in order to prevent them from protesting against Kerala chief minister Pinarayi Vijayan's two-rupee cess imposed on petrol and diesel in the state budget.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 2 confirms that the arrest was a preventive measure.", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "A lot of information is missing, though the answer is valid", "ebr_raw": "Score: 50", "ebr_num": 50, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and seems correct given the info in the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "8": {"question_id": "4", "question": "Why are people protesting the Union government's policies?", "system": {"davinci": {"answer": " The Union government's policies are not the same as the policies of the state government. The Union government is the government of the whole country. The state government is the government of a state.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question using the text.  The protests were in response to a tax imposed on necessary fuel", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "People are protesting the policy because the policy ahs imposed an increase in the price of petrol and diesel. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The people are angered by the two - rupee cess imposed on petrol and diesel in the state budget last week putting a large tax burden on them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer gets caught up in semantics and does not address the question. It is therefore not relevant. The document does not specifically speak to why they are protesting. What they are protesting is given in 1 which states it is against the two-rupee cess on petrol and diesel. Why the don't want this can be assumed, but there is no info in the document which states why they would be against it.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is irrelevant and unsupported. It missed the information in sentence 1.", "ebr_raw": "Score: 0. The response missed the information in sentence 1 and was irrelevant.", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "The article does not specify why people are protesting the Union government's policies. The protests mentioned in the article are mainly against the two-rupee cess imposed on petrol and diesel in the state budget.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 1", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This answer matches the information given in the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct in the that the broad policies described in the question are not talked about in the document beyond the sentence 1 mention of the 2 rupee cess.", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The people are protesting against the increment on petrol and diesel price", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "human_1": {"answer": "The people are protesting due to the two-rupee cess. Other reasons they might be protesting are not directly stated.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. Information in #1. Not other reasons are given for the protests, though in #7 states policies, which is plural, but again, no other reasons were given.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer should have given more detail about the two-rupee cess, but otherwise, there are no other relevant passages to refer to", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The following information should be in the answer. The two - rupee cess was imposed on petrol and diesel in the state budget.", "ebr_raw": "50", "ebr_num": 50, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer makes proper use of sentence 1 to answer the question. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "They are protesting policies such as the two rupee cess on petrol and diesel. ", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "4"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The Union government policies made a two-rupee cess increase in petrol and diesel", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Answering this question would have required outside information (or speculation)", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document, when in fact the information is contained in sentences 1,10 and 12 in the document,", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The people are angered by the two - rupee cess imposed on petrol and diesel in the state budget last week.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "3"}}}, "text-davinci-003_few_shot": {"answer": " The Union government imposed a two-rupee cess on petrol and diesel in the state budget last week, which is the cause of the protests.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer based on the text", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 1", "ebr_raw": "Score: 100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer correctly uses sentence 1 of the document to answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 states the reason for the protest and this corresponds to the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "People are protesting the Union government's policies because of the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Because of teh increase in price of petrol", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant, answers the question and is consistent with the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 corresponds to the machine response. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}}}, "9": {"question_id": "4", "question": "Why is Vijayan's visit causing so much unrest?", "system": {"davinci": {"answer": " The answer is simple. The Congress party is trying to make a mountain out of a molehill.", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Hus visit is causing unrest because he was the one who imposed two-rupee cess on petrol and diesel. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "No attempt was made to answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The people are angered by the two - rupee cess imposed on petrol and diesel in the state budget last week putting a large tax burden on them.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is too vague to be helpful in answering the question and is therefore not relevant. Sentence 12 indicates that he is the ruler and 1 indicates that it is a time of unpopular government policies. Together, that answers the question.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not stated in the document anywhere. This makes the response incorrect and unsupported.The reasons for the unrest are stated in sentences 1,11,12,15.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Vijayan's visit is causing unrest because opposition Congress workers are protesting against the two-rupee cess imposed on petrol and diesel in the state budget. They showed black flags to Kerala Chief Minister Pinarayi Vijayan as part of their protest.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is as much information as can be taken from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1-6 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. They are protesting the increase on petrol and diesel prices.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by document sentence 1 and effectively answers the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}}}, "human_1": {"answer": "The visit is causing unrest due to the two-rupee cess, which was presumably an action taken by Vijayan or his government.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This is a valid answer, given the limited text, but it should have specified that the two-rupee cess applies to fuel", "ebr_raw": "50", "ebr_num": 50, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is supported by sentence 1 which notes Vijayan's position, the unrest which is taking place, and the cess which was part of the state budget (which indicates the supposed government action noted in the answer)", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response is essentially correct and uses information from the document", "ebr_raw": "85", "ebr_num": 85, "workerid": "2"}}}, "human_2": {"answer": "information not stated in the document", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_major", "feedback": "It can be inferred that Vijayan visit os causing so much unrest because people are not happy with his rulings and policies. ", "ebr_raw": "Score: 10", "ebr_num": 10, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The people are angered by the the two-rupee cess imposed on petrol and diesel. It is a tax burden they don't feel they should have to pay.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "All relevant information from the text was ignored", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is not helpful and misses important information that is relevant to the question", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "2"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This response incorrectly states that the information is not in the document. As per sentence 1 in the document, the protest/unrest is against the two - rupee cess imposed on petrol and diesel in the state budget last week.", "ebr_raw": "0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": "Vijayan imposed a two-rupee cess on petrol and diesel in the state budget last week, which has caused unrest among Congress workers. They are protesting against the imposition of the cess and have been showing black flags to Vijayan in protest.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The question is answered sufficiently", "ebr_raw": "100", "ebr_num": 100, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Vijayan's price increase on petrol and diesel is the major cause of the unrest and protest as seen in sentence 1", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer uses sentence 1 of the document to appropriately answer the question.", "ebr_raw": "100", "ebr_num": 100, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "His visit is causing unrest because he was the one that imposed the two rupee cess on petrol and diesel which the people are protesting against", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "Vijayan's visit is causing unrest because he imposed a two-rupee cess on petrol and diesel in the state budget last week, which has been met with opposition from Congress workers. The Congress workers have been protesting against the two-rupee cess by showing black flags to Vijayan during his visit.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "According to the document, his visit is causing an unrest as people are not happy about the addition of two- rupee cess on petrol and diesel budget. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, though some more detail would have helped", "ebr_raw": "80", "ebr_num": 80, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is relevant and complete and contains info from the document", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "This machine response is marked as correct and mentioning all relevant information from the document.", "ebr_raw": "100", "ebr_num": 100, "workerid": "1"}}}}}, "10": {"question_id": "5", "question": "Why were the six Congress workers arrested for showing black flags?", "system": {"davinci": {"answer": " The sentence is not a complete sentence. It is a fragment. It is a part of a sentence. It is a part of a sentence which is incomplete. It is a part of a sentence which is not a complete sentence. It is a part of a sentence which is a fragment. It is a part of a sentence which is a part of a sentence which is incomplete. It is a part of a sentence which is a part of a sentence which is not a complete sentence. It is a", "annotation_details": {"0": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "Sentence 6 states the reason why they were arrested for showing black flags. ", "ebr_raw": "0", "ebr_num": 0, "workerid": "7"}, "1": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "This answer is completely incoherent, with no attempt made to answer the question using the text", "ebr_raw": "0", "ebr_num": 0, "workerid": "5"}, "2": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The black flags were a symbol of the protest against the two-rupee cess imposed.", "ebr_raw": "0", "ebr_num": 0, "workerid": "3"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer gets lost in semantics and is therefore not relevant. The info needed to answer the question is not specifically given, as the specific law violated is not given, but sentences 5 and 6 provide relevant info by stating that youth workers were said to be behaving like suicide squads, menacingly running towards the convoy and deliberately creating problems.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response is gibberish. The reason is because black flag means death, as per sentence 9.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "gpt4": {"answer": "Six Congress workers were arrested for showing black flags to protest the two-rupee cess imposed on petrol and diesel in the Kerala state budget.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The text doesn't really provide an answer to the \"why\" question, but more detail in this answer could have given a better picture of the circumstances.", "ebr_raw": "60", "ebr_num": 60, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "Sentence 1 confirms the machine response. ", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "Correct. It is already in the statement the question was asking from. As part of the protests, black flags are being used and the 6 workers were showing the flags.", "ebr_raw": "100", "ebr_num": 100, "workerid": "4"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is not correct as it merely states that there were arrests, it does not explain why they were arrested. There is no info in the document which would answer this however, so the question is not answerable based on the document's contents.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}}}, "human_1": {"answer": "It is not stated, but it is implied that the black flags are a form of protest.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is a valid answer, given the limited text", "ebr_raw": "Score: 80", "ebr_num": 80, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The machine response is evident in sentence 1", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_minor", "feedback": "The first part of the answer is correct in that the document does not state what made this act illegal (no laws were mentioned about protesting in this manner). The second part of the answer is not correct however in that it is not implied to be a form of protest but rather is indicated to be one (shown as part of protests) in sentence 1.", "ebr_raw": "50", "ebr_num": 50, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears reasonable and accurate based on the info in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}}}, "human_2": {"answer": "Based on the information provided in the article, the black flags seem to be a symbol for political resistance and are also used as a threat to leaders the black flag users oppose. The Congress workers were likely arrested for showing black flags because they were being used as a symbol of threat. The arrests could also relate to the \"preventative arrests\" that took place before Vijayan's visit. Though showing black flags was a non-violent act, it is possible the authorities believed the act would escalate. ", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response appears accurate and reasonable given the information in the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "1": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They were arrested as a preventive measure", "ebr_raw": "30", "ebr_num": 30, "workerid": "7"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "This is an excellent round-up of the information in the text, and it presents very reasonable assumptions in order to answer the question", "ebr_raw": "95", "ebr_num": 95, "workerid": "5"}, "4": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The response looks good but is formed by inferral and unsupported by the document. The response failed to mention black flag relates to death and Vijayan dreads black as per sentences 8 and 11.", "ebr_raw": "Score: 0", "ebr_num": 0, "workerid": "1"}}}, "text-davinci-003_few_shot": {"answer": " The six Congress workers were arrested for showing black flags to Kerala chief minister Pinarayi Vijayan as part of protests against the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "This answer fits with the text, but it could have included more detail about the circumstances", "ebr_raw": "Score: 70", "ebr_num": 70, "workerid": "5"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "complete", "feedback": "They were arrested for showing black flags as the police have been told to remove every black flags and arrest anyone they can arrest. this is seen in sentence 3", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "3": {"correct_label": "irrelevant", "complete_label": "missing_all", "feedback": "The answer is merely repeating that black flags were shown without explaining why they were arrested for it and is therefore not relevant. The info needed to answer the question comes from sentence 6.", "ebr_raw": "0", "ebr_num": 0, "workerid": "6"}, "4": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "They were arrested because they were behaving like suicide squads.", "ebr_raw": "50", "ebr_num": 50, "workerid": "7"}}}, "text-davinci-003_zero_shot": {"answer": "The six Congress workers were arrested for showing black flags to Kerala chief minister Pinarayi Vijayan as part of protests against the two-rupee cess imposed on petrol and diesel in the state budget last week.", "annotation_details": {"0": {"correct_label": "correct", "complete_label": "complete", "feedback": "Anything black was not to be posted around the city, and this was the reason why they were arrested", "ebr_raw": "100", "ebr_num": 100, "workerid": "7"}, "1": {"correct_label": "correct", "complete_label": "complete", "feedback": "The answer is correct and complete.", "ebr_raw": "100", "ebr_num": 100, "workerid": "3"}, "2": {"correct_label": "correct", "complete_label": "missing_minor", "feedback": "The answer is valid, but needs more detail", "ebr_raw": "70", "ebr_num": 70, "workerid": "5"}, "3": {"correct_label": "correct", "complete_label": "complete", "feedback": "The response correctly answers the question with information from the article", "ebr_raw": "100", "ebr_num": 100, "workerid": "2"}, "4": {"correct_label": "correct", "complete_label": "missing_major", "feedback": "The response missed the fact that the youth congress workers caused problems, as per sentences 5 and 6.", "ebr_raw": "Score: 20", "ebr_num": 20, "workerid": "1"}}}}}}}}